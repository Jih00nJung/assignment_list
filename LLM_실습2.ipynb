{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "t1UrDEEJ7su7"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/100문 100답 데이터.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    raw_data = f.read()\n",
        "\n",
        "template = \"### 질문: {instruction}\\n\\n### 답변: {response}\"\n",
        "\n",
        "formatted_data = []\n",
        "for line in raw_data.strip().split('\\n'):\n",
        "    try:\n",
        "        parts = line.split('\\t', 1)[1].split('|', 1)\n",
        "        instruction = parts[0].strip()\n",
        "        response = parts[1].strip().replace('\"', '')\n",
        "\n",
        "        full_text = template.format(instruction=instruction, response=response)\n",
        "        formatted_data.append({\"text\": full_text})\n",
        "    except:\n",
        "        continue\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q transformers accelerate bitsandbytes peft trl torch"
      ],
      "metadata": {
        "id": "Dv2ZYsEv79G6"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LoRA"
      ],
      "metadata": {
        "id": "ip4L7Sal8HOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "\n",
        "model_id = \"EleutherAI/polyglot-ko-1.3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "# 3. LoRA 설정 및 모델 적용\n",
        "lora_config = LoraConfig(\n",
        "    r=16, # 랭크 (작을수록 가벼움)\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print(model.print_trainable_parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84\n        },
        "id": "N6u_sPpz7-nk",
        "outputId": "c1ced859-8485-4e99-d66f-3f1e873c1191"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,145,728 || all params: 1,334,956,032 || trainable%: 0.2356\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 실행"
      ],
      "metadata": {
        "id": "3acO-UIe8Elj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=25,\n",
        "    save_steps=50,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557\n        },
        "id": "WZ8jjBJf8Ac8",
        "outputId": "359dac37-0e92-4e47-f2b8-e179ae5bad54"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 02:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.492600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.172400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.010400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.858000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.713700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.500800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.440800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.405200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=250, training_loss=1.0856275787353515, metrics={'train_runtime': 122.0553, 'train_samples_per_second': 8.193, 'train_steps_per_second': 2.048, 'total_flos': 174194579865600.0, 'train_loss': 1.0856275787353515, 'epoch': 10.0})"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "결과 확인"
      ],
      "metadata": {
        "id": "fCTLjmoy8CsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습된 모델 저장\n",
        "trainer.model.save_pretrained(\"./fine-tuned-model\")"
      ],
      "metadata": {
        "id": "rGJgp4Jg8Bpx"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = [\n",
        "    \"### 질문: 좋아하는 가수\\n\",\n",
        "    \"### 질문: 가장 좋아하는 과목이 뭐야?\\n\",\n",
        "    \"### 질문: 취미는 뭐야?\\n\",\n",
        "    \"### 질문: 나의 좌우명\\n\"\n",
        "]\n",
        "\n",
        "for input_text in input_texts:\n",
        "    print(f\"{input_text.strip()}\")\n",
        "\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"{response.replace(input_text, '').strip()}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3UyzDP7-gIp",
        "outputId": "57461d9d-6226-487c-cc51-1c48e464d8fa"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### 질문: 좋아하는 가수\n",
            "### 답변: 에스파, 엔믹스\n",
            "\n",
            "### 질문: 가장 좋아하는 과목이 뭐야?\n",
            "### 답변: 과학\n",
            "\n",
            "### 질문: 취미는 뭐야?\n",
            "### 답변: 인터넷 쇼핑하기\n",
            "\n",
            "### 질문: 나의 좌우명\n",
            "### 답변: 없음\n",
            "\n"
          ]
        }
      ]
    }
  ]
}