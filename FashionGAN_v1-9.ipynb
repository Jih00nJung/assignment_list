{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/FashionGAN_v1-9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpbfeLn-pqfI"
      },
      "source": [
        "## 파라미터\n",
        "### v1\n",
        "결과: 다양성은 30%정도 된 것같은데 형체가 무너짐, 원본 형태 강제가 더 필요할 듯\n",
        "```\n",
        "iter: 10000\n",
        "sty: 5\n",
        "ds: 10\n",
        "cyc: 1\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "### v2\n",
        "결과: 원본의 강제성이 너무 심함 (500iter부터 드러남), 그리고 진행할 수록 원인불명의 뭉개짐과 흐려짐(1500~) 발생\n",
        "```\n",
        "iter: 5000\n",
        "sty: 8\n",
        "ds: 13\n",
        "cyc: 2\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "## v3\n",
        "결과: v2때보단 덜 하지만 1500iter부터 점점 흐려짐과 뭉개짐 발생, 2 > 3 >>> 1이니까 파라미터 중 sty가 유력함(5 -> 8 -> 6), 일단 sty는 5로 고정하고 다른 변수 조정\n",
        "```\n",
        "iter: 2500\n",
        "sty: 6\n",
        "ds: 13\n",
        "cyc: 1\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "## v4\n",
        "결과: 2500iter까지는 경과를 더 볼까 싶었는데 3000iter부터 급격히 뭉개짐과 흐려짐, ds에서 문제가 발생한건가봄, ds를 좀 줄이고 시도해보기\n",
        "```\n",
        "iter: 3500\n",
        "sty: 5\n",
        "ds: 12\n",
        "cyc: 0.8\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "## v5\n",
        "결과: ds가 감소해서 다양성이 사라지고 3000iter부터 다시 흐려졌다. v1이랑 거의 비슷한 파라미터인데 왜 이런 결과인지 모르겠다.\n",
        "```\n",
        "iter: 4000\n",
        "sty: 5\n",
        "ds: 9\n",
        "cyc: 0.8\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "## v6\n",
        "결과: 원본의 뭉개짐 문제가 거의 처음부터 발생했다. 판별자재호출을 다시 2로 늘려서 다시 시도해보자\n",
        "```\n",
        "iter: 5000\n",
        "sty: 5\n",
        "ds: 10\n",
        "cyc: 1\n",
        "r1: 0.8\n",
        "d_repeats: 1\n",
        "```\n",
        "\n",
        "## v7\n",
        "결과: v1의 경우를 봐서 판별자재호출을 3으로 늘려 시도해보자, 흐려짐은 나아졌다. (아마 판별자재호출의 영향인듯?) 하지만 형태가 뭉개졌다. cyc를 소폭 늘려보자\n",
        "```\n",
        "iter: 3500\n",
        "sty: 5\n",
        "ds: 10\n",
        "cyc: 1\n",
        "r1: 0.8\n",
        "d_repeats: 2\n",
        "```\n",
        "\n",
        "## v5re (b1)\n",
        "결과: 이때까지 한 것들 중 가장 괜찮아보여서 했는데 5500iter부터 뭉개지고 뿌옇게 됨\n",
        "```\n",
        "iter: 8000\n",
        "sty: 5\n",
        "ds: 9\n",
        "cyc: 0.8\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "batch: 64\n",
        "```\n",
        "\n",
        "## v7re (b2)\n",
        "결과:\n",
        "```\n",
        "iter: 5000\n",
        "sty: 5\n",
        "ds: 10\n",
        "cyc: 1\n",
        "r1: 0.8\n",
        "d_repeats: 2\n",
        "batch: 64\n",
        "```\n",
        "\n",
        "## v8 (b3)\n",
        "결과:\n",
        "```\n",
        "iter:\n",
        "sty: 5\n",
        "ds: 9\n",
        "cyc: 0.01\n",
        "r1: 1\n",
        "d_repeats: 2\n",
        "batch: 64\n",
        "```\n",
        "\n",
        "## v9 A100 (b4)\n",
        "결과:\n",
        "```\n",
        "iter:\n",
        "sty: 5\n",
        "ds: 8\n",
        "cyc: 1.2\n",
        "r1: 1\n",
        "d_repeats: 3\n",
        "batch: 64\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwJWmulsWjmS"
      },
      "source": [
        "1. config.py (설정 및 환경 준비)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr3U21KWYwp",
        "outputId": "d695c8c3-b24e-4e6f-eb0a-96e61db00904"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "import argparse\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"학습에 필요한 모든 하이퍼파라미터를 정의합니다.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # 데이터 및 경로 설정\n",
        "    parser.add_argument('--project_name', type=str, default='FashionGAN_v1-9')\n",
        "    parser.add_argument('--save_root', type=str, default='/content/drive/MyDrive/Colab Notebooks/GAN_assignment')\n",
        "    parser.add_argument('--img_size', type=int, default=64, help='이미지 크기 (FMNIST 기본 28 -> 64 리사이즈)')\n",
        "    parser.add_argument('--batch_size', type=int, default=64)\n",
        "\n",
        "    # 모델 하이퍼파라미터\n",
        "    parser.add_argument('--style_dim', type=int, default=64, help='스타일 코드 차원')\n",
        "    parser.add_argument('--latent_dim', type=int, default=16, help='랜덤 노이즈 차원')\n",
        "    parser.add_argument('--num_domains', type=int, default=10, help='Fashion MNIST 클래스 개수')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=256, help='Mapping Network 히든 차원')\n",
        "\n",
        "    # 학습 설정\n",
        "    parser.add_argument('--total_iters', type=int, default=100000)\n",
        "    parser.add_argument('--resume_iter', type=int, default=0)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--w_hpf', type=float, default=1, help='High-pass filtering 가중치')\n",
        "\n",
        "    # Loss 가중치\n",
        "    parser.add_argument('--lambda_sty', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_ds', type=float, default=8.0)\n",
        "    parser.add_argument('--lambda_cyc', type=float, default=1.2)\n",
        "    parser.add_argument('--lambda_r1', type=float, default=1.0, help='R1 regularization loss 가중치') # R1 loss 가중치\n",
        "\n",
        "    parser.add_argument('--d_train_repeats', type=int, default=3, help='Discriminator 학습 반복 횟수')\n",
        "\n",
        "    # 로깅 주기\n",
        "    parser.add_argument('--sample_freq', type=int, default=500) # 샘플 저장 주기\n",
        "    parser.add_argument('--save_freq', type=int, default=5000)  # 체크포인트 및 평가 주기\n",
        "\n",
        "    # FID/LPIPS 평가용 설정\n",
        "    parser.add_argument('--num_fid_samples', type=int, default=1000, help='FID 계산에 사용할 생성 이미지 수')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def prepare_environment(args):\n",
        "    \"\"\"Google Drive 마운트 및 체크포인트/샘플 디렉토리를 생성합니다.\"\"\"\n",
        "    print(\"--- 환경 설정 중 ---\")\n",
        "    save_path = os.path.join(args.save_root, args.project_name)\n",
        "\n",
        "    # Google Drive 마운트 로직 제거 (외부 셀에서 마운트할 것임)\n",
        "    if not os.path.exists('/content/drive'):\n",
        "\n",
        "        print(\"Google Drive 마운트가 필요합니다. 학습 셀 실행 전에 Drive를 마운트해주세요.\")\n",
        "\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_path, 'samples'), exist_ok=True)\n",
        "    print(f\"저장 경로: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def get_data_transform(img_size):\n",
        "    \"\"\"Fashion MNIST 데이터 전처리를 정의합니다.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # [0,1] -> [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_domain_labels():\n",
        "    \"\"\"Fashion MNIST의 10개 도메인 레이블을 반환합니다.\"\"\"\n",
        "    return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywi2i9fZWl3i"
      },
      "source": [
        "2. model.py (네트워크 아키텍처)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ozdU0eWnJ3",
        "outputId": "07a8de54-9460-4453-9548-91fc5bbe5651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        # 1. 정규화 도구 (학습 파라미터 없음, 단순 통계 정규화)\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        # 2. 스타일 코드 s를 변환하여 감마(스케일)와 베타(시프트)를 만드는 선형 층\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        # s를 통해 파라미터 생성 (h)\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        # 생성된 파라미터를 감마와 베타로 나눔\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        # 정규화된 x에 감마를 곱하고 베타를 더함 -> 스타일 주입\n",
        "        return (1 + gamma) * self.norm(x) + beta\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"기본 ResBlock (다운샘플링 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            actv,\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            actv,\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "class AdaINResBlock(nn.Module):\n",
        "    \"\"\"Generator용 AdaIN ResBlock (Bottleneck 및 Up-sampling 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, style_dim, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self.norm1(x, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + self.shortcut(x)\n",
        "\n",
        "# --- (1) Generator (G) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64  # 경량화를 위한 시작 필터 수\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # 1. 입력부: 흑백(1채널) 이미지를 받아서 32채널 특징 맵으로 변환\n",
        "        self.from_rgb = nn.Conv2d(1, dim_in, 3, 1, 1) # Grayscale 1채널 입력\n",
        "\n",
        "        # 2. 인코더 (Down-sampling): 형태 정보 압축\n",
        "        # Down-sampling blocks (64 -> 8)\n",
        "        self.encode = nn.ModuleList()\n",
        "        curr_dim = dim_in\n",
        "        for _ in range(3): # 8x8 병목 생성\n",
        "            self.encode.append(ResBlock(curr_dim, curr_dim * 2))\n",
        "            self.encode.append(nn.AvgPool2d(2))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # 3. 병목 (Bottleneck): 스타일 주입 시작\n",
        "        # Bottleneck (8x8 유지, AdaIN 적용)\n",
        "        self.decode = nn.ModuleList()\n",
        "        curr_dim = min(curr_dim, max_conv_dim)\n",
        "        for _ in range(2):\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim, style_dim))\n",
        "\n",
        "        # 4. 디코더 (Up-sampling): 이미지 복원 + 스타일 입히기\n",
        "        # Up-sampling blocks (8 -> 64)\n",
        "        for _ in range(3):\n",
        "            self.decode.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim // 2, style_dim))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        # 5. 출력부: 최종적으로 1채널(흑백) 이미지로 변환\n",
        "        # Final Conv\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(curr_dim, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(curr_dim, 1, 1, 1, 0) # Grayscale 1채널 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.from_rgb(x)\n",
        "        for block in self.encode:\n",
        "            x = block(x)\n",
        "\n",
        "        for block in self.decode:\n",
        "            if isinstance(block, AdaINResBlock):\n",
        "                x = block(x, s)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "# --- (2) Mapping Network (F) ---\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        # 공유 레이어 (Shared)\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(latent_dim if not layers else hidden_dim, hidden_dim)]\n",
        "            layers += [nn.ReLU()]\n",
        "        # 1. 공유 레이어 (Shared): 모든 도메인이 공통으로 사용하는 특징 추출\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        # 2. 비공유 레이어 (Unshared): 각 도메인(T-shirt, Pants...)별 전용 스타일 생성기\n",
        "        # 도메인별 출력 레이어 (Unshared)\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, style_dim)\n",
        "            ))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, style_dim)\n",
        "\n",
        "        # 사용자가 요청한 도메인(y)에 해당하는 스타일만 쏙 뽑아서 리턴\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y] # 해당 도메인의 스타일 코드만 선택\n",
        "        return s\n",
        "\n",
        "# --- (3) Style Encoder (E) ---\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지를 보며 특징을 추출 (CNN 구조)\n",
        "        # 64 -> 32 -> 16 -> 8 로 줄어들며 추상적인 특징을 잡아냄\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 비공유 레이어: 추출된 특징을 보고 \"이건 바지 스타일로는 s_pants, 티셔츠로는 s_shirt야\" 라고 해석\n",
        "        # 도메인별 Style Code 출력\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, style_dim))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # 이미지 x에서 시각적 특징 추출\n",
        "        h = self.shared(x) # (batch, curr_dim, 1, 1)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n",
        "\n",
        "# --- (4) Discriminator (D) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지가 진짜인지 가짜인지 판단하기 위한 단서(특징) 추출\n",
        "        # ResBlock이나 Conv 레이어를 사용하여 이미지를 분석함\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 멀티 태스크 헤드: 각 도메인별로 진위 여부를 따로 판별\n",
        "        # 도메인별 진위 판별 헤드\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, 1))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, 1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        score = out[idx, y]\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om5Q1y1oy7kK"
      },
      "source": [
        "3. FID와 LPIPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spZpg9VIzce6",
        "outputId": "4d3519d8-c26b-49c9-d001-e6b6bc2f1f55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch-fidelity in /usr/local/lib/python3.12/dist-packages (0.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (1.16.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (0.24.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-fidelity) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->torch-fidelity) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torch-fidelity) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torch-fidelity) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-fidelity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "QW9PGTKtyobs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96bed43-ed83-478d-8e69-07143ddcb09c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing fid_lpips_calculator.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile fid_lpips_calculator.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch_fidelity import calculate_metrics\n",
        "\n",
        "def generate_and_save_fake_images(G, F, args, device, samples_count, output_dir):\n",
        "    \"\"\"\n",
        "    Fake 이미지를 samples_count만큼 생성하여 디스크에 개별 파일로 저장합니다.\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    generated_count = 0\n",
        "    while generated_count < samples_count:\n",
        "        current_batch_size = min(args.batch_size, samples_count - generated_count)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z_trg = torch.randn(current_batch_size, args.latent_dim).to(device)\n",
        "            y_trg = torch.randint(0, args.num_domains, (current_batch_size,)).to(device)\n",
        "\n",
        "            x_real_dummy = torch.zeros(\n",
        "                current_batch_size, 1, args.img_size, args.img_size\n",
        "            ).to(device)\n",
        "\n",
        "            s_trg = F(z_trg, y_trg)\n",
        "            x_fake = G(x_real_dummy, s_trg)\n",
        "\n",
        "            # [-1, 1] 범위의 이미지를 [0, 1]로 변환하여 저장\n",
        "            x_fake_normalized = (x_fake + 1) / 2\n",
        "\n",
        "        # 이미지를 디스크에 개별 파일로 저장\n",
        "        for i in range(current_batch_size):\n",
        "            filename = os.path.join(output_dir, f'fake_{generated_count + i:04d}.png')\n",
        "            save_image(x_fake_normalized[i], filename)\n",
        "\n",
        "        generated_count += current_batch_size\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def prepare_real_images(args, output_dir):\n",
        "    \"\"\"\n",
        "    Fashion MNIST 테스트 셋을 로드하여 PNG 이미지 파일로 변환 후 디스크에 저장합니다.\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    to_tensor = transforms.Compose([\n",
        "        transforms.Resize((args.img_size, args.img_size)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    test_dataset = datasets.FashionMNIST(root='/content/data', train=False, download=False, transform=to_tensor)\n",
        "\n",
        "    for i, (img_tensor, _) in enumerate(test_dataset):\n",
        "        if i >= 10000:\n",
        "             break\n",
        "\n",
        "        filename = os.path.join(output_dir, f'real_{i:04d}.png')\n",
        "        save_image(img_tensor, filename)\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def compute_fid(G, F, args, device, num_gen=1000): # 함수명을 compute_fid로 변경\n",
        "    \"\"\"\n",
        "    저장된 Real Data와 생성된 Fake Data를 비교하여 FID를 계산합니다.\n",
        "    \"\"\"\n",
        "\n",
        "    FAKE_DIR = '/content/fid_tmp/fake'\n",
        "    REAL_DIR = '/content/fid_tmp/real'\n",
        "\n",
        "    # 1. Real 이미지 디스크 저장 (최초 1회만 필요)\n",
        "    if not os.path.exists(REAL_DIR) or len(os.listdir(REAL_DIR)) < 10000:\n",
        "        print(f\"Real 이미지 10000개 디스크 준비 중...\")\n",
        "        prepare_real_images(args, REAL_DIR)\n",
        "\n",
        "    # 2. Fake 이미지 생성 및 디스크 저장\n",
        "    print(f\"FID 계산을 시작합니다. (생성 이미지 {num_gen}개 디스크 저장 중...)\")\n",
        "    generate_and_save_fake_images(G, F, args, device, num_gen, FAKE_DIR)\n",
        "\n",
        "    # 3. FID 계산 실행\n",
        "    print(\"FID 계산을 시작합니다. (Inception V3 모델 로딩 중...)\")\n",
        "\n",
        "    metrics = calculate_metrics(\n",
        "        input1=REAL_DIR,\n",
        "        input1_name='real',\n",
        "\n",
        "        input2=FAKE_DIR,\n",
        "        input2_name='generated',\n",
        "\n",
        "        cuda=device.type == 'cuda',\n",
        "        kid=True,\n",
        "        fid=True,\n",
        "        lpips=False, # LPIPS 계산 제외\n",
        "        verbose=False,\n",
        "    )\n",
        "\n",
        "    # 임시 디렉터리 삭제\n",
        "    shutil.rmtree(FAKE_DIR)\n",
        "\n",
        "    # FID 점수만 반환\n",
        "    fid_score = metrics.get('frechet_inception_distance', float('inf'))\n",
        "\n",
        "    return fid_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m__DYADCWoiR"
      },
      "source": [
        "4. solver.py (메인 실행 및 학습 루프)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "m3gRsH6MWpyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf80703c-68e2-48b8-a026-dd3e803e3df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing solver.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile solver.py\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import grad as torch_grad\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, prepare_environment, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "class Solver:\n",
        "    def __init__(self, args, device):\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.save_dir = prepare_environment(args)\n",
        "        self.domain_labels = get_domain_labels()\n",
        "\n",
        "        # 데이터셋 준비 (FashionMNIST)\n",
        "        transform = get_data_transform(args.img_size)\n",
        "\n",
        "        # 학습용 데이터 로더 (안정화 설정 적용)\n",
        "        dataset = datasets.FashionMNIST(root='/content/data', train=True, download=False, transform=transform)\n",
        "        self.loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=12, drop_last=True, pin_memory=True)\n",
        "\n",
        "        # 모델 초기화\n",
        "        self.nets = {\n",
        "            'G': Generator(args.img_size, args.style_dim),\n",
        "            'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim),\n",
        "            'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains),\n",
        "            'D': Discriminator(args.img_size, args.num_domains)\n",
        "        }\n",
        "\n",
        "        for name, module in self.nets.items():\n",
        "            module.to(self.device)\n",
        "            module.train()\n",
        "\n",
        "        # 옵티마이저 (D의 betas=(0.0, 0.99)로 수정 완료)\n",
        "        self.optims = {\n",
        "            'G': torch.optim.Adam(self.nets['G'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'F': torch.optim.Adam(self.nets['F'].parameters(), lr=args.lr*0.01, betas=(0.0, 0.99)),\n",
        "            'E': torch.optim.Adam(self.nets['E'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'D': torch.optim.Adam(self.nets['D'].parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "        }\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        self.start_iter = 0\n",
        "        if args.resume_iter > 0:\n",
        "            self.load_checkpoint(args.resume_iter)\n",
        "            self.start_iter = args.resume_iter\n",
        "\n",
        "    def save_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        state = {\n",
        "            'nets': {name: net.state_dict() for name, net in self.nets.items()},\n",
        "            'optims': {name: opt.state_dict() for name, opt in self.optims.items()},\n",
        "            'step': step\n",
        "        }\n",
        "        torch.save(state, path)\n",
        "        print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Checkpoint not found!\")\n",
        "            return\n",
        "\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        for name, net in self.nets.items():\n",
        "            net.load_state_dict(ckpt['nets'][name])\n",
        "        for name, opt in self.optims.items():\n",
        "            opt.load_state_dict(ckpt['optims'][name])\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "    def r1_loss(self, d_out, x_in):\n",
        "        \"\"\"Discriminator의 R1 Gradient Penalty를 계산합니다.\"\"\"\n",
        "        grad_dout = torch_grad(\n",
        "            outputs=d_out.sum(), inputs=x_in,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]\n",
        "        grad_dout2 = grad_dout.pow(2)\n",
        "        assert(grad_dout2.size() == x_in.size())\n",
        "        r1_loss = grad_dout2.reshape(x_in.size(0), -1).sum(1).mean(0)\n",
        "        return r1_loss\n",
        "\n",
        "    def train(self):\n",
        "        print(\"--- 학습 시작 ---\\n\")\n",
        "        nets = self.nets\n",
        "        optims = self.optims\n",
        "        args = self.args\n",
        "\n",
        "        data_iter = iter(self.loader)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for i in range(self.start_iter, args.total_iters):\n",
        "\n",
        "            # D를 G보다 d_train_repeats 만큼 더 학습시킵니다.\n",
        "            for d_repeat in range(args.d_train_repeats): # <-- D 반복 학습 루프 시작\n",
        "\n",
        "                # 1. 데이터 가져오기\n",
        "                try:\n",
        "                    x_real, y_org = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    data_iter = iter(self.loader)\n",
        "                    x_real, y_org = next(data_iter)\n",
        "\n",
        "                x_real = x_real.to(self.device)\n",
        "                y_org = y_org.to(self.device)\n",
        "\n",
        "                # R1 Loss 계산을 위해 x_real에 그래디언트 추적 활성화\n",
        "                x_real.requires_grad_(True)\n",
        "\n",
        "                # 타겟 도메인 및 Latent 생성\n",
        "                y_trg = torch.randint(0, args.num_domains, (x_real.size(0),)).to(self.device)\n",
        "                z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "                # =================================================================================== #\n",
        "                #                               1. Discriminator 학습                                 #\n",
        "                # =================================================================================== #\n",
        "\n",
        "                # Real Loss\n",
        "                d_out_real = nets['D'](x_real, y_org)\n",
        "                d_loss_real = torch.mean(F.relu(1.0 - d_out_real))\n",
        "\n",
        "                # R1 Regularization Loss 계산\n",
        "                d_loss_r1 = self.r1_loss(d_out_real, x_real)\n",
        "\n",
        "                # Fake Loss (Latent 기반 생성)\n",
        "                with torch.no_grad():\n",
        "                    s_trg = nets['F'](z_trg, y_trg)\n",
        "                    x_fake = nets['G'](x_real, s_trg)\n",
        "\n",
        "                d_out_fake = nets['D'](x_fake.detach(), y_trg)\n",
        "                d_loss_fake = torch.mean(F.relu(1.0 + d_out_fake))\n",
        "\n",
        "                # D Total Loss: Hinge Loss + R1 Loss\n",
        "                d_loss = d_loss_real + d_loss_fake + args.lambda_r1 * d_loss_r1\n",
        "\n",
        "                optims['D'].zero_grad()\n",
        "                d_loss.backward()\n",
        "                optims['D'].step()\n",
        "\n",
        "                # 그래디언트 추적 해제\n",
        "                x_real.requires_grad_(False)\n",
        "\n",
        "            # G 학습은 1번만 수행\n",
        "            # =================================================================================== #\n",
        "            #                     2. Generator, Mapping, Encoder 학습                             #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # G 학습에 사용할 z_trg, z_trg2는 여기서 생성\n",
        "            z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "            z_trg2 = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "            # Adversarial Loss\n",
        "            s_trg = nets['F'](z_trg, y_trg)\n",
        "            x_fake = nets['G'](x_real, s_trg)\n",
        "            d_out_fake = nets['D'](x_fake, y_trg)\n",
        "            g_loss_adv = -torch.mean(d_out_fake)\n",
        "\n",
        "            # Style Reconstruction Loss\n",
        "            s_pred = nets['E'](x_fake, y_trg)\n",
        "            g_loss_sty = torch.mean(torch.abs(s_trg - s_pred))\n",
        "\n",
        "            # Diversity Sensitive Loss\n",
        "            s_trg2 = nets['F'](z_trg2, y_trg)\n",
        "            x_fake2 = nets['G'](x_real, s_trg2)\n",
        "            g_loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            s_org = nets['E'](x_real, y_org)\n",
        "            x_rec = nets['G'](x_fake, s_org)\n",
        "            g_loss_cyc = torch.mean(torch.abs(x_real - x_rec))\n",
        "\n",
        "            # Total Loss\n",
        "            g_loss = g_loss_adv \\\n",
        "                     + args.lambda_sty * g_loss_sty \\\n",
        "                     - args.lambda_ds * g_loss_ds \\\n",
        "                     + args.lambda_cyc * g_loss_cyc\n",
        "\n",
        "            optims['G'].zero_grad()\n",
        "            optims['F'].zero_grad()\n",
        "            optims['E'].zero_grad()\n",
        "            g_loss.backward()\n",
        "            optims['G'].step()\n",
        "            optims['F'].step()\n",
        "            optims['E'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 3. 로깅 및 저장                                     #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # 100 iter마다 로깅 및 평가 (save_freq 설정에 따름)\n",
        "            if (i + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Iter [{i+1}/{args.total_iters}] Time: {elapsed:.2f}s | \"\n",
        "                      f\"D_loss: {d_loss.item():.4f} | G_adv: {g_loss_adv.item():.4f} | \"\n",
        "                      f\"Sty: {g_loss_sty.item():.4f} | Cyc: {g_loss_cyc.item():.4f}\")\n",
        "\n",
        "            if (i + 1) % args.sample_freq == 0:\n",
        "                self.save_samples(x_real, y_org, i + 1)\n",
        "\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                self.save_checkpoint(i + 1)\n",
        "\n",
        "    def save_samples(self, x_real, y_org, step):\n",
        "        \"\"\"학습 중간 결과 이미지 저장 (도메인 라벨 포함 시각화 개선)\"\"\"\n",
        "        nets = self.nets\n",
        "        args = self.args\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nets['G'].eval()\n",
        "            nets['F'].eval()\n",
        "\n",
        "            x_real_subset = x_real[:args.num_domains].to(self.device)\n",
        "\n",
        "            z_fix = torch.randn(1, args.latent_dim).repeat(args.num_domains, 1).to(self.device)\n",
        "            y_fix = torch.arange(args.num_domains).to(self.device)\n",
        "            s_fix = nets['F'](z_fix, y_fix)\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # 1. 소스 이미지\n",
        "            source_row = [x_real_subset[i].cpu() for i in range(len(x_real_subset))]\n",
        "            images.extend(source_row)\n",
        "\n",
        "            # 2. 나머지 영역: 변환된 이미지 (스타일 변환 매트릭스)\n",
        "            for j in range(args.num_domains):\n",
        "                s_curr = s_fix[j].unsqueeze(0).repeat(x_real_subset.size(0), 1)\n",
        "                x_fake_row = nets['G'](x_real_subset, s_curr)\n",
        "                images.extend([x_fake_row[i].cpu() for i in range(len(x_real_subset))])\n",
        "\n",
        "            images = torch.stack(images, dim=0)\n",
        "\n",
        "            path = os.path.join(self.save_dir, 'samples', f'{step:06d}_grid.jpg')\n",
        "            save_image(images, path, nrow=len(x_real_subset), padding=2, normalize=True)\n",
        "            print(f\"Sample image grid saved to {path}\")\n",
        "\n",
        "        # 다시 학습 모드\n",
        "        nets['G'].train()\n",
        "        nets['F'].train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 시드 고정\n",
        "    torch.manual_seed(777)\n",
        "    np.random.seed(777)\n",
        "\n",
        "    # 설정 로드\n",
        "    config = get_config()\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Solver 시작\n",
        "    solver = Solver(config, device)\n",
        "    solver.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 진행"
      ],
      "metadata": {
        "id": "-Wz7YyrUrtFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P7jdfs27gtW",
        "outputId": "9127d97e-3d48-4bbe-d202-ed6558c8d741"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.datasets as datasets\n",
        "\n",
        "datasets.FashionMNIST(root='/content/data', train=True, download=True)\n",
        "datasets.FashionMNIST(root='/content/data', train=False, download=True)\n",
        "\n",
        "print(\"Fashion MNIST 데이터셋 다운로드 및 준비 완료. 이제 코드에서 download=False로 설정 가능합니다.\")"
      ],
      "metadata": {
        "id": "Q609p7X1_MQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7e7da3-3089-4e3b-d7f1-8ee1962421d9"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fashion MNIST 데이터셋 다운로드 및 준비 완료. 이제 코드에서 download=False로 설정 가능합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python solver.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nNHMVu5jroO5",
        "outputId": "cadfb903-ad22-4158-91c4-3cbdf183e326"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- 환경 설정 중 ---\n",
            "저장 경로: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9\n",
            "--- 학습 시작 ---\n",
            "\n",
            "Iter [100/100000] Time: 28.07s | D_loss: 0.4296 | G_adv: 1.5965 | Sty: 0.0094 | Cyc: 0.3910\n",
            "Iter [200/100000] Time: 55.30s | D_loss: 0.3296 | G_adv: 1.9685 | Sty: 0.0090 | Cyc: 0.3576\n",
            "Iter [300/100000] Time: 82.44s | D_loss: 0.4180 | G_adv: 1.7920 | Sty: 0.0085 | Cyc: 0.3347\n",
            "Iter [400/100000] Time: 109.91s | D_loss: 0.3341 | G_adv: 2.0609 | Sty: 0.0100 | Cyc: 0.3322\n",
            "Iter [500/100000] Time: 137.13s | D_loss: 0.8764 | G_adv: 1.7631 | Sty: 0.0088 | Cyc: 0.3196\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/000500_grid.jpg\n",
            "Iter [600/100000] Time: 164.46s | D_loss: 0.5457 | G_adv: 2.0876 | Sty: 0.0096 | Cyc: 0.3517\n",
            "Iter [700/100000] Time: 192.01s | D_loss: 0.5308 | G_adv: 1.5064 | Sty: 0.0102 | Cyc: 0.3133\n",
            "Iter [800/100000] Time: 219.17s | D_loss: 0.7943 | G_adv: 1.6826 | Sty: 0.0098 | Cyc: 0.3194\n",
            "Iter [900/100000] Time: 246.32s | D_loss: 1.1226 | G_adv: 1.3378 | Sty: 0.0111 | Cyc: 0.3589\n",
            "Iter [1000/100000] Time: 273.78s | D_loss: 1.1275 | G_adv: 1.4473 | Sty: 0.0107 | Cyc: 0.3152\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/001000_grid.jpg\n",
            "Iter [1100/100000] Time: 301.05s | D_loss: 0.8815 | G_adv: 1.4227 | Sty: 0.0111 | Cyc: 0.3112\n",
            "Iter [1200/100000] Time: 328.21s | D_loss: 1.0336 | G_adv: 0.3007 | Sty: 0.0113 | Cyc: 0.2918\n",
            "Iter [1300/100000] Time: 355.74s | D_loss: 1.0922 | G_adv: 1.5086 | Sty: 0.0124 | Cyc: 0.3826\n",
            "Iter [1400/100000] Time: 382.94s | D_loss: 0.6459 | G_adv: 1.5058 | Sty: 0.0122 | Cyc: 0.3466\n",
            "Iter [1500/100000] Time: 410.10s | D_loss: 0.3911 | G_adv: 2.2176 | Sty: 0.0139 | Cyc: 0.3832\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/001500_grid.jpg\n",
            "Iter [1600/100000] Time: 437.74s | D_loss: 0.3266 | G_adv: 1.5890 | Sty: 0.0148 | Cyc: 0.3170\n",
            "Iter [1700/100000] Time: 464.89s | D_loss: 0.3515 | G_adv: 1.5800 | Sty: 0.0157 | Cyc: 0.3098\n",
            "Iter [1800/100000] Time: 492.04s | D_loss: 0.3121 | G_adv: 2.1002 | Sty: 0.0126 | Cyc: 0.2870\n",
            "Iter [1900/100000] Time: 519.48s | D_loss: 0.3495 | G_adv: 2.1493 | Sty: 0.0120 | Cyc: 0.3055\n",
            "Iter [2000/100000] Time: 546.64s | D_loss: 0.3720 | G_adv: 1.6867 | Sty: 0.0132 | Cyc: 0.2686\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/002000_grid.jpg\n",
            "Iter [2100/100000] Time: 573.86s | D_loss: 0.6524 | G_adv: 1.6974 | Sty: 0.0126 | Cyc: 0.2676\n",
            "Iter [2200/100000] Time: 601.35s | D_loss: 0.5172 | G_adv: 1.8336 | Sty: 0.0134 | Cyc: 0.2695\n",
            "Iter [2300/100000] Time: 628.55s | D_loss: 0.5284 | G_adv: 1.9861 | Sty: 0.0121 | Cyc: 0.2517\n",
            "Iter [2400/100000] Time: 655.72s | D_loss: 0.3241 | G_adv: 1.6298 | Sty: 0.0138 | Cyc: 0.2951\n",
            "Iter [2500/100000] Time: 683.22s | D_loss: 0.6542 | G_adv: 1.6876 | Sty: 0.0163 | Cyc: 0.2066\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/002500_grid.jpg\n",
            "Iter [2600/100000] Time: 710.54s | D_loss: 0.4397 | G_adv: 1.8369 | Sty: 0.0130 | Cyc: 0.2107\n",
            "Iter [2700/100000] Time: 737.75s | D_loss: 0.3028 | G_adv: 1.7822 | Sty: 0.0126 | Cyc: 0.2056\n",
            "Iter [2800/100000] Time: 764.94s | D_loss: 0.4716 | G_adv: 1.9812 | Sty: 0.0149 | Cyc: 0.2096\n",
            "Iter [2900/100000] Time: 792.47s | D_loss: 0.5307 | G_adv: 2.0355 | Sty: 0.0139 | Cyc: 0.1990\n",
            "Iter [3000/100000] Time: 819.61s | D_loss: 0.4565 | G_adv: 1.4581 | Sty: 0.0135 | Cyc: 0.2112\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/003000_grid.jpg\n",
            "Iter [3100/100000] Time: 846.90s | D_loss: 0.5037 | G_adv: 2.1375 | Sty: 0.0148 | Cyc: 0.2427\n",
            "Iter [3200/100000] Time: 874.41s | D_loss: 0.6847 | G_adv: 1.7334 | Sty: 0.0118 | Cyc: 0.1948\n",
            "Iter [3300/100000] Time: 901.61s | D_loss: 0.4977 | G_adv: 1.7604 | Sty: 0.0118 | Cyc: 0.2075\n",
            "Iter [3400/100000] Time: 928.79s | D_loss: 0.4593 | G_adv: 1.3319 | Sty: 0.0122 | Cyc: 0.2007\n",
            "Iter [3500/100000] Time: 956.31s | D_loss: 0.3875 | G_adv: 2.1490 | Sty: 0.0126 | Cyc: 0.1800\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/003500_grid.jpg\n",
            "Iter [3600/100000] Time: 983.59s | D_loss: 0.5402 | G_adv: 1.5679 | Sty: 0.0144 | Cyc: 0.2141\n",
            "Iter [3700/100000] Time: 1010.75s | D_loss: 0.6163 | G_adv: 1.5839 | Sty: 0.0117 | Cyc: 0.2041\n",
            "Iter [3800/100000] Time: 1038.28s | D_loss: 0.6904 | G_adv: 1.4552 | Sty: 0.0110 | Cyc: 0.1902\n",
            "Iter [3900/100000] Time: 1065.43s | D_loss: 0.7604 | G_adv: 1.7628 | Sty: 0.0119 | Cyc: 0.1938\n",
            "Iter [4000/100000] Time: 1092.59s | D_loss: 0.5602 | G_adv: 1.7024 | Sty: 0.0110 | Cyc: 0.1721\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/004000_grid.jpg\n",
            "Iter [4100/100000] Time: 1120.24s | D_loss: 0.5476 | G_adv: 1.5319 | Sty: 0.0107 | Cyc: 0.1870\n",
            "Iter [4200/100000] Time: 1147.40s | D_loss: 0.4046 | G_adv: 1.4425 | Sty: 0.0111 | Cyc: 0.1850\n",
            "Iter [4300/100000] Time: 1174.57s | D_loss: 0.5434 | G_adv: 1.4059 | Sty: 0.0121 | Cyc: 0.1960\n",
            "Iter [4400/100000] Time: 1202.06s | D_loss: 0.5187 | G_adv: 1.9189 | Sty: 0.0107 | Cyc: 0.1794\n",
            "Iter [4500/100000] Time: 1229.22s | D_loss: 0.4529 | G_adv: 1.7000 | Sty: 0.0129 | Cyc: 0.1891\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/004500_grid.jpg\n",
            "Iter [4600/100000] Time: 1256.47s | D_loss: 0.4575 | G_adv: 1.6404 | Sty: 0.0125 | Cyc: 0.1961\n",
            "Iter [4700/100000] Time: 1283.96s | D_loss: 0.5230 | G_adv: 1.0595 | Sty: 0.0118 | Cyc: 0.1797\n",
            "Iter [4800/100000] Time: 1311.08s | D_loss: 0.8405 | G_adv: 1.9096 | Sty: 0.0111 | Cyc: 0.1673\n",
            "Iter [4900/100000] Time: 1338.26s | D_loss: 0.7682 | G_adv: 1.9525 | Sty: 0.0107 | Cyc: 0.1747\n",
            "Iter [5000/100000] Time: 1365.70s | D_loss: 0.3498 | G_adv: 1.9322 | Sty: 0.0115 | Cyc: 0.1806\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/005000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/checkpoints/005000.ckpt\n",
            "Iter [5100/100000] Time: 1393.64s | D_loss: 0.3598 | G_adv: 1.3126 | Sty: 0.0109 | Cyc: 0.1819\n",
            "Iter [5200/100000] Time: 1420.77s | D_loss: 0.4767 | G_adv: 2.0475 | Sty: 0.0119 | Cyc: 0.1540\n",
            "Iter [5300/100000] Time: 1447.85s | D_loss: 0.9088 | G_adv: 1.4312 | Sty: 0.0110 | Cyc: 0.1699\n",
            "Iter [5400/100000] Time: 1475.30s | D_loss: 0.4458 | G_adv: 1.5548 | Sty: 0.0105 | Cyc: 0.1677\n",
            "Iter [5500/100000] Time: 1502.39s | D_loss: 0.7093 | G_adv: 1.6993 | Sty: 0.0107 | Cyc: 0.1752\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/005500_grid.jpg\n",
            "Iter [5600/100000] Time: 1529.61s | D_loss: 0.4691 | G_adv: 2.1758 | Sty: 0.0105 | Cyc: 0.1689\n",
            "Iter [5700/100000] Time: 1557.02s | D_loss: 0.4403 | G_adv: 1.6402 | Sty: 0.0119 | Cyc: 0.1753\n",
            "Iter [5800/100000] Time: 1584.11s | D_loss: 0.7851 | G_adv: 2.2697 | Sty: 0.0219 | Cyc: 0.1674\n",
            "Iter [5900/100000] Time: 1611.23s | D_loss: 0.6480 | G_adv: 0.6642 | Sty: 0.0120 | Cyc: 0.1615\n",
            "Iter [6000/100000] Time: 1638.71s | D_loss: 0.6800 | G_adv: 1.3319 | Sty: 0.0115 | Cyc: 0.1615\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/006000_grid.jpg\n",
            "Iter [6100/100000] Time: 1665.97s | D_loss: 0.5923 | G_adv: 1.1503 | Sty: 0.0115 | Cyc: 0.1801\n",
            "Iter [6200/100000] Time: 1693.11s | D_loss: 0.6855 | G_adv: 1.0936 | Sty: 0.0121 | Cyc: 0.1770\n",
            "Iter [6300/100000] Time: 1720.59s | D_loss: 0.4029 | G_adv: 1.7243 | Sty: 0.0105 | Cyc: 0.1640\n",
            "Iter [6400/100000] Time: 1747.70s | D_loss: 0.5722 | G_adv: 0.9741 | Sty: 0.0109 | Cyc: 0.1600\n",
            "Iter [6500/100000] Time: 1774.82s | D_loss: 0.7042 | G_adv: 0.9321 | Sty: 0.0156 | Cyc: 0.2417\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/006500_grid.jpg\n",
            "Iter [6600/100000] Time: 1802.34s | D_loss: 0.5467 | G_adv: 1.7785 | Sty: 0.0123 | Cyc: 0.1590\n",
            "Iter [6700/100000] Time: 1829.46s | D_loss: 0.5619 | G_adv: 1.6501 | Sty: 0.0112 | Cyc: 0.1626\n",
            "Iter [6800/100000] Time: 1856.61s | D_loss: 0.6284 | G_adv: 1.7656 | Sty: 0.0121 | Cyc: 0.1596\n",
            "Iter [6900/100000] Time: 1884.08s | D_loss: 0.5711 | G_adv: 1.6857 | Sty: 0.0110 | Cyc: 0.1588\n",
            "Iter [7000/100000] Time: 1911.18s | D_loss: 0.6873 | G_adv: 1.0793 | Sty: 0.0101 | Cyc: 0.1592\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/007000_grid.jpg\n",
            "Iter [7100/100000] Time: 1938.39s | D_loss: 0.5699 | G_adv: 1.3591 | Sty: 0.0113 | Cyc: 0.1497\n",
            "Iter [7200/100000] Time: 1965.80s | D_loss: 0.7940 | G_adv: 1.1435 | Sty: 0.0116 | Cyc: 0.1620\n",
            "Iter [7300/100000] Time: 1992.96s | D_loss: 0.5543 | G_adv: 1.6129 | Sty: 0.0111 | Cyc: 0.1710\n",
            "Iter [7400/100000] Time: 2020.09s | D_loss: 0.5475 | G_adv: 1.8970 | Sty: 0.0112 | Cyc: 0.1667\n",
            "Iter [7500/100000] Time: 2047.49s | D_loss: 0.7430 | G_adv: 1.7842 | Sty: 0.0107 | Cyc: 0.1465\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/007500_grid.jpg\n",
            "Iter [7600/100000] Time: 2074.72s | D_loss: 0.5834 | G_adv: 2.0739 | Sty: 0.0116 | Cyc: 0.1652\n",
            "Iter [7700/100000] Time: 2101.80s | D_loss: 0.7549 | G_adv: 1.3655 | Sty: 0.0103 | Cyc: 0.1474\n",
            "Iter [7800/100000] Time: 2128.94s | D_loss: 0.6435 | G_adv: 2.3385 | Sty: 0.0108 | Cyc: 0.1460\n",
            "Iter [7900/100000] Time: 2156.39s | D_loss: 0.6457 | G_adv: 1.7127 | Sty: 0.0108 | Cyc: 0.1474\n",
            "Iter [8000/100000] Time: 2183.53s | D_loss: 0.6184 | G_adv: 1.7587 | Sty: 0.0101 | Cyc: 0.1350\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/008000_grid.jpg\n",
            "Iter [8100/100000] Time: 2210.86s | D_loss: 0.5695 | G_adv: 1.8656 | Sty: 0.0112 | Cyc: 0.1508\n",
            "Iter [8200/100000] Time: 2238.31s | D_loss: 0.6462 | G_adv: 1.4418 | Sty: 0.0105 | Cyc: 0.1582\n",
            "Iter [8300/100000] Time: 2265.48s | D_loss: 0.7622 | G_adv: 1.1426 | Sty: 0.0113 | Cyc: 0.1597\n",
            "Iter [8400/100000] Time: 2292.58s | D_loss: 0.6555 | G_adv: 1.4131 | Sty: 0.0102 | Cyc: 0.1466\n",
            "Iter [8500/100000] Time: 2320.02s | D_loss: 0.5687 | G_adv: 1.7852 | Sty: 0.0109 | Cyc: 0.1401\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/008500_grid.jpg\n",
            "Iter [8600/100000] Time: 2347.24s | D_loss: 0.7923 | G_adv: 1.5709 | Sty: 0.0117 | Cyc: 0.1444\n",
            "Iter [8700/100000] Time: 2374.43s | D_loss: 0.7236 | G_adv: 1.3241 | Sty: 0.0108 | Cyc: 0.1489\n",
            "Iter [8800/100000] Time: 2402.00s | D_loss: 0.7554 | G_adv: 1.5981 | Sty: 0.0106 | Cyc: 0.1634\n",
            "Iter [8900/100000] Time: 2429.09s | D_loss: 0.7111 | G_adv: 1.0136 | Sty: 0.0118 | Cyc: 0.1570\n",
            "Iter [9000/100000] Time: 2456.18s | D_loss: 0.6956 | G_adv: 1.3606 | Sty: 0.0108 | Cyc: 0.1607\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/009000_grid.jpg\n",
            "Iter [9100/100000] Time: 2483.76s | D_loss: 0.6929 | G_adv: 1.9465 | Sty: 0.0110 | Cyc: 0.1584\n",
            "Iter [9200/100000] Time: 2510.90s | D_loss: 0.4731 | G_adv: 1.8882 | Sty: 0.0116 | Cyc: 0.1516\n",
            "Iter [9300/100000] Time: 2538.03s | D_loss: 0.6232 | G_adv: 1.3530 | Sty: 0.0102 | Cyc: 0.1428\n",
            "Iter [9400/100000] Time: 2565.47s | D_loss: 0.6865 | G_adv: 1.3851 | Sty: 0.0121 | Cyc: 0.1480\n",
            "Iter [9500/100000] Time: 2592.58s | D_loss: 0.4921 | G_adv: 1.8086 | Sty: 0.0109 | Cyc: 0.1496\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/009500_grid.jpg\n",
            "Iter [9600/100000] Time: 2619.78s | D_loss: 0.5930 | G_adv: 1.3810 | Sty: 0.0116 | Cyc: 0.1533\n",
            "Iter [9700/100000] Time: 2647.27s | D_loss: 0.6787 | G_adv: 1.1945 | Sty: 0.0115 | Cyc: 0.1537\n",
            "Iter [9800/100000] Time: 2674.42s | D_loss: 0.5852 | G_adv: 1.2573 | Sty: 0.0111 | Cyc: 0.1489\n",
            "Iter [9900/100000] Time: 2701.54s | D_loss: 0.6113 | G_adv: 1.5540 | Sty: 0.0109 | Cyc: 0.1484\n",
            "Iter [10000/100000] Time: 2728.98s | D_loss: 0.7827 | G_adv: 1.6089 | Sty: 0.0109 | Cyc: 0.1378\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/010000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/checkpoints/010000.ckpt\n",
            "Iter [10100/100000] Time: 2756.97s | D_loss: 0.8225 | G_adv: 1.8938 | Sty: 0.0107 | Cyc: 0.1421\n",
            "Iter [10200/100000] Time: 2784.12s | D_loss: 0.9319 | G_adv: 1.2352 | Sty: 0.0109 | Cyc: 0.1551\n",
            "Iter [10300/100000] Time: 2811.24s | D_loss: 0.6628 | G_adv: 1.9794 | Sty: 0.0110 | Cyc: 0.1430\n",
            "Iter [10400/100000] Time: 2838.66s | D_loss: 0.8152 | G_adv: 1.0611 | Sty: 0.0106 | Cyc: 0.1464\n",
            "Iter [10500/100000] Time: 2865.98s | D_loss: 0.7428 | G_adv: 1.6493 | Sty: 0.0107 | Cyc: 0.1409\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/010500_grid.jpg\n",
            "Iter [10600/100000] Time: 2893.25s | D_loss: 0.9263 | G_adv: 1.1189 | Sty: 0.0105 | Cyc: 0.1380\n",
            "Iter [10700/100000] Time: 2920.67s | D_loss: 0.7025 | G_adv: 1.5569 | Sty: 0.0103 | Cyc: 0.1398\n",
            "Iter [10800/100000] Time: 2947.79s | D_loss: 0.7161 | G_adv: 1.1814 | Sty: 0.0104 | Cyc: 0.1432\n",
            "Iter [10900/100000] Time: 2974.94s | D_loss: 0.8494 | G_adv: 1.0587 | Sty: 0.0106 | Cyc: 0.1671\n",
            "Iter [11000/100000] Time: 3002.38s | D_loss: 0.6096 | G_adv: 2.2141 | Sty: 0.0102 | Cyc: 0.1265\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/011000_grid.jpg\n",
            "Iter [11100/100000] Time: 3029.64s | D_loss: 0.6610 | G_adv: 1.8148 | Sty: 0.0119 | Cyc: 0.1433\n",
            "Iter [11200/100000] Time: 3056.75s | D_loss: 0.8481 | G_adv: 2.1824 | Sty: 0.0104 | Cyc: 0.1483\n",
            "Iter [11300/100000] Time: 3084.17s | D_loss: 0.8268 | G_adv: 1.4481 | Sty: 0.0111 | Cyc: 0.1352\n",
            "Iter [11400/100000] Time: 3111.26s | D_loss: 0.7319 | G_adv: 2.3567 | Sty: 0.0108 | Cyc: 0.1335\n",
            "Iter [11500/100000] Time: 3138.35s | D_loss: 0.7368 | G_adv: 2.1392 | Sty: 0.0106 | Cyc: 0.1403\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/011500_grid.jpg\n",
            "Iter [11600/100000] Time: 3165.91s | D_loss: 0.7455 | G_adv: 2.0150 | Sty: 0.0107 | Cyc: 0.1197\n",
            "Iter [11700/100000] Time: 3193.07s | D_loss: 0.8744 | G_adv: 1.7756 | Sty: 0.0106 | Cyc: 0.1401\n",
            "Iter [11800/100000] Time: 3220.25s | D_loss: 0.7757 | G_adv: 1.6914 | Sty: 0.0105 | Cyc: 0.1495\n",
            "Iter [11900/100000] Time: 3247.78s | D_loss: 0.7515 | G_adv: 1.8144 | Sty: 0.0113 | Cyc: 0.1480\n",
            "Iter [12000/100000] Time: 3274.87s | D_loss: 0.8523 | G_adv: 1.3174 | Sty: 0.0111 | Cyc: 0.1411\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/012000_grid.jpg\n",
            "Iter [12100/100000] Time: 3302.06s | D_loss: 0.7243 | G_adv: 1.9103 | Sty: 0.0107 | Cyc: 0.1481\n",
            "Iter [12200/100000] Time: 3329.46s | D_loss: 0.6926 | G_adv: 1.6051 | Sty: 0.0112 | Cyc: 0.1456\n",
            "Iter [12300/100000] Time: 3356.64s | D_loss: 0.7298 | G_adv: 2.0158 | Sty: 0.0115 | Cyc: 0.1423\n",
            "Iter [12400/100000] Time: 3383.81s | D_loss: 0.8123 | G_adv: 2.2829 | Sty: 0.0102 | Cyc: 0.1356\n",
            "Iter [12500/100000] Time: 3411.28s | D_loss: 0.9602 | G_adv: 1.1843 | Sty: 0.0108 | Cyc: 0.1682\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/012500_grid.jpg\n",
            "Iter [12600/100000] Time: 3438.51s | D_loss: 0.8166 | G_adv: 1.7125 | Sty: 0.0117 | Cyc: 0.1430\n",
            "Iter [12700/100000] Time: 3465.66s | D_loss: 0.7424 | G_adv: 0.9190 | Sty: 0.0107 | Cyc: 0.1256\n",
            "Iter [12800/100000] Time: 3492.83s | D_loss: 0.6091 | G_adv: 1.7877 | Sty: 0.0115 | Cyc: 0.1327\n",
            "Iter [12900/100000] Time: 3520.28s | D_loss: 0.7838 | G_adv: 2.0590 | Sty: 0.0113 | Cyc: 0.1394\n",
            "Iter [13000/100000] Time: 3547.37s | D_loss: 0.6264 | G_adv: 2.2135 | Sty: 0.0106 | Cyc: 0.1306\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/013000_grid.jpg\n",
            "Iter [13100/100000] Time: 3574.56s | D_loss: 0.7326 | G_adv: 1.7177 | Sty: 0.0113 | Cyc: 0.1349\n",
            "Iter [13200/100000] Time: 3601.95s | D_loss: 0.6195 | G_adv: 1.1421 | Sty: 0.0102 | Cyc: 0.1237\n",
            "Iter [13300/100000] Time: 3629.03s | D_loss: 0.8314 | G_adv: 2.2358 | Sty: 0.0101 | Cyc: 0.1261\n",
            "Iter [13400/100000] Time: 3656.13s | D_loss: 0.7387 | G_adv: 2.1804 | Sty: 0.0104 | Cyc: 0.1354\n",
            "Iter [13500/100000] Time: 3683.58s | D_loss: 0.6952 | G_adv: 1.9537 | Sty: 0.0104 | Cyc: 0.1433\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/013500_grid.jpg\n",
            "Iter [13600/100000] Time: 3710.81s | D_loss: 0.8679 | G_adv: 2.5312 | Sty: 0.0112 | Cyc: 0.1281\n",
            "Iter [13700/100000] Time: 3737.90s | D_loss: 0.6860 | G_adv: 1.7492 | Sty: 0.0103 | Cyc: 0.1259\n",
            "Iter [13800/100000] Time: 3765.29s | D_loss: 0.8604 | G_adv: 1.9372 | Sty: 0.0104 | Cyc: 0.1313\n",
            "Iter [13900/100000] Time: 3792.38s | D_loss: 0.8056 | G_adv: 2.2433 | Sty: 0.0103 | Cyc: 0.1294\n",
            "Iter [14000/100000] Time: 3819.43s | D_loss: 0.7133 | G_adv: 1.8241 | Sty: 0.0110 | Cyc: 0.1291\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/014000_grid.jpg\n",
            "Iter [14100/100000] Time: 3847.00s | D_loss: 0.6980 | G_adv: 3.0099 | Sty: 0.0098 | Cyc: 0.1440\n",
            "Iter [14200/100000] Time: 3874.13s | D_loss: 0.7420 | G_adv: 2.4024 | Sty: 0.0110 | Cyc: 0.1410\n",
            "Iter [14300/100000] Time: 3901.24s | D_loss: 0.6339 | G_adv: 1.8981 | Sty: 0.0104 | Cyc: 0.1249\n",
            "Iter [14400/100000] Time: 3928.70s | D_loss: 0.8365 | G_adv: 2.1944 | Sty: 0.0099 | Cyc: 0.1421\n",
            "Iter [14500/100000] Time: 3955.82s | D_loss: 0.6840 | G_adv: 2.6185 | Sty: 0.0113 | Cyc: 0.1467\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/014500_grid.jpg\n",
            "Iter [14600/100000] Time: 3983.02s | D_loss: 0.7384 | G_adv: 1.7333 | Sty: 0.0105 | Cyc: 0.1274\n",
            "Iter [14700/100000] Time: 4010.49s | D_loss: 0.8289 | G_adv: 3.1685 | Sty: 0.0116 | Cyc: 0.1365\n",
            "Iter [14800/100000] Time: 4037.63s | D_loss: 0.7205 | G_adv: 2.0734 | Sty: 0.0100 | Cyc: 0.1271\n",
            "Iter [14900/100000] Time: 4064.76s | D_loss: 0.7350 | G_adv: 2.4336 | Sty: 0.0114 | Cyc: 0.1281\n",
            "Iter [15000/100000] Time: 4092.19s | D_loss: 0.7475 | G_adv: 2.0483 | Sty: 0.0106 | Cyc: 0.1323\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/015000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/checkpoints/015000.ckpt\n",
            "Iter [15100/100000] Time: 4120.28s | D_loss: 0.6658 | G_adv: 1.9026 | Sty: 0.0154 | Cyc: 0.1974\n",
            "Iter [15200/100000] Time: 4147.45s | D_loss: 0.7005 | G_adv: 2.0154 | Sty: 0.0111 | Cyc: 0.1383\n",
            "Iter [15300/100000] Time: 4174.57s | D_loss: 0.7081 | G_adv: 2.1263 | Sty: 0.0111 | Cyc: 0.1559\n",
            "Iter [15400/100000] Time: 4202.08s | D_loss: 0.6464 | G_adv: 2.1065 | Sty: 0.0109 | Cyc: 0.1405\n",
            "Iter [15500/100000] Time: 4229.22s | D_loss: 1.0253 | G_adv: 2.5761 | Sty: 0.0107 | Cyc: 0.1352\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/015500_grid.jpg\n",
            "Iter [15600/100000] Time: 4256.46s | D_loss: 0.6656 | G_adv: 2.0232 | Sty: 0.0108 | Cyc: 0.1248\n",
            "Iter [15700/100000] Time: 4283.95s | D_loss: 0.5916 | G_adv: 2.6617 | Sty: 0.0123 | Cyc: 0.1335\n",
            "Iter [15800/100000] Time: 4311.10s | D_loss: 0.6947 | G_adv: 2.4351 | Sty: 0.0110 | Cyc: 0.1276\n",
            "Iter [15900/100000] Time: 4338.23s | D_loss: 0.9231 | G_adv: 3.0191 | Sty: 0.0107 | Cyc: 0.1231\n",
            "Iter [16000/100000] Time: 4365.64s | D_loss: 1.0512 | G_adv: 2.0952 | Sty: 0.0105 | Cyc: 0.1378\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/016000_grid.jpg\n",
            "Iter [16100/100000] Time: 4392.93s | D_loss: 0.6714 | G_adv: 1.9063 | Sty: 0.0119 | Cyc: 0.1216\n",
            "Iter [16200/100000] Time: 4420.05s | D_loss: 0.8806 | G_adv: 1.5842 | Sty: 0.0128 | Cyc: 0.1277\n",
            "Iter [16300/100000] Time: 4447.49s | D_loss: 0.9155 | G_adv: 2.2960 | Sty: 0.0267 | Cyc: 0.1930\n",
            "Iter [16400/100000] Time: 4474.60s | D_loss: 0.6626 | G_adv: 3.9297 | Sty: 0.0111 | Cyc: 0.1370\n",
            "Iter [16500/100000] Time: 4501.73s | D_loss: 0.8946 | G_adv: 1.5083 | Sty: 0.0103 | Cyc: 0.1339\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/016500_grid.jpg\n",
            "Iter [16600/100000] Time: 4529.31s | D_loss: 0.7662 | G_adv: 2.1195 | Sty: 0.0105 | Cyc: 0.1325\n",
            "Iter [16700/100000] Time: 4556.45s | D_loss: 0.7337 | G_adv: 1.3290 | Sty: 0.0104 | Cyc: 0.1223\n",
            "Iter [16800/100000] Time: 4583.56s | D_loss: 0.6216 | G_adv: 1.3585 | Sty: 0.0115 | Cyc: 0.1145\n",
            "Iter [16900/100000] Time: 4611.05s | D_loss: 0.6824 | G_adv: 2.9585 | Sty: 0.0112 | Cyc: 0.1238\n",
            "Iter [17000/100000] Time: 4638.14s | D_loss: 0.6630 | G_adv: 1.2396 | Sty: 0.0102 | Cyc: 0.1467\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/017000_grid.jpg\n",
            "Iter [17100/100000] Time: 4665.35s | D_loss: 0.8977 | G_adv: 2.3261 | Sty: 0.0102 | Cyc: 0.1392\n",
            "Iter [17200/100000] Time: 4692.82s | D_loss: 0.7787 | G_adv: 3.2895 | Sty: 0.0106 | Cyc: 0.1382\n",
            "Iter [17300/100000] Time: 4719.97s | D_loss: 0.6006 | G_adv: 2.5700 | Sty: 0.0105 | Cyc: 0.1192\n",
            "Iter [17400/100000] Time: 4747.06s | D_loss: 0.7697 | G_adv: 2.2653 | Sty: 0.0119 | Cyc: 0.1155\n",
            "Iter [17500/100000] Time: 4774.47s | D_loss: 0.7323 | G_adv: 2.2899 | Sty: 0.0107 | Cyc: 0.1310\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/017500_grid.jpg\n",
            "Iter [17600/100000] Time: 4801.68s | D_loss: 0.6253 | G_adv: 2.7698 | Sty: 0.0102 | Cyc: 0.1173\n",
            "Iter [17700/100000] Time: 4828.83s | D_loss: 0.7811 | G_adv: 3.8122 | Sty: 0.0110 | Cyc: 0.1350\n",
            "Iter [17800/100000] Time: 4855.96s | D_loss: 0.5953 | G_adv: 3.3490 | Sty: 0.0113 | Cyc: 0.1275\n",
            "Iter [17900/100000] Time: 4883.31s | D_loss: 0.7726 | G_adv: 2.3285 | Sty: 0.0102 | Cyc: 0.1375\n",
            "Iter [18000/100000] Time: 4910.42s | D_loss: 0.5298 | G_adv: 2.4659 | Sty: 0.0113 | Cyc: 0.1254\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/018000_grid.jpg\n",
            "Iter [18100/100000] Time: 4937.61s | D_loss: 0.6187 | G_adv: 1.1519 | Sty: 0.0101 | Cyc: 0.1329\n",
            "Iter [18200/100000] Time: 4965.05s | D_loss: 0.6388 | G_adv: 1.8974 | Sty: 0.0108 | Cyc: 0.1215\n",
            "Iter [18300/100000] Time: 4992.18s | D_loss: 0.7628 | G_adv: 2.9999 | Sty: 0.0105 | Cyc: 0.1196\n",
            "Iter [18400/100000] Time: 5019.29s | D_loss: 1.0291 | G_adv: 1.4120 | Sty: 0.0107 | Cyc: 0.1273\n",
            "Iter [18500/100000] Time: 5046.77s | D_loss: 0.5764 | G_adv: 1.6776 | Sty: 0.0104 | Cyc: 0.1269\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/018500_grid.jpg\n",
            "Iter [18600/100000] Time: 5074.05s | D_loss: 0.9492 | G_adv: 0.8803 | Sty: 0.0107 | Cyc: 0.1278\n",
            "Iter [18700/100000] Time: 5101.19s | D_loss: 0.7201 | G_adv: 1.2521 | Sty: 0.0121 | Cyc: 0.1355\n",
            "Iter [18800/100000] Time: 5128.61s | D_loss: 0.7346 | G_adv: 2.3974 | Sty: 0.0100 | Cyc: 0.1319\n",
            "Iter [18900/100000] Time: 5155.75s | D_loss: 0.5985 | G_adv: 2.1996 | Sty: 0.0108 | Cyc: 0.1214\n",
            "Iter [19000/100000] Time: 5182.89s | D_loss: 0.7386 | G_adv: 2.6742 | Sty: 0.0102 | Cyc: 0.1276\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/019000_grid.jpg\n",
            "Iter [19100/100000] Time: 5210.44s | D_loss: 0.8954 | G_adv: 1.7229 | Sty: 0.0102 | Cyc: 0.1198\n",
            "Iter [19200/100000] Time: 5237.59s | D_loss: 0.6084 | G_adv: 1.8278 | Sty: 0.0102 | Cyc: 0.1398\n",
            "Iter [19300/100000] Time: 5264.70s | D_loss: 0.6500 | G_adv: 1.2072 | Sty: 0.0108 | Cyc: 0.1389\n",
            "Iter [19400/100000] Time: 5292.15s | D_loss: 0.7200 | G_adv: 1.8392 | Sty: 0.0101 | Cyc: 0.1345\n",
            "Iter [19500/100000] Time: 5319.30s | D_loss: 0.7424 | G_adv: 1.6793 | Sty: 0.0097 | Cyc: 0.1236\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/019500_grid.jpg\n",
            "Iter [19600/100000] Time: 5346.54s | D_loss: 0.6255 | G_adv: 2.9252 | Sty: 0.0105 | Cyc: 0.1310\n",
            "Iter [19700/100000] Time: 5374.03s | D_loss: 0.7412 | G_adv: 4.4303 | Sty: 0.0099 | Cyc: 0.1140\n",
            "Iter [19800/100000] Time: 5401.23s | D_loss: 0.7798 | G_adv: 2.4005 | Sty: 0.0112 | Cyc: 0.1340\n",
            "Iter [19900/100000] Time: 5428.38s | D_loss: 0.6753 | G_adv: 3.5616 | Sty: 0.0108 | Cyc: 0.1345\n",
            "Iter [20000/100000] Time: 5455.87s | D_loss: 0.8204 | G_adv: 2.1109 | Sty: 0.0111 | Cyc: 0.1297\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/samples/020000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/checkpoints/020000.ckpt\n",
            "Iter [20100/100000] Time: 5483.94s | D_loss: 0.5806 | G_adv: 1.9390 | Sty: 0.0105 | Cyc: 0.1278\n",
            "object address  : 0x796951b2ba00\n",
            "object refcount : 3\n",
            "object type     : 0xa2a4e0\n",
            "object type name: KeyboardInterrupt\n",
            "object repr     : KeyboardInterrupt()\n",
            "lost sys.stderr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "evaluate"
      ],
      "metadata": {
        "id": "NfBWwW3Xzg8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from fid_lpips_calculator import compute_fid\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "def load_model(args, device, checkpoint_step):\n",
        "    \"\"\"지정된 체크포인트를 불러와 G, F, E 네트워크를 반환합니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "    latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"체크포인트 파일이 없습니다: {ckpt_path}\")\n",
        "\n",
        "    print(f\"\\nLoading checkpoint: {ckpt_path}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 모델 초기화 및 가중치 로드\n",
        "    nets = {\n",
        "        'G': Generator(args.img_size, args.style_dim).to(device),\n",
        "        'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device),\n",
        "        'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains).to(device)\n",
        "    }\n",
        "\n",
        "    for name, net in nets.items():\n",
        "        net.load_state_dict(ckpt['nets'][name])\n",
        "        net.eval()\n",
        "\n",
        "    print(f\"Models loaded at iteration {checkpoint_step}.\")\n",
        "    return nets['G'], nets['F'], nets['E']\n",
        "\n",
        "\n",
        "def compute_metrics_and_visualize(args, device, checkpoint_step):\n",
        "    \"\"\"\n",
        "    체크포인트 로드, FID 계산 및 이미지 시각화를 수행합니다.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 모델 로드\n",
        "    G, F, E = load_model(args, device, checkpoint_step) # E(StyleEncoder)도 함께 로드할 수 있도록 수정\n",
        "\n",
        "    # 2. FID 계산 (FID만 수행)\n",
        "    fid_score = compute_fid(G, F, args, device, args.num_fid_samples)\n",
        "\n",
        "    print(\"\\n--- 평가 결과 ---\")\n",
        "    print(f\"FID Score: {fid_score:.4f} (Lower is better)\")\n",
        "    print(\"-----------------\\n\")\n",
        "\n",
        "    # 3. 시각화 준비 및 실행\n",
        "\n",
        "    # 테스트 데이터 로드 (시각화용)\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='/content/data', train=False, download=False, transform=transform)\n",
        "\n",
        "    # 10장만 샘플링하기 위해 DataLoader 사용\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "    # 소스 이미지 가져오기\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    domain_labels = get_domain_labels()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(args.num_domains + 1, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains):\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = F(z_trg, y_trg) # (1, style_dim)\n",
        "        s_trg = s_trg.repeat(10, 1) # 스타일 코드를 배치 크기만큼 복사\n",
        "\n",
        "        # 이미지 생성\n",
        "        with torch.no_grad():\n",
        "            x_fake = G(x_real, s_trg)\n",
        "\n",
        "        # 결과 출력\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(args.num_domains + 1, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            if col_idx == 0:\n",
        "                plt.text(-25, 32, domain_labels[row_idx], fontsize=10, va='center', ha='right', color='black')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "if __name__ == '__main__':\n",
        "    # 테스트를 원하는 iteration 번호를 직접 지정하세요. (예: 100 iter)\n",
        "    TEST_STEP = 20000\n",
        "\n",
        "    config = get_config()\n",
        "\n",
        "    # config.py의 project_name\n",
        "    config.project_name = 'FashionGAN_v1-9'\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # FID 계산 및 시각화 실행\n",
        "    compute_metrics_and_visualize(config, device, TEST_STEP)"
      ],
      "metadata": {
        "id": "d-CA3pMtzUpt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a06aca-2a10-4754-92dd-dc9e64a8278b"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile evaluate.py\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import save_image\n",
        "from fid_lpips_calculator import compute_fid\n",
        "import re\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "def load_model(args, device, checkpoint_step):\n",
        "    \"\"\"지정된 체크포인트를 불러와 G, F, E 네트워크를 반환합니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "    latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    if not os.path.exists(ckpt_path):\n",
        "        raise FileNotFoundError(f\"체크포인트 파일이 없습니다: {ckpt_path}\")\n",
        "\n",
        "    # 모델 초기화 (load_state_dict 전에 초기화 필요)\n",
        "    nets = {\n",
        "        'G': Generator(args.img_size, args.style_dim).to(device),\n",
        "        'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device),\n",
        "        'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains).to(device)\n",
        "    }\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 가중치 로드 및 eval 모드 설정\n",
        "    for name, net in nets.items():\n",
        "        net.load_state_dict(ckpt['nets'][name])\n",
        "        net.eval()\n",
        "\n",
        "    return nets['G'], nets['F'], nets['E']\n",
        "\n",
        "\n",
        "def plot_fid_history(steps, fids, args):\n",
        "    \"\"\"주어진 FID 기록을 그래프로 시각화하고 저장합니다.\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(steps, fids, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(f'FID Score History ({args.project_name})', fontsize=15)\n",
        "    plt.xlabel('Iteration Step', fontsize=12)\n",
        "    plt.ylabel('FID Score (Lower is better)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- FID 수치 Annotation 추가 로직 ---\n",
        "    for step, fid in zip(steps, fids):\n",
        "        # 수치를 표시할 위치 (x, y)와 텍스트 포맷을 지정\n",
        "        plt.annotate(f'{fid:.2f}', (step, fid),\n",
        "                     textcoords=\"offset points\",\n",
        "                     xytext=(0, 5), # 점 위로 5포인트 이동\n",
        "                     ha='center',\n",
        "                     fontsize=9,\n",
        "                     color='darkred')\n",
        "\n",
        "    plt.xticks(steps, rotation=45)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # 그래프 저장 로직 추가\n",
        "    save_dir = os.path.join(args.save_root, args.project_name)\n",
        "    os.makedirs(save_dir, exist_ok=True) # 저장 디렉토리 확인\n",
        "    save_path = os.path.join(save_dir, f'fid_history_{args.project_name}.png')\n",
        "\n",
        "    plt.savefig(save_path) # PNG 파일로 저장\n",
        "    print(f\"\\nFID 그래프 저장 완료: {save_path}\")\n",
        "\n",
        "    plt.show() # Colab 노트북 셀에서 시각화 출력 시도\n",
        "\n",
        "\n",
        "def get_checkpoint_steps(args, step_interval=5000):\n",
        "    \"\"\"\n",
        "    체크포인트 폴더에서 5000 iter 간격의 .ckpt 파일을 찾아 목록을 반환합니다.\n",
        "    \"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"경고: 체크포인트 디렉터리를 찾을 수 없습니다: {model_path}\")\n",
        "        return []\n",
        "\n",
        "    ckpts = sorted([f for f in os.listdir(model_path) if f.endswith('.ckpt')])\n",
        "\n",
        "    valid_steps = []\n",
        "\n",
        "    for ckpt in ckpts:\n",
        "        match = re.match(r'(\\d{6})\\.ckpt', ckpt)\n",
        "        if match:\n",
        "            step = int(match.group(1))\n",
        "            if step > 0 and step % step_interval == 0:\n",
        "                valid_steps.append(step)\n",
        "\n",
        "    return sorted(list(set(valid_steps)))\n",
        "\n",
        "\n",
        "def visualize_samples(args, device, G, F, step):\n",
        "    \"\"\"단일 체크포인트에 대한 이미지 그리드 시각화를 수행하고 저장합니다.\"\"\"\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='/content/data', train=False, download=False, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
        "\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    domain_labels = get_domain_labels()\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    images_list = []\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(args.num_domains + 1, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "        images_list.append(x_real[i].cpu())\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains):\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = F(z_trg, y_trg)\n",
        "        s_trg = s_trg.repeat(10, 1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x_fake = G(x_real, s_trg)\n",
        "\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(args.num_domains + 1, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            if col_idx == 0:\n",
        "                plt.text(-25, 32, domain_labels[row_idx], fontsize=10, va='center', ha='right', color='black')\n",
        "\n",
        "            images_list.append(x_fake[col_idx].cpu())\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 이미지 파일로 저장하는 로직\n",
        "    save_dir = os.path.join(args.save_root, args.project_name, 'eval_samples')\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    grid_tensor = torch.stack(images_list, dim=0)\n",
        "    save_path = os.path.join(save_dir, f'{step:06d}_eval_grid.png')\n",
        "\n",
        "    save_image(grid_tensor, save_path, nrow=10, normalize=True)\n",
        "    print(f\"\\n이미지 그리드 저장 완료: {save_path}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    config = get_config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    config.project_name = 'FashionGAN_v1-9'\n",
        "\n",
        "    # --- 1. FID 기록 계산 ---\n",
        "    steps_to_evaluate = get_checkpoint_steps(config, step_interval=5000)\n",
        "    fid_results = {}\n",
        "\n",
        "    print(f\"--- 프로젝트: {config.project_name} ---\")\n",
        "    print(f\"평가할 체크포인트 (5000 iter 간격): {steps_to_evaluate}\")\n",
        "\n",
        "    for step in steps_to_evaluate:\n",
        "        print(f\"\\n[Evaluating Iteration {step}]\")\n",
        "        try:\n",
        "            G, F, E = load_model(config, device, step)\n",
        "\n",
        "            # FID 계산\n",
        "            fid_score = compute_fid(G, F, config, device, config.num_fid_samples)\n",
        "\n",
        "            fid_results[step] = fid_score\n",
        "            print(f\"FID Score @ {step}: {fid_score:.4f}\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"오류: {e}\")\n",
        "            continue\n",
        "        except RuntimeError as e:\n",
        "            print(f\"런타임 오류: {e}\")\n",
        "            continue\n",
        "\n",
        "    # --- 2. FID 추이 그래프 시각화 및 저장 ---\n",
        "\n",
        "    if fid_results:\n",
        "        steps = sorted(fid_results.keys())\n",
        "        fids = [fid_results[s] for s in steps]\n",
        "\n",
        "        # 그래프 출력 및 저장 (config를 인수로 전달)\n",
        "        plot_fid_history(steps, fids, config)\n",
        "    else:\n",
        "        print(\"\\n계산된 FID 기록이 없어 그래프를 생성할 수 없습니다.\")\n",
        "\n",
        "\n",
        "    # --- 3. 특정 체크포인트의 이미지 그리드 시각화 및 저장 ---\n",
        "\n",
        "    if steps_to_evaluate:\n",
        "        # 가장 마지막 체크포인트의 이미지 그리드를 시각화하고 저장합니다.\n",
        "        latest_step = steps_to_evaluate[-1]\n",
        "        print(f\"\\n--- 최신 체크포인트 ({latest_step}) 이미지 그리드 시각화 및 저장 ---\")\n",
        "        try:\n",
        "            G, F, E = load_model(config, device, latest_step)\n",
        "            visualize_samples(config, device, G, F, latest_step)\n",
        "        except Exception as e:\n",
        "            print(f\"이미지 시각화 오류: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKFKMWe_GkHV",
        "outputId": "879393d2-6e42-47f9-f78a-c00660c4d084"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing evaluate.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python evaluate.py"
      ],
      "metadata": {
        "id": "_BRLpQm60eJl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c1d7290-b538-4939-e41e-d95acfe72cce"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 프로젝트: FashionGAN_v1-9 ---\n",
            "평가할 체크포인트 (5000 iter 간격): [5000, 10000, 15000, 20000]\n",
            "\n",
            "[Evaluating Iteration 5000]\n",
            "FID 계산을 시작합니다. (생성 이미지 1000개 디스크 저장 중...)\n",
            "FID 계산을 시작합니다. (Inception V3 모델 로딩 중...)\n",
            "/usr/local/lib/python3.12/dist-packages/torch_fidelity/datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "FID Score @ 5000: 168.1529\n",
            "\n",
            "[Evaluating Iteration 10000]\n",
            "FID 계산을 시작합니다. (생성 이미지 1000개 디스크 저장 중...)\n",
            "FID 계산을 시작합니다. (Inception V3 모델 로딩 중...)\n",
            "FID Score @ 10000: 121.9674\n",
            "\n",
            "[Evaluating Iteration 15000]\n",
            "FID 계산을 시작합니다. (생성 이미지 1000개 디스크 저장 중...)\n",
            "FID 계산을 시작합니다. (Inception V3 모델 로딩 중...)\n",
            "FID Score @ 15000: 116.6672\n",
            "\n",
            "[Evaluating Iteration 20000]\n",
            "FID 계산을 시작합니다. (생성 이미지 1000개 디스크 저장 중...)\n",
            "FID 계산을 시작합니다. (Inception V3 모델 로딩 중...)\n",
            "FID Score @ 20000: 120.7809\n",
            "\n",
            "FID 그래프 저장 완료: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/fid_history_FashionGAN_v1-9.png\n",
            "Figure(1000x600)\n",
            "\n",
            "--- 최신 체크포인트 (20000) 이미지 그리드 시각화 및 저장 ---\n",
            "Figure(1500x1000)\n",
            "\n",
            "이미지 그리드 저장 완료: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/FashionGAN_v1-9/eval_samples/020000_eval_grid.png\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}