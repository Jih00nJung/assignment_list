{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/GAN_FMNIST_v3stop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. config.py (설정 및 환경 준비)"
      ],
      "metadata": {
        "id": "VwJWmulsWjmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr3U21KWYwp",
        "outputId": "692df37f-70fc-40db-f8fd-a0cc4c0d3b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "import argparse\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"학습에 필요한 모든 하이퍼파라미터를 정의합니다.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # 데이터 및 경로 설정\n",
        "    parser.add_argument('--project_name', type=str, default='GAN_FMNIST_v3')\n",
        "    parser.add_argument('--save_root', type=str, default='/content/drive/MyDrive/Colab Notebooks/GAN_assignment')\n",
        "    parser.add_argument('--img_size', type=int, default=64, help='이미지 크기 (FMNIST 기본 28 -> 64 리사이즈)')\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "\n",
        "    # 모델 하이퍼파라미터\n",
        "    parser.add_argument('--style_dim', type=int, default=64, help='스타일 코드 차원')\n",
        "    parser.add_argument('--latent_dim', type=int, default=16, help='랜덤 노이즈 차원')\n",
        "    parser.add_argument('--num_domains', type=int, default=10, help='Fashion MNIST 클래스 개수')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=256, help='Mapping Network 히든 차원')\n",
        "\n",
        "    # 학습 설정\n",
        "    parser.add_argument('--total_iters', type=int, default=100000)\n",
        "    parser.add_argument('--resume_iter', type=int, default=0)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--w_hpf', type=float, default=1, help='High-pass filtering 가중치')\n",
        "    parser.add_argument('--d_train_repeats', type=int, default=5, help='Discriminator 학습 반복 횟수') # 추가된 부분\n",
        "\n",
        "    # Loss 가중치\n",
        "    parser.add_argument('--lambda_sty', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_ds', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_cyc', type=float, default=0)\n",
        "    parser.add_argument('--lambda_r1', type=float, default=1.0, help='R1 regularization loss 가중치') # R1 loss 가중치 추가\n",
        "\n",
        "    # 로깅 주기\n",
        "    parser.add_argument('--sample_freq', type=int, default=1000)\n",
        "    parser.add_argument('--save_freq', type=int, default=5000)\n",
        "\n",
        "    # FID/LPIPS 평가용 설정\n",
        "    parser.add_argument('--num_fid_samples', type=int, default=1000, help='FID 계산에 사용할 생성 이미지 수')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def prepare_environment(args):\n",
        "    \"\"\"Google Drive 마운트 및 체크포인트/샘플 디렉토리를 생성합니다.\"\"\"\n",
        "    print(\"--- 환경 설정 중 ---\")\n",
        "    save_path = os.path.join(args.save_root, args.project_name)\n",
        "\n",
        "    # Google Drive 마운트\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive를 마운트합니다...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted.\")\n",
        "\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_path, 'samples'), exist_ok=True)\n",
        "    print(f\"저장 경로: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def get_data_transform(img_size):\n",
        "    \"\"\"Fashion MNIST 데이터 전처리를 정의합니다.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # [0,1] -> [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_domain_labels():\n",
        "    \"\"\"Fashion MNIST의 10개 도메인 레이블을 반환합니다.\"\"\"\n",
        "    return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. model.py (네트워크 아키텍처)"
      ],
      "metadata": {
        "id": "ywi2i9fZWl3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        # 1. 정규화 도구 (학습 파라미터 없음, 단순 통계 정규화)\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        # 2. 스타일 코드 s를 변환하여 감마(스케일)와 베타(시프트)를 만드는 선형 층\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        # s를 통해 파라미터 생성 (h)\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        # 생성된 파라미터를 감마와 베타로 나눔\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        # 정규화된 x에 감마를 곱하고 베타를 더함 -> 스타일 주입\n",
        "        return (1 + gamma) * self.norm(x) + beta\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"기본 ResBlock (다운샘플링 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            actv,\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            actv,\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "class AdaINResBlock(nn.Module):\n",
        "    \"\"\"Generator용 AdaIN ResBlock (Bottleneck 및 Up-sampling 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, style_dim, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self.norm1(x, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + self.shortcut(x)\n",
        "\n",
        "# --- (1) Generator (G) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64  # 경량화를 위한 시작 필터 수\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # 1. 입력부: 흑백(1채널) 이미지를 받아서 32채널 특징 맵으로 변환\n",
        "        self.from_rgb = nn.Conv2d(1, dim_in, 3, 1, 1) # Grayscale 1채널 입력\n",
        "\n",
        "        # 2. 인코더 (Down-sampling): 형태 정보 압축\n",
        "        # Down-sampling blocks (64 -> 32 -> 16 -> 8)\n",
        "        self.encode = nn.ModuleList()\n",
        "        curr_dim = dim_in\n",
        "        for _ in range(3): # 3번 다운샘플링하여 8x8 병목 생성\n",
        "            self.encode.append(ResBlock(curr_dim, curr_dim * 2))\n",
        "            self.encode.append(nn.AvgPool2d(2))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # 3. 병목 (Bottleneck): 스타일 주입 시작\n",
        "        # Bottleneck (8x8 유지, AdaIN 적용)\n",
        "        self.decode = nn.ModuleList()\n",
        "        curr_dim = min(curr_dim, max_conv_dim)\n",
        "        for _ in range(2):\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim, style_dim))\n",
        "\n",
        "        # 4. 디코더 (Up-sampling): 이미지 복원 + 스타일 입히기\n",
        "        # Up-sampling blocks (8 -> 16 -> 32 -> 64)\n",
        "        for _ in range(3):\n",
        "            self.decode.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim // 2, style_dim))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        # 5. 출력부: 최종적으로 1채널(흑백) 이미지로 변환\n",
        "        # Final Conv\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(curr_dim, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(curr_dim, 1, 1, 1, 0) # Grayscale 1채널 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.from_rgb(x)\n",
        "        for block in self.encode:\n",
        "            x = block(x)\n",
        "\n",
        "        for block in self.decode:\n",
        "            if isinstance(block, AdaINResBlock):\n",
        "                x = block(x, s)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "# --- (2) Mapping Network (F) ---\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        # 공유 레이어 (Shared)\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(latent_dim if not layers else hidden_dim, hidden_dim)]\n",
        "            layers += [nn.ReLU()]\n",
        "        # 1. 공유 레이어 (Shared): 모든 도메인이 공통으로 사용하는 특징 추출\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        # 2. 비공유 레이어 (Unshared): 각 도메인(T-shirt, Pants...)별 전용 스타일 생성기\n",
        "        # 도메인별 출력 레이어 (Unshared)\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, style_dim)\n",
        "            ))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, style_dim)\n",
        "\n",
        "        # 사용자가 요청한 도메인(y)에 해당하는 스타일만 쏙 뽑아서 리턴\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y] # 해당 도메인의 스타일 코드만 선택\n",
        "        return s\n",
        "\n",
        "# --- (3) Style Encoder (E) ---\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지를 보며 특징을 추출 (CNN 구조)\n",
        "        # 64 -> 32 -> 16 -> 8 로 줄어들며 추상적인 특징을 잡아냄\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 비공유 레이어: 추출된 특징을 보고 \"이건 바지 스타일로는 s_pants, 티셔츠로는 s_shirt야\" 라고 해석\n",
        "        # 도메인별 Style Code 출력\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, style_dim))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # 이미지 x에서 시각적 특징 추출\n",
        "        h = self.shared(x) # (batch, curr_dim, 1, 1)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n",
        "\n",
        "# --- (4) Discriminator (D) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지가 진짜인지 가짜인지 판단하기 위한 단서(특징) 추출\n",
        "        # ResBlock이나 Conv 레이어를 사용하여 이미지를 분석함\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 멀티 태스크 헤드: 각 도메인별로 진위 여부를 따로 판별\n",
        "        # 도메인별 진위 판별 헤드\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, 1))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, 1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        score = out[idx, y]\n",
        "        return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ozdU0eWnJ3",
        "outputId": "adb5490f-e8aa-4f58-d777-81fd373c7817"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. solver.py (메인 실행 및 학습 루프)"
      ],
      "metadata": {
        "id": "m__DYADCWoiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import grad as torch_grad # R1 정규화에 필요\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, prepare_environment, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "class Solver:\n",
        "    def __init__(self, args, device):\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.save_dir = prepare_environment(args)\n",
        "        self.domain_labels = get_domain_labels()\n",
        "\n",
        "        # 데이터셋 준비 (FashionMNIST)\n",
        "        transform = get_data_transform(args.img_size)\n",
        "\n",
        "        # 학습용 데이터 로더\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        self.loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "        # 평가용 데이터 로더 (FID/LPIPS 계산 시 사용될 예정)\n",
        "        self.eval_loader = self.get_eval_loader(transform)\n",
        "\n",
        "        # 모델 초기화\n",
        "        # Generator의 채널 용량이 model.py에서 64/512로 증가했다고 가정하고 초기화\n",
        "        self.nets = {\n",
        "            'G': Generator(args.img_size, args.style_dim),\n",
        "            'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim),\n",
        "            'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains),\n",
        "            'D': Discriminator(args.img_size, args.num_domains)\n",
        "        }\n",
        "\n",
        "        for name, module in self.nets.items():\n",
        "            module.to(self.device)\n",
        "            module.train()\n",
        "\n",
        "        # 옵티마이저\n",
        "        self.optims = {\n",
        "            'G': torch.optim.Adam(self.nets['G'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'F': torch.optim.Adam(self.nets['F'].parameters(), lr=args.lr*0.01, betas=(0.0, 0.99)),\n",
        "            'E': torch.optim.Adam(self.nets['E'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'D': torch.optim.Adam(self.nets['D'].parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "        }\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        self.start_iter = 0\n",
        "        if args.resume_iter > 0:\n",
        "            self.load_checkpoint(args.resume_iter)\n",
        "            self.start_iter = args.resume_iter\n",
        "\n",
        "    def get_eval_loader(self, transform):\n",
        "        \"\"\"FID/LPIPS 계산을 위한 평가용 데이터 로더를 준비합니다.\"\"\"\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=self.args.batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "    def save_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        state = {\n",
        "            'nets': {name: net.state_dict() for name, net in self.nets.items()},\n",
        "            'optims': {name: opt.state_dict() for name, opt in self.optims.items()},\n",
        "            'step': step\n",
        "        }\n",
        "        torch.save(state, path)\n",
        "        print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Checkpoint not found!\")\n",
        "            return\n",
        "\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        for name, net in self.nets.items():\n",
        "            net.load_state_dict(ckpt['nets'][name])\n",
        "        for name, opt in self.optims.items():\n",
        "            opt.load_state_dict(ckpt['optims'][name])\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "    def calculate_metrics(self, step):\n",
        "        \"\"\"\n",
        "        FID 및 LPIPS와 같은 정량적 평가지표를 계산합니다. (Placeholder)\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Iteration {step}: Evaluating Metrics ---\")\n",
        "        fid_score = 99.99\n",
        "        print(f\"FID Score: {fid_score:.4f} (낮을수록 좋음)\")\n",
        "        lpips_score = 0.00\n",
        "        print(f\"LPIPS Diversity Score: {lpips_score:.4f} (높을수록 좋음)\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "    def r1_loss(self, d_out, x_in): # R1 Loss 함수 추가\n",
        "        \"\"\"Discriminator의 R1 Gradient Penalty를 계산합니다.\"\"\"\n",
        "        grad_dout = torch_grad(\n",
        "            outputs=d_out.sum(), inputs=x_in,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]\n",
        "        grad_dout2 = grad_dout.pow(2)\n",
        "        assert(grad_dout2.size() == x_in.size())\n",
        "        r1_loss = grad_dout2.reshape(x_in.size(0), -1).sum(1).mean(0)\n",
        "        return r1_loss\n",
        "\n",
        "    def train(self):\n",
        "        print(\"--- 학습 시작 ---\")\n",
        "        nets = self.nets\n",
        "        optims = self.optims\n",
        "        args = self.args\n",
        "\n",
        "        data_iter = iter(self.loader)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for i in range(self.start_iter, args.total_iters):\n",
        "\n",
        "            # D를 G보다 d_train_repeats 만큼 더 학습시킵니다.\n",
        "            for d_repeat in range(args.d_train_repeats): # <-- D 반복 학습 루프 시작 (이 부분이 이전 코드에 없었습니다!)\n",
        "\n",
        "                # 1. 데이터 가져오기\n",
        "                try:\n",
        "                    x_real, y_org = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    data_iter = iter(self.loader)\n",
        "                    x_real, y_org = next(data_iter)\n",
        "\n",
        "                x_real = x_real.to(self.device)\n",
        "                y_org = y_org.to(self.device)\n",
        "\n",
        "                # R1 Loss 계산을 위해 x_real에 그래디언트 추적 활성화\n",
        "                x_real.requires_grad_(True)\n",
        "\n",
        "                # 타겟 도메인 및 Latent 생성\n",
        "                y_trg = torch.randint(0, args.num_domains, (x_real.size(0),)).to(self.device)\n",
        "                z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "                # =================================================================================== #\n",
        "                #                               1. Discriminator 학습                                 #\n",
        "                # =================================================================================== #\n",
        "\n",
        "                # Real Loss\n",
        "                d_out_real = nets['D'](x_real, y_org)\n",
        "                d_loss_real = torch.mean(F.relu(1.0 - d_out_real))\n",
        "\n",
        "                # R1 Regularization Loss 계산\n",
        "                d_loss_r1 = self.r1_loss(d_out_real, x_real)\n",
        "\n",
        "                # Fake Loss (Latent 기반 생성)\n",
        "                with torch.no_grad():\n",
        "                    s_trg = nets['F'](z_trg, y_trg)\n",
        "                    x_fake = nets['G'](x_real, s_trg)\n",
        "\n",
        "                d_out_fake = nets['D'](x_fake.detach(), y_trg)\n",
        "                d_loss_fake = torch.mean(F.relu(1.0 + d_out_fake))\n",
        "\n",
        "                # D Total Loss: Hinge Loss + R1 Loss\n",
        "                d_loss = d_loss_real + d_loss_fake + args.lambda_r1 * d_loss_r1\n",
        "\n",
        "                optims['D'].zero_grad()\n",
        "                d_loss.backward()\n",
        "                optims['D'].step()\n",
        "\n",
        "                # 그래디언트 추적 해제\n",
        "                x_real.requires_grad_(False)\n",
        "\n",
        "            # G 학습은 1번만 수행\n",
        "            # =================================================================================== #\n",
        "            #                     2. Generator, Mapping, Encoder 학습                             #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # G 학습에 사용할 z_trg, z_trg2는 여기서 생성\n",
        "            z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "            z_trg2 = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "            # Adversarial Loss\n",
        "            s_trg = nets['F'](z_trg, y_trg)\n",
        "            x_fake = nets['G'](x_real, s_trg)\n",
        "            d_out_fake = nets['D'](x_fake, y_trg)\n",
        "            g_loss_adv = -torch.mean(d_out_fake)\n",
        "\n",
        "            # Style Reconstruction Loss\n",
        "            s_pred = nets['E'](x_fake, y_trg)\n",
        "            g_loss_sty = torch.mean(torch.abs(s_trg - s_pred))\n",
        "\n",
        "            # Diversity Sensitive Loss\n",
        "            s_trg2 = nets['F'](z_trg2, y_trg)\n",
        "            x_fake2 = nets['G'](x_real, s_trg2)\n",
        "            g_loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            s_org = nets['E'](x_real, y_org)\n",
        "            x_rec = nets['G'](x_fake, s_org)\n",
        "            g_loss_cyc = torch.mean(torch.abs(x_real - x_rec))\n",
        "\n",
        "            # Total Loss\n",
        "            g_loss = g_loss_adv \\\n",
        "                     + args.lambda_sty * g_loss_sty \\\n",
        "                     - args.lambda_ds * g_loss_ds \\\n",
        "                     + args.lambda_cyc * g_loss_cyc\n",
        "\n",
        "            optims['G'].zero_grad()\n",
        "            optims['F'].zero_grad()\n",
        "            optims['E'].zero_grad()\n",
        "            g_loss.backward()\n",
        "            optims['G'].step()\n",
        "            optims['F'].step()\n",
        "            optims['E'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 3. 로깅 및 저장                                     #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Iter [{i+1}/{args.total_iters}] Time: {elapsed:.2f}s | \"\n",
        "                      f\"D_loss: {d_loss.item():.4f} | G_adv: {g_loss_adv.item():.4f} | \"\n",
        "                      f\"Sty: {g_loss_sty.item():.4f} | Cyc: {g_loss_cyc.item():.4f}\")\n",
        "\n",
        "            if (i + 1) % args.sample_freq == 0:\n",
        "                self.save_samples(x_real, y_org, i + 1)\n",
        "\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                self.save_checkpoint(i + 1)\n",
        "                # self.calculate_metrics(i + 1) # 메트릭 계산 (필요 시 주석 해제)\n",
        "\n",
        "    def save_samples(self, x_real, y_org, step):\n",
        "        \"\"\"학습 중간 결과 이미지 저장 (도메인 라벨 포함 시각화 개선)\"\"\"\n",
        "        nets = self.nets\n",
        "        args = self.args\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nets['G'].eval()\n",
        "            nets['F'].eval()\n",
        "\n",
        "            x_real_subset = x_real[:args.num_domains].to(self.device)\n",
        "            # y_org_subset = y_org[:args.num_domains].cpu().numpy()\n",
        "\n",
        "            z_fix = torch.randn(1, args.latent_dim).repeat(args.num_domains, 1).to(self.device)\n",
        "            y_fix = torch.arange(args.num_domains).to(self.device)\n",
        "            s_fix = nets['F'](z_fix, y_fix)\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # 1. 첫 번째 행: 소스 이미지\n",
        "            source_row = [x_real_subset[i].cpu() for i in range(len(x_real_subset))]\n",
        "            images.extend(source_row)\n",
        "\n",
        "            # 2. 나머지 영역: 변환된 이미지 (스타일 변환 매트릭스)\n",
        "            for j in range(args.num_domains):\n",
        "                s_curr = s_fix[j].unsqueeze(0).repeat(x_real_subset.size(0), 1)\n",
        "                x_fake_row = nets['G'](x_real_subset, s_curr)\n",
        "                images.extend([x_fake_row[i].cpu() for i in range(len(x_real_subset))])\n",
        "\n",
        "            images = torch.stack(images, dim=0)\n",
        "\n",
        "            path = os.path.join(self.save_dir, 'samples', f'{step:06d}_grid.jpg')\n",
        "            save_image(images, path, nrow=len(x_real_subset), padding=2, normalize=True)\n",
        "            print(f\"Sample image grid saved to {path}\")\n",
        "\n",
        "        # 다시 학습 모드\n",
        "        nets['G'].train()\n",
        "        nets['F'].train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 시드 고정\n",
        "    torch.manual_seed(777)\n",
        "    np.random.seed(777)\n",
        "\n",
        "    # 설정 로드\n",
        "    config = get_config()\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Solver 시작\n",
        "    solver = Solver(config, device)\n",
        "    solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3gRsH6MWpyA",
        "outputId": "247e9477-7ce8-4402-b7d6-b0b835f13c10"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- 환경 설정 중 ---\n",
            "저장 경로: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3\n",
            "--- 학습 시작 ---\n",
            "Iter [100/100000] Time: 61.32s | D_loss: 0.5263 | G_adv: 1.7756 | Sty: 0.0094 | Cyc: 0.4125\n",
            "Iter [200/100000] Time: 122.60s | D_loss: 0.1953 | G_adv: 2.1152 | Sty: 0.0089 | Cyc: 0.4311\n",
            "Iter [300/100000] Time: 183.76s | D_loss: 0.1653 | G_adv: 2.2987 | Sty: 0.0088 | Cyc: 0.4764\n",
            "Iter [400/100000] Time: 245.07s | D_loss: 0.4761 | G_adv: 1.8414 | Sty: 0.0089 | Cyc: 0.5514\n",
            "Iter [500/100000] Time: 306.29s | D_loss: 0.2799 | G_adv: 1.7395 | Sty: 0.0092 | Cyc: 0.4358\n",
            "Iter [600/100000] Time: 367.47s | D_loss: 0.4617 | G_adv: 1.9223 | Sty: 0.0087 | Cyc: 0.4193\n",
            "Iter [700/100000] Time: 428.65s | D_loss: 0.2628 | G_adv: 1.8927 | Sty: 0.0098 | Cyc: 0.5364\n",
            "Iter [800/100000] Time: 489.87s | D_loss: 0.7662 | G_adv: 2.0758 | Sty: 0.0085 | Cyc: 0.5211\n",
            "Iter [900/100000] Time: 551.04s | D_loss: 0.4375 | G_adv: 1.7783 | Sty: 0.0092 | Cyc: 0.3875\n",
            "Iter [1000/100000] Time: 612.21s | D_loss: 0.2930 | G_adv: 1.6989 | Sty: 0.0081 | Cyc: 0.3998\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/001000_grid.jpg\n",
            "Iter [1100/100000] Time: 674.08s | D_loss: 0.2671 | G_adv: 1.1974 | Sty: 0.0083 | Cyc: 0.3508\n",
            "Iter [1200/100000] Time: 735.32s | D_loss: 0.3795 | G_adv: 1.5609 | Sty: 0.0093 | Cyc: 0.3206\n",
            "Iter [1300/100000] Time: 796.74s | D_loss: 0.7725 | G_adv: 1.4638 | Sty: 0.0078 | Cyc: 0.5052\n",
            "Iter [1400/100000] Time: 858.23s | D_loss: 0.7699 | G_adv: 1.8125 | Sty: 0.0089 | Cyc: 0.2940\n",
            "Iter [1500/100000] Time: 919.85s | D_loss: 0.5573 | G_adv: 1.5283 | Sty: 0.0095 | Cyc: 0.3251\n",
            "Iter [1600/100000] Time: 981.68s | D_loss: 0.5934 | G_adv: 1.1866 | Sty: 0.0089 | Cyc: 0.3536\n",
            "Iter [1700/100000] Time: 1043.31s | D_loss: 0.6733 | G_adv: 1.6516 | Sty: 0.0090 | Cyc: 0.3970\n",
            "Iter [1800/100000] Time: 1104.99s | D_loss: 0.6847 | G_adv: 1.3435 | Sty: 0.0089 | Cyc: 0.3904\n",
            "Iter [1900/100000] Time: 1166.52s | D_loss: 0.7476 | G_adv: 0.6184 | Sty: 0.0095 | Cyc: 0.3233\n",
            "Iter [2000/100000] Time: 1227.96s | D_loss: 1.1390 | G_adv: 0.6861 | Sty: 0.0097 | Cyc: 0.3505\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/002000_grid.jpg\n",
            "Iter [2100/100000] Time: 1289.73s | D_loss: 0.9494 | G_adv: 1.4533 | Sty: 0.0087 | Cyc: 0.3313\n",
            "Iter [2200/100000] Time: 1351.35s | D_loss: 0.9881 | G_adv: 1.5629 | Sty: 0.0087 | Cyc: 0.3253\n",
            "Iter [2300/100000] Time: 1413.12s | D_loss: 0.6084 | G_adv: 1.3586 | Sty: 0.0090 | Cyc: 0.3248\n",
            "Iter [2400/100000] Time: 1474.74s | D_loss: 0.8588 | G_adv: 1.4603 | Sty: 0.0092 | Cyc: 0.3752\n",
            "Iter [2500/100000] Time: 1536.31s | D_loss: 0.7716 | G_adv: 1.3930 | Sty: 0.0083 | Cyc: 0.3316\n",
            "Iter [2600/100000] Time: 1597.79s | D_loss: 0.8420 | G_adv: 1.5916 | Sty: 0.0102 | Cyc: 0.3343\n",
            "Iter [2700/100000] Time: 1659.25s | D_loss: 1.0916 | G_adv: 0.4586 | Sty: 0.0090 | Cyc: 0.2850\n",
            "Iter [2800/100000] Time: 1720.49s | D_loss: 1.2808 | G_adv: 1.0075 | Sty: 0.0093 | Cyc: 0.4208\n",
            "Iter [2900/100000] Time: 1781.78s | D_loss: 0.4261 | G_adv: 1.8268 | Sty: 0.0097 | Cyc: 0.5109\n",
            "Iter [3000/100000] Time: 1843.03s | D_loss: 0.6650 | G_adv: 1.4230 | Sty: 0.0094 | Cyc: 0.3820\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/003000_grid.jpg\n",
            "Iter [3100/100000] Time: 1904.68s | D_loss: 0.7350 | G_adv: 1.7793 | Sty: 0.0089 | Cyc: 0.4449\n",
            "Iter [3200/100000] Time: 1966.04s | D_loss: 0.7717 | G_adv: 1.1390 | Sty: 0.0090 | Cyc: 0.3063\n",
            "Iter [3300/100000] Time: 2027.26s | D_loss: 0.7292 | G_adv: 1.2328 | Sty: 0.0087 | Cyc: 0.4123\n",
            "Iter [3400/100000] Time: 2088.52s | D_loss: 0.7268 | G_adv: 1.5530 | Sty: 0.0083 | Cyc: 0.3239\n",
            "Iter [3500/100000] Time: 2149.77s | D_loss: 0.7698 | G_adv: 1.0935 | Sty: 0.0109 | Cyc: 0.3676\n",
            "Iter [3600/100000] Time: 2211.13s | D_loss: 0.6948 | G_adv: 0.7573 | Sty: 0.0087 | Cyc: 0.4322\n",
            "Iter [3700/100000] Time: 2272.46s | D_loss: 0.5360 | G_adv: 1.8473 | Sty: 0.0083 | Cyc: 0.4123\n",
            "Iter [3800/100000] Time: 2333.76s | D_loss: 0.3895 | G_adv: 1.8655 | Sty: 0.0089 | Cyc: 0.4267\n",
            "Iter [3900/100000] Time: 2395.01s | D_loss: 1.0010 | G_adv: 1.7882 | Sty: 0.0101 | Cyc: 0.4355\n",
            "Iter [4000/100000] Time: 2456.35s | D_loss: 0.9683 | G_adv: 0.7438 | Sty: 0.0099 | Cyc: 0.3913\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/004000_grid.jpg\n",
            "Iter [4100/100000] Time: 2518.16s | D_loss: 0.7423 | G_adv: 0.8610 | Sty: 0.0085 | Cyc: 0.4511\n",
            "Iter [4200/100000] Time: 2579.67s | D_loss: 0.4922 | G_adv: 1.3979 | Sty: 0.0085 | Cyc: 0.4046\n",
            "Iter [4300/100000] Time: 2641.14s | D_loss: 0.5757 | G_adv: 0.8663 | Sty: 0.0086 | Cyc: 0.3634\n",
            "Iter [4400/100000] Time: 2702.71s | D_loss: 0.8238 | G_adv: 1.0963 | Sty: 0.0085 | Cyc: 0.3650\n",
            "Iter [4500/100000] Time: 2764.23s | D_loss: 0.9534 | G_adv: 0.9652 | Sty: 0.0082 | Cyc: 0.3936\n",
            "Iter [4600/100000] Time: 2825.66s | D_loss: 0.7881 | G_adv: 1.3316 | Sty: 0.0089 | Cyc: 0.4461\n",
            "Iter [4700/100000] Time: 2887.23s | D_loss: 1.1975 | G_adv: 0.8922 | Sty: 0.0085 | Cyc: 0.3326\n",
            "Iter [4800/100000] Time: 2948.87s | D_loss: 0.8482 | G_adv: 0.9968 | Sty: 0.0088 | Cyc: 0.3503\n",
            "Iter [4900/100000] Time: 3010.37s | D_loss: 0.6574 | G_adv: 1.1405 | Sty: 0.0088 | Cyc: 0.3958\n",
            "Iter [5000/100000] Time: 3071.96s | D_loss: 0.7802 | G_adv: 1.4732 | Sty: 0.0094 | Cyc: 0.4096\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/005000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/checkpoints/005000.ckpt\n",
            "Iter [5100/100000] Time: 3134.89s | D_loss: 0.7938 | G_adv: 0.4964 | Sty: 0.0088 | Cyc: 0.4741\n",
            "Iter [5200/100000] Time: 3196.27s | D_loss: 0.7494 | G_adv: 0.4896 | Sty: 0.0097 | Cyc: 0.3754\n",
            "Iter [5300/100000] Time: 3257.58s | D_loss: 0.8592 | G_adv: 0.9638 | Sty: 0.0086 | Cyc: 0.4418\n",
            "Iter [5400/100000] Time: 3318.94s | D_loss: 1.0034 | G_adv: 0.8239 | Sty: 0.0101 | Cyc: 0.4446\n",
            "Iter [5500/100000] Time: 3380.33s | D_loss: 0.6197 | G_adv: 1.7253 | Sty: 0.0104 | Cyc: 0.4292\n",
            "Iter [5600/100000] Time: 3441.73s | D_loss: 0.4178 | G_adv: 1.2966 | Sty: 0.0083 | Cyc: 0.3504\n",
            "Iter [5700/100000] Time: 3503.12s | D_loss: 0.7843 | G_adv: 1.6146 | Sty: 0.0093 | Cyc: 0.5740\n",
            "Iter [5800/100000] Time: 3564.59s | D_loss: 0.8149 | G_adv: 1.6927 | Sty: 0.0093 | Cyc: 0.4506\n",
            "Iter [5900/100000] Time: 3626.02s | D_loss: 0.7638 | G_adv: 1.3090 | Sty: 0.0098 | Cyc: 0.4180\n",
            "Iter [6000/100000] Time: 3687.43s | D_loss: 0.7381 | G_adv: 1.0710 | Sty: 0.0092 | Cyc: 0.4560\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/006000_grid.jpg\n",
            "Iter [6100/100000] Time: 3749.44s | D_loss: 0.6813 | G_adv: 1.7843 | Sty: 0.0089 | Cyc: 0.4046\n",
            "Iter [6200/100000] Time: 3810.87s | D_loss: 1.3588 | G_adv: 0.7403 | Sty: 0.0092 | Cyc: 0.3892\n",
            "Iter [6300/100000] Time: 3872.34s | D_loss: 1.0117 | G_adv: 1.7351 | Sty: 0.0081 | Cyc: 0.3841\n",
            "Iter [6400/100000] Time: 3933.88s | D_loss: 0.7916 | G_adv: 1.6501 | Sty: 0.0094 | Cyc: 0.4187\n",
            "Iter [6500/100000] Time: 3995.45s | D_loss: 1.0578 | G_adv: 0.3667 | Sty: 0.0086 | Cyc: 0.3805\n",
            "Iter [6600/100000] Time: 4056.96s | D_loss: 0.6176 | G_adv: 0.7190 | Sty: 0.0088 | Cyc: 0.5376\n",
            "Iter [6700/100000] Time: 4118.50s | D_loss: 0.7075 | G_adv: 0.9855 | Sty: 0.0090 | Cyc: 0.5330\n",
            "Iter [6800/100000] Time: 4179.99s | D_loss: 0.3679 | G_adv: 1.4364 | Sty: 0.0101 | Cyc: 0.3754\n",
            "Iter [6900/100000] Time: 4241.43s | D_loss: 0.6419 | G_adv: 1.1167 | Sty: 0.0085 | Cyc: 0.3858\n",
            "Iter [7000/100000] Time: 4302.84s | D_loss: 0.9397 | G_adv: 1.0321 | Sty: 0.0084 | Cyc: 0.3643\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/007000_grid.jpg\n",
            "Iter [7100/100000] Time: 4364.57s | D_loss: 0.9988 | G_adv: 1.8963 | Sty: 0.0092 | Cyc: 0.4207\n",
            "Iter [7200/100000] Time: 4426.01s | D_loss: 0.7363 | G_adv: 0.7098 | Sty: 0.0088 | Cyc: 0.4096\n",
            "Iter [7300/100000] Time: 4487.58s | D_loss: 0.8654 | G_adv: 1.1380 | Sty: 0.0084 | Cyc: 0.3968\n",
            "Iter [7400/100000] Time: 4549.20s | D_loss: 0.7928 | G_adv: 1.1005 | Sty: 0.0093 | Cyc: 0.4277\n",
            "Iter [7500/100000] Time: 4610.69s | D_loss: 0.7453 | G_adv: 1.1206 | Sty: 0.0090 | Cyc: 0.4332\n",
            "Iter [7600/100000] Time: 4672.43s | D_loss: 0.5566 | G_adv: 0.9495 | Sty: 0.0091 | Cyc: 0.5040\n",
            "Iter [7700/100000] Time: 4734.02s | D_loss: 0.8935 | G_adv: 1.1217 | Sty: 0.0099 | Cyc: 0.3647\n",
            "Iter [7800/100000] Time: 4795.48s | D_loss: 0.7396 | G_adv: 1.4223 | Sty: 0.0086 | Cyc: 0.4034\n",
            "Iter [7900/100000] Time: 4857.03s | D_loss: 0.7596 | G_adv: 1.6814 | Sty: 0.0096 | Cyc: 0.3664\n",
            "Iter [8000/100000] Time: 4918.48s | D_loss: 0.9634 | G_adv: 1.3159 | Sty: 0.0089 | Cyc: 0.3847\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/008000_grid.jpg\n",
            "Iter [8100/100000] Time: 4980.22s | D_loss: 0.6911 | G_adv: 1.1810 | Sty: 0.0093 | Cyc: 0.3753\n",
            "Iter [8200/100000] Time: 5041.64s | D_loss: 0.5002 | G_adv: 1.3024 | Sty: 0.0097 | Cyc: 0.4348\n",
            "Iter [8300/100000] Time: 5103.21s | D_loss: 1.2182 | G_adv: 0.7690 | Sty: 0.0087 | Cyc: 0.4423\n",
            "Iter [8400/100000] Time: 5164.67s | D_loss: 0.7673 | G_adv: 0.7924 | Sty: 0.0090 | Cyc: 0.3384\n",
            "Iter [8500/100000] Time: 5226.06s | D_loss: 0.7827 | G_adv: 1.4714 | Sty: 0.0097 | Cyc: 0.4573\n",
            "Iter [8600/100000] Time: 5287.41s | D_loss: 1.0121 | G_adv: 1.0440 | Sty: 0.0085 | Cyc: 0.4560\n",
            "Iter [8700/100000] Time: 5348.74s | D_loss: 0.7927 | G_adv: 0.6099 | Sty: 0.0088 | Cyc: 0.3159\n",
            "Iter [8800/100000] Time: 5409.99s | D_loss: 0.8766 | G_adv: 0.9209 | Sty: 0.0100 | Cyc: 0.5521\n",
            "Iter [8900/100000] Time: 5471.24s | D_loss: 0.9884 | G_adv: 1.3002 | Sty: 0.0095 | Cyc: 0.4135\n",
            "Iter [9000/100000] Time: 5532.51s | D_loss: 1.3754 | G_adv: 0.5387 | Sty: 0.0089 | Cyc: 0.3367\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/009000_grid.jpg\n",
            "Iter [9100/100000] Time: 5594.26s | D_loss: 0.8802 | G_adv: 0.9561 | Sty: 0.0093 | Cyc: 0.3984\n",
            "Iter [9200/100000] Time: 5655.51s | D_loss: 0.6746 | G_adv: 1.3239 | Sty: 0.0084 | Cyc: 0.4770\n",
            "Iter [9300/100000] Time: 5716.76s | D_loss: 1.1386 | G_adv: 0.9649 | Sty: 0.0089 | Cyc: 0.3964\n",
            "Iter [9400/100000] Time: 5778.04s | D_loss: 0.9344 | G_adv: 0.7489 | Sty: 0.0084 | Cyc: 0.5741\n",
            "Iter [9500/100000] Time: 5839.27s | D_loss: 0.7385 | G_adv: 1.3971 | Sty: 0.0095 | Cyc: 0.3735\n",
            "Iter [9600/100000] Time: 5900.57s | D_loss: 1.2711 | G_adv: 1.3076 | Sty: 0.0092 | Cyc: 0.4296\n",
            "Iter [9700/100000] Time: 5961.88s | D_loss: 0.7072 | G_adv: 1.0610 | Sty: 0.0085 | Cyc: 0.3286\n",
            "Iter [9800/100000] Time: 6023.47s | D_loss: 0.7542 | G_adv: 1.5816 | Sty: 0.0085 | Cyc: 0.4275\n",
            "Iter [9900/100000] Time: 6085.07s | D_loss: 0.6774 | G_adv: 1.1976 | Sty: 0.0090 | Cyc: 0.3882\n",
            "Iter [10000/100000] Time: 6146.68s | D_loss: 0.8527 | G_adv: 1.3799 | Sty: 0.0099 | Cyc: 0.3901\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/010000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/checkpoints/010000.ckpt\n",
            "Iter [10100/100000] Time: 6212.90s | D_loss: 0.7257 | G_adv: 0.6941 | Sty: 0.0090 | Cyc: 0.3717\n",
            "Iter [10200/100000] Time: 6274.45s | D_loss: 0.9279 | G_adv: 0.8868 | Sty: 0.0090 | Cyc: 0.4199\n",
            "Iter [10300/100000] Time: 6335.89s | D_loss: 0.8222 | G_adv: 0.6724 | Sty: 0.0079 | Cyc: 0.4374\n",
            "Iter [10400/100000] Time: 6397.32s | D_loss: 0.5476 | G_adv: 1.2687 | Sty: 0.0090 | Cyc: 0.4790\n",
            "Iter [10500/100000] Time: 6458.61s | D_loss: 0.8746 | G_adv: 1.5533 | Sty: 0.0094 | Cyc: 0.4782\n",
            "Iter [10600/100000] Time: 6520.10s | D_loss: 1.0451 | G_adv: 0.7666 | Sty: 0.0089 | Cyc: 0.3259\n",
            "Iter [10700/100000] Time: 6581.47s | D_loss: 1.1250 | G_adv: 1.3695 | Sty: 0.0092 | Cyc: 0.5111\n",
            "Iter [10800/100000] Time: 6642.85s | D_loss: 1.0114 | G_adv: 0.9949 | Sty: 0.0094 | Cyc: 0.3191\n",
            "Iter [10900/100000] Time: 6704.29s | D_loss: 0.9536 | G_adv: 0.7182 | Sty: 0.0084 | Cyc: 0.4273\n",
            "Iter [11000/100000] Time: 6765.66s | D_loss: 0.9852 | G_adv: 0.9161 | Sty: 0.0088 | Cyc: 0.3986\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/011000_grid.jpg\n",
            "Iter [11100/100000] Time: 6827.34s | D_loss: 0.8391 | G_adv: 1.1229 | Sty: 0.0092 | Cyc: 0.3954\n",
            "Iter [11200/100000] Time: 6888.61s | D_loss: 1.4705 | G_adv: 1.0368 | Sty: 0.0090 | Cyc: 0.4630\n",
            "Iter [11300/100000] Time: 6950.09s | D_loss: 0.7219 | G_adv: 0.9712 | Sty: 0.0106 | Cyc: 0.3919\n",
            "Iter [11400/100000] Time: 7011.47s | D_loss: 0.9479 | G_adv: 1.4511 | Sty: 0.0097 | Cyc: 0.4032\n",
            "Iter [11500/100000] Time: 7072.93s | D_loss: 0.7275 | G_adv: 1.3537 | Sty: 0.0090 | Cyc: 0.4458\n",
            "Iter [11600/100000] Time: 7134.36s | D_loss: 0.8734 | G_adv: 1.4777 | Sty: 0.0098 | Cyc: 0.4867\n",
            "Iter [11700/100000] Time: 7195.77s | D_loss: 0.5771 | G_adv: 1.5284 | Sty: 0.0086 | Cyc: 0.4347\n",
            "Iter [11800/100000] Time: 7257.12s | D_loss: 0.9043 | G_adv: 0.5055 | Sty: 0.0088 | Cyc: 0.5188\n",
            "Iter [11900/100000] Time: 7318.56s | D_loss: 0.8339 | G_adv: 0.6312 | Sty: 0.0093 | Cyc: 0.3928\n",
            "Iter [12000/100000] Time: 7380.00s | D_loss: 1.0832 | G_adv: 1.2516 | Sty: 0.0091 | Cyc: 0.3008\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/012000_grid.jpg\n",
            "Iter [12100/100000] Time: 7441.84s | D_loss: 0.9923 | G_adv: 1.2361 | Sty: 0.0088 | Cyc: 0.4789\n",
            "Iter [12200/100000] Time: 7503.28s | D_loss: 0.9568 | G_adv: 1.0005 | Sty: 0.0086 | Cyc: 0.4767\n",
            "Iter [12300/100000] Time: 7564.65s | D_loss: 0.6236 | G_adv: 1.9065 | Sty: 0.0082 | Cyc: 0.5942\n",
            "Iter [12400/100000] Time: 7626.05s | D_loss: 1.2688 | G_adv: 0.6274 | Sty: 0.0087 | Cyc: 0.4318\n",
            "Iter [12500/100000] Time: 7687.59s | D_loss: 0.6701 | G_adv: 0.8678 | Sty: 0.0088 | Cyc: 0.3629\n",
            "Iter [12600/100000] Time: 7748.96s | D_loss: 0.8831 | G_adv: 1.1971 | Sty: 0.0093 | Cyc: 0.4514\n",
            "Iter [12700/100000] Time: 7810.31s | D_loss: 0.4273 | G_adv: 0.6524 | Sty: 0.0090 | Cyc: 0.4186\n",
            "Iter [12800/100000] Time: 7871.71s | D_loss: 0.8012 | G_adv: 1.0862 | Sty: 0.0084 | Cyc: 0.6489\n",
            "Iter [12900/100000] Time: 7933.02s | D_loss: 1.0012 | G_adv: 1.2537 | Sty: 0.0092 | Cyc: 0.6419\n",
            "Iter [13000/100000] Time: 7994.34s | D_loss: 0.9976 | G_adv: 0.8274 | Sty: 0.0091 | Cyc: 0.5100\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/013000_grid.jpg\n",
            "Iter [13100/100000] Time: 8055.93s | D_loss: 0.9650 | G_adv: 1.5048 | Sty: 0.0096 | Cyc: 0.5305\n",
            "Iter [13200/100000] Time: 8117.25s | D_loss: 0.8301 | G_adv: 1.1226 | Sty: 0.0082 | Cyc: 0.3933\n",
            "Iter [13300/100000] Time: 8178.61s | D_loss: 1.0898 | G_adv: 0.7783 | Sty: 0.0096 | Cyc: 0.5871\n",
            "Iter [13400/100000] Time: 8239.94s | D_loss: 0.8681 | G_adv: 1.5901 | Sty: 0.0096 | Cyc: 0.3947\n",
            "Iter [13500/100000] Time: 8301.19s | D_loss: 1.1316 | G_adv: 1.7110 | Sty: 0.0101 | Cyc: 0.5061\n",
            "Iter [13600/100000] Time: 8362.66s | D_loss: 0.8145 | G_adv: 1.3056 | Sty: 0.0094 | Cyc: 0.4967\n",
            "Iter [13700/100000] Time: 8423.97s | D_loss: 1.0592 | G_adv: 1.6744 | Sty: 0.0094 | Cyc: 0.5927\n",
            "Iter [13800/100000] Time: 8485.26s | D_loss: 0.6582 | G_adv: 0.9046 | Sty: 0.0097 | Cyc: 0.4912\n",
            "Iter [13900/100000] Time: 8546.65s | D_loss: 1.2935 | G_adv: 0.9608 | Sty: 0.0091 | Cyc: 0.3501\n",
            "Iter [14000/100000] Time: 8607.96s | D_loss: 0.9777 | G_adv: 1.3762 | Sty: 0.0094 | Cyc: 0.3637\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/014000_grid.jpg\n",
            "Iter [14100/100000] Time: 8669.60s | D_loss: 1.2088 | G_adv: 1.3880 | Sty: 0.0096 | Cyc: 0.4474\n",
            "Iter [14200/100000] Time: 8730.95s | D_loss: 0.5005 | G_adv: 1.1549 | Sty: 0.0098 | Cyc: 0.5632\n",
            "Iter [14300/100000] Time: 8792.29s | D_loss: 1.0124 | G_adv: 1.7431 | Sty: 0.0085 | Cyc: 0.5555\n",
            "Iter [14400/100000] Time: 8853.64s | D_loss: 0.6102 | G_adv: 1.6766 | Sty: 0.0091 | Cyc: 0.6494\n",
            "Iter [14500/100000] Time: 8914.95s | D_loss: 0.9108 | G_adv: 1.5713 | Sty: 0.0100 | Cyc: 0.5405\n",
            "Iter [14600/100000] Time: 8976.31s | D_loss: 0.8931 | G_adv: 0.7479 | Sty: 0.0093 | Cyc: 0.4144\n",
            "Iter [14700/100000] Time: 9037.68s | D_loss: 0.7912 | G_adv: 0.7770 | Sty: 0.0102 | Cyc: 0.3504\n",
            "Iter [14800/100000] Time: 9099.16s | D_loss: 0.9585 | G_adv: 1.2605 | Sty: 0.0096 | Cyc: 0.3544\n",
            "Iter [14900/100000] Time: 9160.57s | D_loss: 0.8019 | G_adv: 1.5504 | Sty: 0.0087 | Cyc: 0.4192\n",
            "Iter [15000/100000] Time: 9221.90s | D_loss: 1.1636 | G_adv: 1.3778 | Sty: 0.0085 | Cyc: 0.5374\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/015000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/checkpoints/015000.ckpt\n",
            "Iter [15100/100000] Time: 9284.76s | D_loss: 0.8723 | G_adv: 0.8995 | Sty: 0.0089 | Cyc: 0.3981\n",
            "Iter [15200/100000] Time: 9346.12s | D_loss: 1.1755 | G_adv: 0.6787 | Sty: 0.0093 | Cyc: 0.4687\n",
            "Iter [15300/100000] Time: 9407.44s | D_loss: 0.9457 | G_adv: 1.1404 | Sty: 0.0097 | Cyc: 0.4613\n",
            "Iter [15400/100000] Time: 9468.76s | D_loss: 1.0680 | G_adv: 1.0852 | Sty: 0.0093 | Cyc: 0.4679\n",
            "Iter [15500/100000] Time: 9530.06s | D_loss: 0.7967 | G_adv: 1.4930 | Sty: 0.0091 | Cyc: 0.5113\n",
            "Iter [15600/100000] Time: 9591.44s | D_loss: 1.0121 | G_adv: 1.3500 | Sty: 0.0094 | Cyc: 0.3923\n",
            "Iter [15700/100000] Time: 9652.78s | D_loss: 1.0565 | G_adv: 1.1961 | Sty: 0.0086 | Cyc: 0.4893\n",
            "Iter [15800/100000] Time: 9714.07s | D_loss: 1.1977 | G_adv: 0.8966 | Sty: 0.0088 | Cyc: 0.4659\n",
            "Iter [15900/100000] Time: 9775.32s | D_loss: 1.2206 | G_adv: 0.8280 | Sty: 0.0098 | Cyc: 0.4310\n",
            "Iter [16000/100000] Time: 9836.67s | D_loss: 0.8903 | G_adv: 1.1750 | Sty: 0.0090 | Cyc: 0.4806\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/016000_grid.jpg\n",
            "Iter [16100/100000] Time: 9898.26s | D_loss: 0.8351 | G_adv: 0.8225 | Sty: 0.0083 | Cyc: 0.3106\n",
            "Iter [16200/100000] Time: 9959.53s | D_loss: 0.9379 | G_adv: 0.6628 | Sty: 0.0084 | Cyc: 0.5126\n",
            "Iter [16300/100000] Time: 10020.81s | D_loss: 0.7902 | G_adv: 0.8825 | Sty: 0.0093 | Cyc: 0.5727\n",
            "Iter [16400/100000] Time: 10082.16s | D_loss: 0.8441 | G_adv: 0.8421 | Sty: 0.0089 | Cyc: 0.4540\n",
            "Iter [16500/100000] Time: 10143.47s | D_loss: 0.7836 | G_adv: 1.4475 | Sty: 0.0092 | Cyc: 0.3545\n",
            "Iter [16600/100000] Time: 10204.81s | D_loss: 0.7400 | G_adv: 1.2574 | Sty: 0.0086 | Cyc: 0.6036\n",
            "Iter [16700/100000] Time: 10266.10s | D_loss: 0.8070 | G_adv: 0.2107 | Sty: 0.0082 | Cyc: 0.4044\n",
            "Iter [16800/100000] Time: 10327.52s | D_loss: 0.8463 | G_adv: 1.1796 | Sty: 0.0093 | Cyc: 0.4565\n",
            "Iter [16900/100000] Time: 10388.79s | D_loss: 0.9274 | G_adv: 1.5693 | Sty: 0.0088 | Cyc: 0.3752\n",
            "Iter [17000/100000] Time: 10450.07s | D_loss: 0.8009 | G_adv: 0.6897 | Sty: 0.0090 | Cyc: 0.5587\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/017000_grid.jpg\n",
            "Iter [17100/100000] Time: 10511.56s | D_loss: 0.7615 | G_adv: 0.9499 | Sty: 0.0086 | Cyc: 0.5326\n",
            "Iter [17200/100000] Time: 10573.00s | D_loss: 0.8349 | G_adv: 0.8289 | Sty: 0.0090 | Cyc: 0.3596\n",
            "Iter [17300/100000] Time: 10634.51s | D_loss: 0.9269 | G_adv: 0.8457 | Sty: 0.0085 | Cyc: 0.5399\n",
            "Iter [17400/100000] Time: 10695.93s | D_loss: 0.7338 | G_adv: 0.8287 | Sty: 0.0085 | Cyc: 0.5279\n",
            "Iter [17500/100000] Time: 10757.41s | D_loss: 0.9115 | G_adv: 0.7901 | Sty: 0.0095 | Cyc: 0.3796\n",
            "Iter [17600/100000] Time: 10818.92s | D_loss: 0.8837 | G_adv: 1.1292 | Sty: 0.0093 | Cyc: 0.4610\n",
            "Iter [17700/100000] Time: 10880.25s | D_loss: 0.7848 | G_adv: 1.3609 | Sty: 0.0089 | Cyc: 0.3685\n",
            "Iter [17800/100000] Time: 10941.72s | D_loss: 0.7320 | G_adv: 1.4860 | Sty: 0.0094 | Cyc: 0.4634\n",
            "Iter [17900/100000] Time: 11003.26s | D_loss: 0.9962 | G_adv: 1.4045 | Sty: 0.0094 | Cyc: 0.4764\n",
            "Iter [18000/100000] Time: 11064.69s | D_loss: 0.8109 | G_adv: 1.4412 | Sty: 0.0084 | Cyc: 0.4433\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/018000_grid.jpg\n",
            "Iter [18100/100000] Time: 11126.86s | D_loss: 0.9352 | G_adv: 1.3547 | Sty: 0.0090 | Cyc: 0.3927\n",
            "Iter [18200/100000] Time: 11188.34s | D_loss: 1.0453 | G_adv: 1.2209 | Sty: 0.0078 | Cyc: 0.4556\n",
            "Iter [18300/100000] Time: 11249.78s | D_loss: 0.8939 | G_adv: 0.7364 | Sty: 0.0084 | Cyc: 0.4387\n",
            "Iter [18400/100000] Time: 11311.14s | D_loss: 0.8661 | G_adv: 1.3113 | Sty: 0.0094 | Cyc: 0.5531\n",
            "Iter [18500/100000] Time: 11372.39s | D_loss: 0.9703 | G_adv: 1.1951 | Sty: 0.0091 | Cyc: 0.5537\n",
            "Iter [18600/100000] Time: 11433.61s | D_loss: 1.1199 | G_adv: 0.7662 | Sty: 0.0083 | Cyc: 0.4137\n",
            "Iter [18700/100000] Time: 11494.82s | D_loss: 0.8470 | G_adv: 0.7951 | Sty: 0.0089 | Cyc: 0.5720\n",
            "Iter [18800/100000] Time: 11556.20s | D_loss: 1.2421 | G_adv: 0.9562 | Sty: 0.0093 | Cyc: 0.3932\n",
            "Iter [18900/100000] Time: 11617.42s | D_loss: 1.1433 | G_adv: 1.2479 | Sty: 0.0085 | Cyc: 0.4862\n",
            "Iter [19000/100000] Time: 11678.64s | D_loss: 0.7790 | G_adv: 1.4544 | Sty: 0.0097 | Cyc: 0.4121\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/019000_grid.jpg\n",
            "Iter [19100/100000] Time: 11740.14s | D_loss: 0.9029 | G_adv: 1.6568 | Sty: 0.0090 | Cyc: 0.5440\n",
            "Iter [19200/100000] Time: 11801.37s | D_loss: 0.8723 | G_adv: 1.6637 | Sty: 0.0093 | Cyc: 0.5268\n",
            "Iter [19300/100000] Time: 11862.61s | D_loss: 0.8896 | G_adv: 1.4700 | Sty: 0.0093 | Cyc: 0.4525\n",
            "Iter [19400/100000] Time: 11923.85s | D_loss: 1.0934 | G_adv: 0.5674 | Sty: 0.0094 | Cyc: 0.4377\n",
            "Iter [19500/100000] Time: 11985.07s | D_loss: 1.0290 | G_adv: 0.7294 | Sty: 0.0085 | Cyc: 0.5973\n",
            "Iter [19600/100000] Time: 12046.45s | D_loss: 0.9638 | G_adv: 0.7071 | Sty: 0.0092 | Cyc: 0.5235\n",
            "Iter [19700/100000] Time: 12107.66s | D_loss: 0.9140 | G_adv: 0.7805 | Sty: 0.0094 | Cyc: 0.4763\n",
            "Iter [19800/100000] Time: 12168.91s | D_loss: 0.9119 | G_adv: 0.7314 | Sty: 0.0099 | Cyc: 0.5605\n",
            "Iter [19900/100000] Time: 12230.18s | D_loss: 1.4633 | G_adv: 0.6530 | Sty: 0.0087 | Cyc: 0.5362\n",
            "Iter [20000/100000] Time: 12291.50s | D_loss: 1.4505 | G_adv: 0.4835 | Sty: 0.0093 | Cyc: 0.3918\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/020000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/checkpoints/020000.ckpt\n",
            "Iter [20100/100000] Time: 12354.28s | D_loss: 0.8493 | G_adv: 1.1866 | Sty: 0.0088 | Cyc: 0.4499\n",
            "Iter [20200/100000] Time: 12415.54s | D_loss: 1.1080 | G_adv: 1.0494 | Sty: 0.0093 | Cyc: 0.4229\n",
            "Iter [20300/100000] Time: 12476.95s | D_loss: 1.1690 | G_adv: 1.0654 | Sty: 0.0095 | Cyc: 0.3425\n",
            "Iter [20400/100000] Time: 12538.31s | D_loss: 0.7178 | G_adv: 0.9738 | Sty: 0.0087 | Cyc: 0.4227\n",
            "Iter [20500/100000] Time: 12599.60s | D_loss: 0.8715 | G_adv: 0.9612 | Sty: 0.0089 | Cyc: 0.5649\n",
            "Iter [20600/100000] Time: 12660.89s | D_loss: 1.2764 | G_adv: 1.0188 | Sty: 0.0091 | Cyc: 0.3369\n",
            "Iter [20700/100000] Time: 12722.15s | D_loss: 0.6892 | G_adv: 1.5612 | Sty: 0.0094 | Cyc: 0.4911\n",
            "Iter [20800/100000] Time: 12783.44s | D_loss: 0.8341 | G_adv: 0.9757 | Sty: 0.0089 | Cyc: 0.4988\n",
            "Iter [20900/100000] Time: 12844.69s | D_loss: 0.7306 | G_adv: 0.5373 | Sty: 0.0080 | Cyc: 0.4180\n",
            "Iter [21000/100000] Time: 12905.95s | D_loss: 0.7472 | G_adv: 0.7694 | Sty: 0.0086 | Cyc: 0.4682\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3/samples/021000_grid.jpg\n",
            "Iter [21100/100000] Time: 12967.59s | D_loss: 0.8975 | G_adv: 0.6936 | Sty: 0.0088 | Cyc: 0.3769\n",
            "Iter [21200/100000] Time: 13028.97s | D_loss: 0.7650 | G_adv: 0.9266 | Sty: 0.0100 | Cyc: 0.4321\n",
            "Iter [21300/100000] Time: 13090.25s | D_loss: 0.9238 | G_adv: 0.6389 | Sty: 0.0091 | Cyc: 0.5134\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-269401278.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# Solver 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-269401278.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# 기존 설정과 모델 불러오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder\n",
        "\n",
        "def load_model(args, device, checkpoint_step=None):\n",
        "    \"\"\"저장된 체크포인트를 불러옵니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "\n",
        "    # 체크포인트 지정이 없으면 가장 마지막(최신) 파일 로드\n",
        "    if checkpoint_step is None:\n",
        "        ckpts = sorted([f for f in os.listdir(model_path) if f.endswith('.ckpt')])\n",
        "        if not ckpts:\n",
        "            raise FileNotFoundError(\"체크포인트가 없습니다!\")\n",
        "        latest_ckpt = ckpts[-1]\n",
        "    else:\n",
        "        latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 모델 초기화 및 가중치 로드\n",
        "    generator = Generator(args.img_size, args.style_dim).to(device)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device)\n",
        "    # Style Encoder는 Reference Guided Synthesis 할 때 필요 (여기서는 Latent Guided만 시연)\n",
        "\n",
        "    generator.load_state_dict(ckpt['nets']['G'])\n",
        "    mapping_network.load_state_dict(ckpt['nets']['F'])\n",
        "\n",
        "    generator.eval()\n",
        "    mapping_network.eval()\n",
        "\n",
        "    return generator, mapping_network\n",
        "\n",
        "def inference(args, device):\n",
        "    # 1. 모델 로드\n",
        "    generator, mapping_net = load_model(args, device)\n",
        "\n",
        "    # 2. 테스트 데이터 로드 (학습에 안 쓴 데이터)\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True) # 10장만 샘플링\n",
        "\n",
        "    # 3. 소스 이미지 가져오기\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    # 4. 시각화 준비\n",
        "    domain_labels = get_domain_labels()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(11, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    # 고정된 Random Noise z 하나를 모든 도메인에 적용해 봅니다.\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains): # 0~9 (각 도메인별)\n",
        "        # 해당 도메인(row_idx)의 스타일 코드 생성\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = mapping_net(z_trg, y_trg) # (1, style_dim)\n",
        "\n",
        "        # 스타일 코드를 배치 크기만큼 복사 (1 -> 10)\n",
        "        s_trg = s_trg.repeat(10, 1)\n",
        "\n",
        "        # 이미지 생성\n",
        "        with torch.no_grad():\n",
        "            x_fake = generator(x_real, s_trg)\n",
        "\n",
        "        # 결과 출력\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(11, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 왼쪽 첫 열에만 도메인 이름 표시\n",
        "            if col_idx == 0:\n",
        "                plt.text(-10, 32, domain_labels[row_idx], fontsize=10, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "if __name__ == '__main__':\n",
        "    config = get_config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inference(config, device)"
      ],
      "metadata": {
        "id": "7DKw-bIshf8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}