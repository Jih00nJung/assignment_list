{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+MTSlRRvU2oK0Fqx3yanP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/GAN_FMNIST_sepa_stop1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. config.py (설정 및 환경 준비)"
      ],
      "metadata": {
        "id": "VwJWmulsWjmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr3U21KWYwp",
        "outputId": "abeb9fd5-437c-4f01-a0a8-42d40c429b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "import argparse\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"학습에 필요한 모든 하이퍼파라미터를 정의합니다.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # 데이터 및 경로 설정\n",
        "    parser.add_argument('--project_name', type=str, default='GAN_FMNIST_v2')\n",
        "    parser.add_argument('--save_root', type=str, default='/content/drive/MyDrive/Colab Notebooks/GAN_assignment')\n",
        "    parser.add_argument('--img_size', type=int, default=64, help='이미지 크기 (FMNIST 기본 28 -> 64 리사이즈)')\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "\n",
        "    # 모델 하이퍼파라미터\n",
        "    parser.add_argument('--style_dim', type=int, default=64, help='스타일 코드 차원')\n",
        "    parser.add_argument('--latent_dim', type=int, default=16, help='랜덤 노이즈 차원')\n",
        "    parser.add_argument('--num_domains', type=int, default=10, help='Fashion MNIST 클래스 개수')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=256, help='Mapping Network 히든 차원')\n",
        "\n",
        "    # 학습 설정\n",
        "    parser.add_argument('--total_iters', type=int, default=100000)\n",
        "    parser.add_argument('--resume_iter', type=int, default=0)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--w_hpf', type=float, default=1, help='High-pass filtering 가중치')\n",
        "\n",
        "    # Loss 가중치\n",
        "    parser.add_argument('--lambda_sty', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_ds', type=float, default=2.0)\n",
        "    parser.add_argument('--lambda_cyc', type=float, default=0)\n",
        "\n",
        "    # 로깅 주기\n",
        "    parser.add_argument('--sample_freq', type=int, default=1000)\n",
        "    parser.add_argument('--save_freq', type=int, default=5000)\n",
        "\n",
        "    # FID/LPIPS 평가용 설정\n",
        "    parser.add_argument('--num_fid_samples', type=int, default=1000, help='FID 계산에 사용할 생성 이미지 수')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def prepare_environment(args):\n",
        "    \"\"\"Google Drive 마운트 및 체크포인트/샘플 디렉토리를 생성합니다.\"\"\"\n",
        "    print(\"--- 환경 설정 중 ---\")\n",
        "    save_path = os.path.join(args.save_root, args.project_name)\n",
        "\n",
        "    # Google Drive 마운트\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive를 마운트합니다...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted.\")\n",
        "\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_path, 'samples'), exist_ok=True)\n",
        "    print(f\"저장 경로: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def get_data_transform(img_size):\n",
        "    \"\"\"Fashion MNIST 데이터 전처리를 정의합니다.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # [0,1] -> [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_domain_labels():\n",
        "    \"\"\"Fashion MNIST의 10개 도메인 레이블을 반환합니다.\"\"\"\n",
        "    return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. model.py (네트워크 아키텍처)"
      ],
      "metadata": {
        "id": "ywi2i9fZWl3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        # 1. 정규화 도구 (학습 파라미터 없음, 단순 통계 정규화)\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        # 2. 스타일 코드 s를 변환하여 감마(스케일)와 베타(시프트)를 만드는 선형 층\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        # s를 통해 파라미터 생성 (h)\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        # 생성된 파라미터를 감마와 베타로 나눔\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        # 정규화된 x에 감마를 곱하고 베타를 더함 -> 스타일 주입\n",
        "        return (1 + gamma) * self.norm(x) + beta\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"기본 ResBlock (다운샘플링 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            actv,\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            actv,\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "class AdaINResBlock(nn.Module):\n",
        "    \"\"\"Generator용 AdaIN ResBlock (Bottleneck 및 Up-sampling 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, style_dim, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self.norm1(x, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + self.shortcut(x)\n",
        "\n",
        "# --- (1) Generator (G) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, max_conv_dim=256):\n",
        "        super().__init__()\n",
        "        dim_in = 32  # 경량화를 위한 시작 필터 수\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # 1. 입력부: 흑백(1채널) 이미지를 받아서 32채널 특징 맵으로 변환\n",
        "        self.from_rgb = nn.Conv2d(1, dim_in, 3, 1, 1) # Grayscale 1채널 입력\n",
        "\n",
        "        # 2. 인코더 (Down-sampling): 형태 정보 압축\n",
        "        # Down-sampling blocks (64 -> 32 -> 16 -> 8)\n",
        "        self.encode = nn.ModuleList()\n",
        "        curr_dim = dim_in\n",
        "        for _ in range(3): # 3번 다운샘플링하여 8x8 병목 생성\n",
        "            self.encode.append(ResBlock(curr_dim, curr_dim * 2))\n",
        "            self.encode.append(nn.AvgPool2d(2))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # 3. 병목 (Bottleneck): 스타일 주입 시작\n",
        "        # Bottleneck (8x8 유지, AdaIN 적용)\n",
        "        self.decode = nn.ModuleList()\n",
        "        curr_dim = min(curr_dim, max_conv_dim)\n",
        "        for _ in range(2):\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim, style_dim))\n",
        "\n",
        "        # 4. 디코더 (Up-sampling): 이미지 복원 + 스타일 입히기\n",
        "        # Up-sampling blocks (8 -> 16 -> 32 -> 64)\n",
        "        for _ in range(3):\n",
        "            self.decode.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim // 2, style_dim))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        # 5. 출력부: 최종적으로 1채널(흑백) 이미지로 변환\n",
        "        # Final Conv\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(curr_dim, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(curr_dim, 1, 1, 1, 0) # Grayscale 1채널 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.from_rgb(x)\n",
        "        for block in self.encode:\n",
        "            x = block(x)\n",
        "\n",
        "        for block in self.decode:\n",
        "            if isinstance(block, AdaINResBlock):\n",
        "                x = block(x, s)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "# --- (2) Mapping Network (F) ---\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        # 공유 레이어 (Shared)\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(latent_dim if not layers else hidden_dim, hidden_dim)]\n",
        "            layers += [nn.ReLU()]\n",
        "        # 1. 공유 레이어 (Shared): 모든 도메인이 공통으로 사용하는 특징 추출\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        # 2. 비공유 레이어 (Unshared): 각 도메인(T-shirt, Pants...)별 전용 스타일 생성기\n",
        "        # 도메인별 출력 레이어 (Unshared)\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, style_dim)\n",
        "            ))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, style_dim)\n",
        "\n",
        "        # 사용자가 요청한 도메인(y)에 해당하는 스타일만 쏙 뽑아서 리턴\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y] # 해당 도메인의 스타일 코드만 선택\n",
        "        return s\n",
        "\n",
        "# --- (3) Style Encoder (E) ---\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지를 보며 특징을 추출 (CNN 구조)\n",
        "        # 64 -> 32 -> 16 -> 8 로 줄어들며 추상적인 특징을 잡아냄\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 비공유 레이어: 추출된 특징을 보고 \"이건 바지 스타일로는 s_pants, 티셔츠로는 s_shirt야\" 라고 해석\n",
        "        # 도메인별 Style Code 출력\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, style_dim))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # 이미지 x에서 시각적 특징 추출\n",
        "        h = self.shared(x) # (batch, curr_dim, 1, 1)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n",
        "\n",
        "# --- (4) Discriminator (D) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지가 진짜인지 가짜인지 판단하기 위한 단서(특징) 추출\n",
        "        # ResBlock이나 Conv 레이어를 사용하여 이미지를 분석함\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 멀티 태스크 헤드: 각 도메인별로 진위 여부를 따로 판별\n",
        "        # 도메인별 진위 판별 헤드\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, 1))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, 1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        score = out[idx, y]\n",
        "        return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ozdU0eWnJ3",
        "outputId": "1fcfa1a6-847e-4c9b-a8fd-3622f1ca01af"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. solver.py (메인 실행 및 학습 루프)"
      ],
      "metadata": {
        "id": "m__DYADCWoiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, prepare_environment, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "class Solver:\n",
        "    def __init__(self, args, device):\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.save_dir = prepare_environment(args)\n",
        "        self.domain_labels = get_domain_labels()\n",
        "\n",
        "        # 데이터셋 준비 (FashionMNIST)\n",
        "        transform = get_data_transform(args.img_size)\n",
        "\n",
        "        # 학습용 데이터\n",
        "        # Colab에서 데이터셋을 로드하므로 root 경로를 현재 위치로 지정\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        self.loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "        # 평가용 데이터 로더 (FID/LPIPS 계산 시 사용될 예정)\n",
        "        self.eval_loader = self.get_eval_loader(transform)\n",
        "\n",
        "        # 모델 초기화\n",
        "        self.nets = {\n",
        "            'G': Generator(args.img_size, args.style_dim),\n",
        "            'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim),\n",
        "            'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains),\n",
        "            'D': Discriminator(args.img_size, args.num_domains)\n",
        "        }\n",
        "\n",
        "        for name, module in self.nets.items():\n",
        "            module.to(self.device)\n",
        "            # He Initialization (논문 권장)은 PyTorch 기본 설정과 유사하므로 생략하거나,\n",
        "            # torch.nn.init.kaiming_normal_ 등을 사용할 수 있습니다. 여기서는 간단히 기본값 사용.\n",
        "            module.train()\n",
        "\n",
        "        # 옵티마이저\n",
        "        self.optims = {\n",
        "            'G': torch.optim.Adam(self.nets['G'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'F': torch.optim.Adam(self.nets['F'].parameters(), lr=args.lr*0.01, betas=(0.0, 0.99)), # F는 낮은 LR 사용\n",
        "            'E': torch.optim.Adam(self.nets['E'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'D': torch.optim.Adam(self.nets['D'].parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "        }\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        self.start_iter = 0\n",
        "        if args.resume_iter > 0:\n",
        "            self.load_checkpoint(args.resume_iter)\n",
        "            self.start_iter = args.resume_iter\n",
        "\n",
        "    def get_eval_loader(self, transform): # NEW: 평가용 로더 함수\n",
        "        \"\"\"FID/LPIPS 계산을 위한 평가용 데이터 로더를 준비합니다.\"\"\"\n",
        "        # 테스트 셋을 사용하며, 셔플은 하지 않습니다.\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=self.args.batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "    def save_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        state = {\n",
        "            'nets': {name: net.state_dict() for name, net in self.nets.items()},\n",
        "            'optims': {name: opt.state_dict() for name, opt in self.optims.items()},\n",
        "            'step': step\n",
        "        }\n",
        "        torch.save(state, path)\n",
        "        print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Checkpoint not found!\")\n",
        "            return\n",
        "\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        for name, net in self.nets.items():\n",
        "            net.load_state_dict(ckpt['nets'][name])\n",
        "        for name, opt in self.optims.items():\n",
        "            opt.load_state_dict(ckpt['optims'][name])\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "    def calculate_metrics(self, step): # NEW: 메트릭 계산 Placeholder 함수\n",
        "        \"\"\"\n",
        "        FID 및 LPIPS와 같은 정량적 평가지표를 계산합니다.\n",
        "\n",
        "        NOTE: 실제 계산을 위해서는 'pytorch-fid' 또는 'piq'와 같은 라이브러리가\n",
        "        별도로 설치되어야 하며, InceptionV3 모델이 로드되어야 합니다.\n",
        "        여기서는 계산 로직 대신 출력 메시지만 제공합니다.\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Iteration {step}: Evaluating Metrics ---\")\n",
        "\n",
        "        # 1. FID Score (가짜 이미지 분포 vs 진짜 이미지 분포)\n",
        "        fid_score = 99.99 # Placeholder Value (실제 계산 필요)\n",
        "        print(f\"FID Score: {fid_score:.4f} (낮을수록 좋음)\")\n",
        "\n",
        "        # 2. LPIPS Score (다양성 측정)\n",
        "        lpips_score = 0.00 # Placeholder Value (실제 계산 필요)\n",
        "        print(f\"LPIPS Diversity Score: {lpips_score:.4f} (높을수록 좋음)\")\n",
        "\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "    def train(self):\n",
        "        print(\"--- 학습 시작 ---\")\n",
        "        nets = self.nets\n",
        "        optims = self.optims\n",
        "        args = self.args\n",
        "\n",
        "        data_iter = iter(self.loader)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for i in range(self.start_iter, args.total_iters):\n",
        "\n",
        "            # 1. 데이터 가져오기\n",
        "            try:\n",
        "                x_real, y_org = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(self.loader)\n",
        "                x_real, y_org = next(data_iter)\n",
        "\n",
        "            x_real = x_real.to(self.device)\n",
        "            y_org = y_org.to(self.device)\n",
        "\n",
        "            # 타겟 도메인 및 Latent 생성\n",
        "            y_trg = torch.randint(0, args.num_domains, (x_real.size(0),)).to(self.device)\n",
        "            z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "            z_trg2 = torch.randn(x_real.size(0), args.latent_dim).to(self.device) # Diversity Loss용\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                               1. Discriminator 학습                                 #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Real Loss (Hinge Loss)\n",
        "            d_out_real = nets['D'](x_real, y_org)\n",
        "            d_loss_real = torch.mean(F.relu(1.0 - d_out_real))\n",
        "\n",
        "            # Fake Loss (Latent 기반 생성)\n",
        "            with torch.no_grad():\n",
        "                s_trg = nets['F'](z_trg, y_trg)\n",
        "                x_fake = nets['G'](x_real, s_trg)\n",
        "\n",
        "            d_out_fake = nets['D'](x_fake.detach(), y_trg)\n",
        "            d_loss_fake = torch.mean(F.relu(1.0 + d_out_fake))\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            optims['D'].zero_grad()\n",
        "            d_loss.backward()\n",
        "            optims['D'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                     2. Generator, Mapping, Encoder 학습                             #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # Adversarial Loss\n",
        "            s_trg = nets['F'](z_trg, y_trg)\n",
        "            x_fake = nets['G'](x_real, s_trg)\n",
        "            d_out_fake = nets['D'](x_fake, y_trg)\n",
        "            g_loss_adv = -torch.mean(d_out_fake)\n",
        "\n",
        "            # Style Reconstruction Loss\n",
        "            # $\\mathcal{L}_{sty}=\\mathbb{E}_{x,\\overline{y},z}[||\\tilde{s}-E_{\\overline{y}}(G(x,\\tilde{s}))||_{1}]$\n",
        "            s_pred = nets['E'](x_fake, y_trg)\n",
        "            g_loss_sty = torch.mean(torch.abs(s_trg - s_pred))\n",
        "\n",
        "            # Diversity Sensitive Loss\n",
        "            # $\\mathcal{L}_{ds}=\\mathbb{\\mathbb{E}}_{x,\\overline{y},z_{1},z_{2}}[||G(x,\\overline{s}_{1})-G(x,\\overline{s}_{2})||_{1}]$\n",
        "            s_trg2 = nets['F'](z_trg2, y_trg)\n",
        "            x_fake2 = nets['G'](x_real, s_trg2)\n",
        "            g_loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            # $\\mathcal{L}_{cyc}=\\mathbb{E}_{x,y,\\overline{y},z}[||x-G(G(x,\\tilde{s}),\\hat{s})||_{1}]$\n",
        "            s_org = nets['E'](x_real, y_org)\n",
        "            x_rec = nets['G'](x_fake, s_org)\n",
        "            g_loss_cyc = torch.mean(torch.abs(x_real - x_rec))\n",
        "\n",
        "            # Total Loss\n",
        "            g_loss = g_loss_adv \\\n",
        "                     + args.lambda_sty * g_loss_sty \\\n",
        "                     - args.lambda_ds * g_loss_ds \\\n",
        "                     + args.lambda_cyc * g_loss_cyc\n",
        "\n",
        "            optims['G'].zero_grad()\n",
        "            optims['F'].zero_grad()\n",
        "            optims['E'].zero_grad()\n",
        "            g_loss.backward()\n",
        "            optims['G'].step()\n",
        "            optims['F'].step()\n",
        "            optims['E'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 3. 로깅 및 저장                                     #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Iter [{i+1}/{args.total_iters}] Time: {elapsed:.2f}s | \"\n",
        "                      f\"D_loss: {d_loss.item():.4f} | G_adv: {g_loss_adv.item():.4f} | \"\n",
        "                      f\"Sty: {g_loss_sty.item():.4f} | Cyc: {g_loss_cyc.item():.4f}\")\n",
        "\n",
        "            if (i + 1) % args.sample_freq == 0:\n",
        "                self.save_samples(x_real, y_org, i + 1)\n",
        "\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                self.save_checkpoint(i + 1)\n",
        "                # self.calculate_metrics(i + 1) # 메트릭 계산 (필요 시 주석 해제)\n",
        "\n",
        "    def save_samples(self, x_real, y_org, step):\n",
        "        \"\"\"학습 중간 결과 이미지 저장 (Latent-guided synthesis)\"\"\"\n",
        "        nets = self.nets\n",
        "        args = self.args\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nets['G'].eval()\n",
        "            nets['F'].eval()\n",
        "\n",
        "            # 소스 이미지 서브셋 선택 (10x10 그리드 출력을 위해)\n",
        "            x_real_subset = x_real[:args.num_domains].to(self.device)\n",
        "            y_org_subset = y_org[:args.num_domains].cpu().numpy()\n",
        "\n",
        "            # 고정된 z와 다양한 y로 스타일 생성 (Style-Mixing 효과 확인을 위함)\n",
        "            z_fix = torch.randn(1, args.latent_dim).repeat(args.num_domains, 1).to(self.device)\n",
        "            y_fix = torch.arange(args.num_domains).to(self.device)\n",
        "            s_fix = nets['F'](z_fix, y_fix) # (10, style_dim)\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # 1. 첫 번째 행: 소스 이미지\n",
        "            # ERROR FIX: GPU에 있는 텐서를 CPU로 옮깁니다.\n",
        "            source_row = [x_real_subset[i].cpu() for i in range(len(x_real_subset))] # .cpu() 추가\n",
        "            images.extend(source_row)\n",
        "\n",
        "            # 2. 나머지 영역: 변환된 이미지 (스타일 변환 매트릭스)\n",
        "            for j in range(args.num_domains): # 타겟 스타일 (Row)\n",
        "                s_curr = s_fix[j].unsqueeze(0).repeat(x_real_subset.size(0), 1)\n",
        "                x_fake_row = nets['G'](x_real_subset, s_curr)\n",
        "                # 이미 .cpu()가 호출되고 있으므로 문제 없음\n",
        "                images.extend([x_fake_row[i].cpu() for i in range(len(x_real_subset))])\n",
        "\n",
        "            images = torch.stack(images, dim=0)\n",
        "\n",
        "            path = os.path.join(self.save_dir, 'samples', f'{step:06d}_grid.jpg')\n",
        "            save_image(images, path, nrow=len(x_real_subset), padding=2, normalize=True)\n",
        "            print(f\"Sample image grid saved to {path}\")\n",
        "\n",
        "        # 다시 학습 모드\n",
        "        nets['G'].train()\n",
        "        nets['F'].train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 시드 고정 (재현성)\n",
        "    torch.manual_seed(777)\n",
        "    np.random.seed(777)\n",
        "\n",
        "    # 설정 로드\n",
        "    config = get_config()\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Solver 시작\n",
        "    solver = Solver(config, device)\n",
        "    solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3gRsH6MWpyA",
        "outputId": "f554a80c-81ae-4a61-b13e-ed79bbdfb4e7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- 환경 설정 중 ---\n",
            "저장 경로: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2\n",
            "--- 학습 시작 ---\n",
            "Iter [100/100000] Time: 16.51s | D_loss: 0.8700 | G_adv: 1.6346 | Sty: 0.0082 | Cyc: 0.6486\n",
            "Iter [200/100000] Time: 32.68s | D_loss: 0.4034 | G_adv: 1.8741 | Sty: 0.0083 | Cyc: 0.4618\n",
            "Iter [300/100000] Time: 48.87s | D_loss: 0.7617 | G_adv: 1.8423 | Sty: 0.0091 | Cyc: 0.5410\n",
            "Iter [400/100000] Time: 65.73s | D_loss: 0.9533 | G_adv: 1.2499 | Sty: 0.0084 | Cyc: 0.4995\n",
            "Iter [500/100000] Time: 82.20s | D_loss: 0.5824 | G_adv: 1.8717 | Sty: 0.0089 | Cyc: 0.4203\n",
            "Iter [600/100000] Time: 98.80s | D_loss: 0.8437 | G_adv: 1.8083 | Sty: 0.0090 | Cyc: 0.4460\n",
            "Iter [700/100000] Time: 115.34s | D_loss: 0.6969 | G_adv: 1.9620 | Sty: 0.0082 | Cyc: 0.4893\n",
            "Iter [800/100000] Time: 131.76s | D_loss: 0.4959 | G_adv: 1.4990 | Sty: 0.0088 | Cyc: 0.3624\n",
            "Iter [900/100000] Time: 148.10s | D_loss: 0.6207 | G_adv: 1.3709 | Sty: 0.0082 | Cyc: 0.4099\n",
            "Iter [1000/100000] Time: 164.55s | D_loss: 1.2683 | G_adv: 1.3334 | Sty: 0.0082 | Cyc: 0.3595\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/001000_grid.jpg\n",
            "Iter [1100/100000] Time: 181.09s | D_loss: 1.5990 | G_adv: 0.7713 | Sty: 0.0091 | Cyc: 0.3965\n",
            "Iter [1200/100000] Time: 197.53s | D_loss: 0.9783 | G_adv: 1.1636 | Sty: 0.0088 | Cyc: 0.3354\n",
            "Iter [1300/100000] Time: 213.97s | D_loss: 1.1704 | G_adv: 1.6150 | Sty: 0.0082 | Cyc: 0.3796\n",
            "Iter [1400/100000] Time: 230.38s | D_loss: 1.7703 | G_adv: 0.9238 | Sty: 0.0086 | Cyc: 0.3055\n",
            "Iter [1500/100000] Time: 246.77s | D_loss: 1.3069 | G_adv: 1.0553 | Sty: 0.0088 | Cyc: 0.3306\n",
            "Iter [1600/100000] Time: 263.14s | D_loss: 1.7409 | G_adv: -0.4272 | Sty: 0.0083 | Cyc: 0.3789\n",
            "Iter [1700/100000] Time: 279.55s | D_loss: 1.2216 | G_adv: 0.9306 | Sty: 0.0083 | Cyc: 0.3490\n",
            "Iter [1800/100000] Time: 295.95s | D_loss: 1.6280 | G_adv: 0.6921 | Sty: 0.0088 | Cyc: 0.3486\n",
            "Iter [1900/100000] Time: 312.33s | D_loss: 1.6783 | G_adv: 0.9644 | Sty: 0.0082 | Cyc: 0.3469\n",
            "Iter [2000/100000] Time: 328.73s | D_loss: 1.4292 | G_adv: 1.2681 | Sty: 0.0081 | Cyc: 0.4241\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/002000_grid.jpg\n",
            "Iter [2100/100000] Time: 345.23s | D_loss: 1.2898 | G_adv: 0.8572 | Sty: 0.0084 | Cyc: 0.3770\n",
            "Iter [2200/100000] Time: 361.84s | D_loss: 0.8448 | G_adv: 1.6957 | Sty: 0.0084 | Cyc: 0.3176\n",
            "Iter [2300/100000] Time: 378.36s | D_loss: 1.1063 | G_adv: 1.0140 | Sty: 0.0083 | Cyc: 0.3683\n",
            "Iter [2400/100000] Time: 394.78s | D_loss: 1.6240 | G_adv: 0.1730 | Sty: 0.0080 | Cyc: 0.3522\n",
            "Iter [2500/100000] Time: 411.21s | D_loss: 1.5582 | G_adv: 2.4868 | Sty: 0.0088 | Cyc: 0.3093\n",
            "Iter [2600/100000] Time: 427.68s | D_loss: 1.2149 | G_adv: 0.3738 | Sty: 0.0080 | Cyc: 0.2780\n",
            "Iter [2700/100000] Time: 444.16s | D_loss: 0.8916 | G_adv: 1.9928 | Sty: 0.0079 | Cyc: 0.4135\n",
            "Iter [2800/100000] Time: 460.63s | D_loss: 1.3745 | G_adv: 1.3472 | Sty: 0.0083 | Cyc: 0.3255\n",
            "Iter [2900/100000] Time: 477.04s | D_loss: 0.7886 | G_adv: 1.3190 | Sty: 0.0085 | Cyc: 0.2758\n",
            "Iter [3000/100000] Time: 493.50s | D_loss: 1.5235 | G_adv: 2.3437 | Sty: 0.0089 | Cyc: 0.3939\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/003000_grid.jpg\n",
            "Iter [3100/100000] Time: 510.10s | D_loss: 1.8463 | G_adv: 0.8442 | Sty: 0.0091 | Cyc: 0.3024\n",
            "Iter [3200/100000] Time: 526.58s | D_loss: 1.2886 | G_adv: 1.0963 | Sty: 0.0089 | Cyc: 0.3830\n",
            "Iter [3300/100000] Time: 543.05s | D_loss: 1.7425 | G_adv: 2.1339 | Sty: 0.0088 | Cyc: 0.3326\n",
            "Iter [3400/100000] Time: 559.56s | D_loss: 1.2440 | G_adv: 1.9763 | Sty: 0.0095 | Cyc: 0.2885\n",
            "Iter [3500/100000] Time: 576.21s | D_loss: 1.1793 | G_adv: 0.8302 | Sty: 0.0088 | Cyc: 0.3053\n",
            "Iter [3600/100000] Time: 592.70s | D_loss: 0.9977 | G_adv: 1.9591 | Sty: 0.0086 | Cyc: 0.3306\n",
            "Iter [3700/100000] Time: 609.15s | D_loss: 1.1968 | G_adv: 2.3338 | Sty: 0.0099 | Cyc: 0.2862\n",
            "Iter [3800/100000] Time: 625.68s | D_loss: 1.4961 | G_adv: 2.0796 | Sty: 0.0090 | Cyc: 0.4453\n",
            "Iter [3900/100000] Time: 642.16s | D_loss: 1.2351 | G_adv: 1.4645 | Sty: 0.0088 | Cyc: 0.4152\n",
            "Iter [4000/100000] Time: 658.67s | D_loss: 1.0257 | G_adv: 1.8504 | Sty: 0.0083 | Cyc: 0.3291\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/004000_grid.jpg\n",
            "Iter [4100/100000] Time: 675.25s | D_loss: 1.1146 | G_adv: 1.8595 | Sty: 0.0087 | Cyc: 0.3447\n",
            "Iter [4200/100000] Time: 691.76s | D_loss: 1.0019 | G_adv: 2.1374 | Sty: 0.0088 | Cyc: 0.3446\n",
            "Iter [4300/100000] Time: 708.23s | D_loss: 1.1039 | G_adv: 0.8909 | Sty: 0.0077 | Cyc: 0.2817\n",
            "Iter [4400/100000] Time: 724.68s | D_loss: 0.8752 | G_adv: 2.9630 | Sty: 0.0081 | Cyc: 0.4322\n",
            "Iter [4500/100000] Time: 741.12s | D_loss: 1.6497 | G_adv: 1.3167 | Sty: 0.0090 | Cyc: 0.3697\n",
            "Iter [4600/100000] Time: 757.62s | D_loss: 1.4472 | G_adv: 1.9289 | Sty: 0.0086 | Cyc: 0.2618\n",
            "Iter [4700/100000] Time: 774.16s | D_loss: 1.0254 | G_adv: 1.8278 | Sty: 0.0085 | Cyc: 0.2939\n",
            "Iter [4800/100000] Time: 790.60s | D_loss: 0.9883 | G_adv: 1.6065 | Sty: 0.0100 | Cyc: 0.3901\n",
            "Iter [4900/100000] Time: 807.07s | D_loss: 1.3233 | G_adv: 0.7975 | Sty: 0.0094 | Cyc: 0.3241\n",
            "Iter [5000/100000] Time: 823.51s | D_loss: 1.3299 | G_adv: 3.9920 | Sty: 0.0089 | Cyc: 0.3520\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/005000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/checkpoints/005000.ckpt\n",
            "Iter [5100/100000] Time: 841.07s | D_loss: 1.4049 | G_adv: 1.2991 | Sty: 0.0088 | Cyc: 0.3635\n",
            "Iter [5200/100000] Time: 857.49s | D_loss: 1.6376 | G_adv: 1.4426 | Sty: 0.0105 | Cyc: 0.3451\n",
            "Iter [5300/100000] Time: 873.94s | D_loss: 0.9989 | G_adv: 0.3062 | Sty: 0.0086 | Cyc: 0.3574\n",
            "Iter [5400/100000] Time: 890.44s | D_loss: 1.1330 | G_adv: 0.7131 | Sty: 0.0079 | Cyc: 0.2650\n",
            "Iter [5500/100000] Time: 906.94s | D_loss: 1.3907 | G_adv: 1.8354 | Sty: 0.0085 | Cyc: 0.3033\n",
            "Iter [5600/100000] Time: 923.45s | D_loss: 1.1008 | G_adv: 1.7603 | Sty: 0.0099 | Cyc: 0.2989\n",
            "Iter [5700/100000] Time: 939.88s | D_loss: 1.2261 | G_adv: 2.1611 | Sty: 0.0091 | Cyc: 0.3255\n",
            "Iter [5800/100000] Time: 956.31s | D_loss: 1.0436 | G_adv: 1.1330 | Sty: 0.0086 | Cyc: 0.3621\n",
            "Iter [5900/100000] Time: 972.98s | D_loss: 0.8064 | G_adv: 2.7011 | Sty: 0.0086 | Cyc: 0.3070\n",
            "Iter [6000/100000] Time: 989.44s | D_loss: 1.1438 | G_adv: 0.4992 | Sty: 0.0077 | Cyc: 0.3444\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/006000_grid.jpg\n",
            "Iter [6100/100000] Time: 1005.96s | D_loss: 0.3869 | G_adv: 1.5127 | Sty: 0.0091 | Cyc: 0.3393\n",
            "Iter [6200/100000] Time: 1022.40s | D_loss: 1.0703 | G_adv: 1.6951 | Sty: 0.0098 | Cyc: 0.3225\n",
            "Iter [6300/100000] Time: 1038.86s | D_loss: 1.3551 | G_adv: 1.8503 | Sty: 0.0089 | Cyc: 0.2845\n",
            "Iter [6400/100000] Time: 1055.29s | D_loss: 1.1132 | G_adv: 2.4449 | Sty: 0.0087 | Cyc: 0.3287\n",
            "Iter [6500/100000] Time: 1071.74s | D_loss: 1.2395 | G_adv: 1.2520 | Sty: 0.0083 | Cyc: 0.2760\n",
            "Iter [6600/100000] Time: 1088.17s | D_loss: 0.8033 | G_adv: 2.3003 | Sty: 0.0086 | Cyc: 0.3505\n",
            "Iter [6700/100000] Time: 1104.66s | D_loss: 1.5313 | G_adv: 4.5852 | Sty: 0.0085 | Cyc: 0.2750\n",
            "Iter [6800/100000] Time: 1121.10s | D_loss: 0.4148 | G_adv: 1.1442 | Sty: 0.0094 | Cyc: 0.3413\n",
            "Iter [6900/100000] Time: 1137.56s | D_loss: 1.1115 | G_adv: 2.3331 | Sty: 0.0083 | Cyc: 0.3009\n",
            "Iter [7000/100000] Time: 1154.02s | D_loss: 0.9222 | G_adv: 3.2368 | Sty: 0.0083 | Cyc: 0.2873\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/007000_grid.jpg\n",
            "Iter [7100/100000] Time: 1170.65s | D_loss: 0.4136 | G_adv: 2.7043 | Sty: 0.0093 | Cyc: 0.2812\n",
            "Iter [7200/100000] Time: 1187.14s | D_loss: 0.2456 | G_adv: 1.0706 | Sty: 0.0093 | Cyc: 0.3199\n",
            "Iter [7300/100000] Time: 1203.59s | D_loss: 0.4065 | G_adv: 4.3782 | Sty: 0.0092 | Cyc: 0.2989\n",
            "Iter [7400/100000] Time: 1220.01s | D_loss: 1.0888 | G_adv: 4.1347 | Sty: 0.0083 | Cyc: 0.3199\n",
            "Iter [7500/100000] Time: 1236.44s | D_loss: 0.7624 | G_adv: 1.7882 | Sty: 0.0092 | Cyc: 0.2996\n",
            "Iter [7600/100000] Time: 1253.00s | D_loss: 1.1407 | G_adv: 0.1582 | Sty: 0.0081 | Cyc: 0.3047\n",
            "Iter [7700/100000] Time: 1269.41s | D_loss: 1.1083 | G_adv: 2.3184 | Sty: 0.0088 | Cyc: 0.3215\n",
            "Iter [7800/100000] Time: 1285.87s | D_loss: 1.0993 | G_adv: 2.4048 | Sty: 0.0089 | Cyc: 0.3018\n",
            "Iter [7900/100000] Time: 1302.30s | D_loss: 1.2413 | G_adv: 1.8512 | Sty: 0.0083 | Cyc: 0.2885\n",
            "Iter [8000/100000] Time: 1318.72s | D_loss: 0.7617 | G_adv: 3.8409 | Sty: 0.0086 | Cyc: 0.2557\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/008000_grid.jpg\n",
            "Iter [8100/100000] Time: 1335.23s | D_loss: 0.6124 | G_adv: 3.1127 | Sty: 0.0090 | Cyc: 0.2699\n",
            "Iter [8200/100000] Time: 1351.60s | D_loss: 1.0366 | G_adv: 1.1410 | Sty: 0.0087 | Cyc: 0.3060\n",
            "Iter [8300/100000] Time: 1368.02s | D_loss: 0.5721 | G_adv: 2.6315 | Sty: 0.0085 | Cyc: 0.3424\n",
            "Iter [8400/100000] Time: 1384.45s | D_loss: 1.1304 | G_adv: 5.9692 | Sty: 0.0089 | Cyc: 0.2873\n",
            "Iter [8500/100000] Time: 1400.87s | D_loss: 0.6070 | G_adv: 1.6674 | Sty: 0.0088 | Cyc: 0.2638\n",
            "Iter [8600/100000] Time: 1417.33s | D_loss: 0.7783 | G_adv: 1.2667 | Sty: 0.0087 | Cyc: 0.2518\n",
            "Iter [8700/100000] Time: 1433.78s | D_loss: 0.7934 | G_adv: 2.4782 | Sty: 0.0090 | Cyc: 0.2928\n",
            "Iter [8800/100000] Time: 1450.23s | D_loss: 0.7987 | G_adv: 2.0545 | Sty: 0.0089 | Cyc: 0.2791\n",
            "Iter [8900/100000] Time: 1466.65s | D_loss: 0.7485 | G_adv: 1.3494 | Sty: 0.0087 | Cyc: 0.2907\n",
            "Iter [9000/100000] Time: 1483.07s | D_loss: 0.4945 | G_adv: 1.8279 | Sty: 0.0094 | Cyc: 0.3008\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/009000_grid.jpg\n",
            "Iter [9100/100000] Time: 1499.65s | D_loss: 0.8287 | G_adv: 1.7698 | Sty: 0.0101 | Cyc: 0.3082\n",
            "Iter [9200/100000] Time: 1516.14s | D_loss: 1.1611 | G_adv: 2.2317 | Sty: 0.0097 | Cyc: 0.2992\n",
            "Iter [9300/100000] Time: 1532.55s | D_loss: 0.7655 | G_adv: 2.3161 | Sty: 0.0091 | Cyc: 0.3370\n",
            "Iter [9400/100000] Time: 1548.94s | D_loss: 0.7076 | G_adv: 1.8757 | Sty: 0.0096 | Cyc: 0.3165\n",
            "Iter [9500/100000] Time: 1565.42s | D_loss: 0.9320 | G_adv: 1.9187 | Sty: 0.0088 | Cyc: 0.2387\n",
            "Iter [9600/100000] Time: 1581.88s | D_loss: 0.2577 | G_adv: 3.1003 | Sty: 0.0089 | Cyc: 0.2540\n",
            "Iter [9700/100000] Time: 1598.41s | D_loss: 0.2976 | G_adv: 1.6531 | Sty: 0.0082 | Cyc: 0.3123\n",
            "Iter [9800/100000] Time: 1614.85s | D_loss: 0.4702 | G_adv: 1.8827 | Sty: 0.0080 | Cyc: 0.3276\n",
            "Iter [9900/100000] Time: 1631.28s | D_loss: 0.6938 | G_adv: 1.7697 | Sty: 0.0096 | Cyc: 0.2795\n",
            "Iter [10000/100000] Time: 1647.78s | D_loss: 0.9403 | G_adv: 3.9440 | Sty: 0.0086 | Cyc: 0.2835\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/010000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/checkpoints/010000.ckpt\n",
            "Iter [10100/100000] Time: 1665.49s | D_loss: 0.5214 | G_adv: 1.9437 | Sty: 0.0092 | Cyc: 0.2744\n",
            "Iter [10200/100000] Time: 1681.91s | D_loss: 0.7228 | G_adv: 3.5296 | Sty: 0.0084 | Cyc: 0.2708\n",
            "Iter [10300/100000] Time: 1698.37s | D_loss: 0.6884 | G_adv: 1.3393 | Sty: 0.0097 | Cyc: 0.3008\n",
            "Iter [10400/100000] Time: 1714.77s | D_loss: 0.9104 | G_adv: 4.0393 | Sty: 0.0087 | Cyc: 0.2628\n",
            "Iter [10500/100000] Time: 1731.16s | D_loss: 1.2913 | G_adv: 0.5306 | Sty: 0.0087 | Cyc: 0.3002\n",
            "Iter [10600/100000] Time: 1747.63s | D_loss: 0.4082 | G_adv: 2.1824 | Sty: 0.0089 | Cyc: 0.3136\n",
            "Iter [10700/100000] Time: 1764.09s | D_loss: 0.2708 | G_adv: 1.1656 | Sty: 0.0088 | Cyc: 0.3188\n",
            "Iter [10800/100000] Time: 1780.55s | D_loss: 1.7306 | G_adv: 1.2028 | Sty: 0.0086 | Cyc: 0.2926\n",
            "Iter [10900/100000] Time: 1797.12s | D_loss: 1.4743 | G_adv: 2.2014 | Sty: 0.0081 | Cyc: 0.2948\n",
            "Iter [11000/100000] Time: 1813.51s | D_loss: 0.4650 | G_adv: 1.5125 | Sty: 0.0089 | Cyc: 0.2903\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/011000_grid.jpg\n",
            "Iter [11100/100000] Time: 1830.01s | D_loss: 0.8727 | G_adv: 4.3524 | Sty: 0.0088 | Cyc: 0.2861\n",
            "Iter [11200/100000] Time: 1846.48s | D_loss: 0.1269 | G_adv: 2.6179 | Sty: 0.0079 | Cyc: 0.2802\n",
            "Iter [11300/100000] Time: 1863.01s | D_loss: 0.7048 | G_adv: 3.2090 | Sty: 0.0083 | Cyc: 0.3068\n",
            "Iter [11400/100000] Time: 1879.44s | D_loss: 0.5591 | G_adv: 3.1135 | Sty: 0.0091 | Cyc: 0.3350\n",
            "Iter [11500/100000] Time: 1895.92s | D_loss: 0.3414 | G_adv: 2.5622 | Sty: 0.0087 | Cyc: 0.3610\n",
            "Iter [11600/100000] Time: 1912.41s | D_loss: 0.4391 | G_adv: 2.8827 | Sty: 0.0089 | Cyc: 0.3245\n",
            "Iter [11700/100000] Time: 1928.90s | D_loss: 0.2911 | G_adv: 2.5522 | Sty: 0.0081 | Cyc: 0.3192\n",
            "Iter [11800/100000] Time: 1945.32s | D_loss: 1.1510 | G_adv: 2.8933 | Sty: 0.0089 | Cyc: 0.3092\n",
            "Iter [11900/100000] Time: 1961.77s | D_loss: 1.7294 | G_adv: 2.6756 | Sty: 0.0080 | Cyc: 0.3387\n",
            "Iter [12000/100000] Time: 1978.21s | D_loss: 0.4106 | G_adv: 1.7632 | Sty: 0.0092 | Cyc: 0.2606\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/012000_grid.jpg\n",
            "Iter [12100/100000] Time: 1994.76s | D_loss: 0.8792 | G_adv: 3.5440 | Sty: 0.0081 | Cyc: 0.3231\n",
            "Iter [12200/100000] Time: 2011.21s | D_loss: 1.1927 | G_adv: 2.0275 | Sty: 0.0087 | Cyc: 0.3207\n",
            "Iter [12300/100000] Time: 2027.63s | D_loss: 0.9711 | G_adv: 3.2456 | Sty: 0.0082 | Cyc: 0.2385\n",
            "Iter [12400/100000] Time: 2044.07s | D_loss: 0.6068 | G_adv: 2.7677 | Sty: 0.0091 | Cyc: 0.2834\n",
            "Iter [12500/100000] Time: 2060.56s | D_loss: 0.6470 | G_adv: 3.0101 | Sty: 0.0089 | Cyc: 0.3211\n",
            "Iter [12600/100000] Time: 2077.02s | D_loss: 0.4533 | G_adv: 1.7891 | Sty: 0.0090 | Cyc: 0.2875\n",
            "Iter [12700/100000] Time: 2093.48s | D_loss: 0.8479 | G_adv: 2.8977 | Sty: 0.0084 | Cyc: 0.3150\n",
            "Iter [12800/100000] Time: 2110.00s | D_loss: 0.8681 | G_adv: 1.5219 | Sty: 0.0092 | Cyc: 0.2785\n",
            "Iter [12900/100000] Time: 2126.41s | D_loss: 0.4493 | G_adv: 2.6120 | Sty: 0.0096 | Cyc: 0.2493\n",
            "Iter [13000/100000] Time: 2142.77s | D_loss: 0.9314 | G_adv: 3.3407 | Sty: 0.0087 | Cyc: 0.2754\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/013000_grid.jpg\n",
            "Iter [13100/100000] Time: 2159.30s | D_loss: 0.4989 | G_adv: 1.6744 | Sty: 0.0086 | Cyc: 0.3069\n",
            "Iter [13200/100000] Time: 2175.83s | D_loss: 0.6300 | G_adv: 4.3423 | Sty: 0.0097 | Cyc: 0.3076\n",
            "Iter [13300/100000] Time: 2192.28s | D_loss: 0.6517 | G_adv: 3.1073 | Sty: 0.0087 | Cyc: 0.2929\n",
            "Iter [13400/100000] Time: 2208.76s | D_loss: 0.5518 | G_adv: 4.5404 | Sty: 0.0091 | Cyc: 0.2768\n",
            "Iter [13500/100000] Time: 2225.23s | D_loss: 0.6129 | G_adv: 1.9853 | Sty: 0.0086 | Cyc: 0.2764\n",
            "Iter [13600/100000] Time: 2241.72s | D_loss: 1.7814 | G_adv: 1.8302 | Sty: 0.0086 | Cyc: 0.2784\n",
            "Iter [13700/100000] Time: 2258.15s | D_loss: 0.4289 | G_adv: 4.2751 | Sty: 0.0084 | Cyc: 0.2607\n",
            "Iter [13800/100000] Time: 2274.58s | D_loss: 0.5795 | G_adv: 3.1937 | Sty: 0.0087 | Cyc: 0.3134\n",
            "Iter [13900/100000] Time: 2291.02s | D_loss: 0.6458 | G_adv: 0.3138 | Sty: 0.0088 | Cyc: 0.2940\n",
            "Iter [14000/100000] Time: 2307.47s | D_loss: 1.1488 | G_adv: 3.1779 | Sty: 0.0083 | Cyc: 0.3268\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/014000_grid.jpg\n",
            "Iter [14100/100000] Time: 2324.00s | D_loss: 1.0945 | G_adv: 3.0634 | Sty: 0.0092 | Cyc: 0.2349\n",
            "Iter [14200/100000] Time: 2340.39s | D_loss: 0.5672 | G_adv: 1.6063 | Sty: 0.0081 | Cyc: 0.2854\n",
            "Iter [14300/100000] Time: 2356.82s | D_loss: 0.4304 | G_adv: 0.8437 | Sty: 0.0086 | Cyc: 0.3353\n",
            "Iter [14400/100000] Time: 2373.27s | D_loss: 1.0673 | G_adv: 0.8256 | Sty: 0.0083 | Cyc: 0.3215\n",
            "Iter [14500/100000] Time: 2389.72s | D_loss: 0.4502 | G_adv: 1.4659 | Sty: 0.0085 | Cyc: 0.3154\n",
            "Iter [14600/100000] Time: 2406.15s | D_loss: 1.5905 | G_adv: 2.1184 | Sty: 0.0095 | Cyc: 0.2812\n",
            "Iter [14700/100000] Time: 2422.60s | D_loss: 0.6145 | G_adv: 2.1008 | Sty: 0.0090 | Cyc: 0.2804\n",
            "Iter [14800/100000] Time: 2439.02s | D_loss: 0.3437 | G_adv: 1.3399 | Sty: 0.0089 | Cyc: 0.3239\n",
            "Iter [14900/100000] Time: 2455.40s | D_loss: 0.4178 | G_adv: 2.7253 | Sty: 0.0083 | Cyc: 0.2966\n",
            "Iter [15000/100000] Time: 2471.73s | D_loss: 0.9581 | G_adv: 1.3550 | Sty: 0.0091 | Cyc: 0.3055\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/015000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/checkpoints/015000.ckpt\n",
            "Iter [15100/100000] Time: 2489.04s | D_loss: 0.5812 | G_adv: 1.4786 | Sty: 0.0089 | Cyc: 0.2930\n",
            "Iter [15200/100000] Time: 2505.50s | D_loss: 0.1895 | G_adv: 3.3731 | Sty: 0.0093 | Cyc: 0.3177\n",
            "Iter [15300/100000] Time: 2521.93s | D_loss: 1.2542 | G_adv: 3.3408 | Sty: 0.0091 | Cyc: 0.2904\n",
            "Iter [15400/100000] Time: 2538.31s | D_loss: 0.7881 | G_adv: 1.9353 | Sty: 0.0081 | Cyc: 0.3184\n",
            "Iter [15500/100000] Time: 2554.75s | D_loss: 0.7354 | G_adv: 3.4518 | Sty: 0.0090 | Cyc: 0.2602\n",
            "Iter [15600/100000] Time: 2571.14s | D_loss: 0.4698 | G_adv: 2.5251 | Sty: 0.0095 | Cyc: 0.2757\n",
            "Iter [15700/100000] Time: 2587.52s | D_loss: 0.8755 | G_adv: 4.0510 | Sty: 0.0088 | Cyc: 0.2947\n",
            "Iter [15800/100000] Time: 2603.87s | D_loss: 0.6619 | G_adv: 1.4442 | Sty: 0.0093 | Cyc: 0.2919\n",
            "Iter [15900/100000] Time: 2620.25s | D_loss: 0.9643 | G_adv: 1.5818 | Sty: 0.0084 | Cyc: 0.2949\n",
            "Iter [16000/100000] Time: 2636.75s | D_loss: 0.7150 | G_adv: 1.2768 | Sty: 0.0084 | Cyc: 0.2812\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/016000_grid.jpg\n",
            "Iter [16100/100000] Time: 2653.30s | D_loss: 0.8536 | G_adv: 3.0099 | Sty: 0.0091 | Cyc: 0.2814\n",
            "Iter [16200/100000] Time: 2669.73s | D_loss: 0.3909 | G_adv: 1.4510 | Sty: 0.0089 | Cyc: 0.2835\n",
            "Iter [16300/100000] Time: 2686.19s | D_loss: 0.3266 | G_adv: 2.5527 | Sty: 0.0087 | Cyc: 0.2925\n",
            "Iter [16400/100000] Time: 2702.62s | D_loss: 1.5505 | G_adv: 2.1831 | Sty: 0.0079 | Cyc: 0.3230\n",
            "Iter [16500/100000] Time: 2719.04s | D_loss: 0.4392 | G_adv: 2.8802 | Sty: 0.0088 | Cyc: 0.3200\n",
            "Iter [16600/100000] Time: 2735.46s | D_loss: 0.6046 | G_adv: 1.6582 | Sty: 0.0091 | Cyc: 0.3183\n",
            "Iter [16700/100000] Time: 2751.86s | D_loss: 0.6459 | G_adv: 2.8750 | Sty: 0.0078 | Cyc: 0.3021\n",
            "Iter [16800/100000] Time: 2768.21s | D_loss: 0.4631 | G_adv: 2.3665 | Sty: 0.0087 | Cyc: 0.3147\n",
            "Iter [16900/100000] Time: 2784.57s | D_loss: 0.3797 | G_adv: 3.8892 | Sty: 0.0093 | Cyc: 0.2994\n",
            "Iter [17000/100000] Time: 2800.97s | D_loss: 0.6705 | G_adv: 2.4107 | Sty: 0.0080 | Cyc: 0.2938\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/017000_grid.jpg\n",
            "Iter [17100/100000] Time: 2817.51s | D_loss: 0.9129 | G_adv: 1.7877 | Sty: 0.0086 | Cyc: 0.2648\n",
            "Iter [17200/100000] Time: 2833.93s | D_loss: 0.6935 | G_adv: 1.5691 | Sty: 0.0077 | Cyc: 0.2740\n",
            "Iter [17300/100000] Time: 2850.48s | D_loss: 0.4657 | G_adv: 1.8308 | Sty: 0.0080 | Cyc: 0.2859\n",
            "Iter [17400/100000] Time: 2866.85s | D_loss: 0.6274 | G_adv: 1.4052 | Sty: 0.0081 | Cyc: 0.3264\n",
            "Iter [17500/100000] Time: 2883.33s | D_loss: 0.5916 | G_adv: 4.7237 | Sty: 0.0086 | Cyc: 0.2607\n",
            "Iter [17600/100000] Time: 2899.78s | D_loss: 0.5927 | G_adv: 2.8076 | Sty: 0.0084 | Cyc: 0.3295\n",
            "Iter [17700/100000] Time: 2916.20s | D_loss: 0.1640 | G_adv: 1.7829 | Sty: 0.0087 | Cyc: 0.2886\n",
            "Iter [17800/100000] Time: 2932.60s | D_loss: 0.3262 | G_adv: 1.6673 | Sty: 0.0099 | Cyc: 0.2883\n",
            "Iter [17900/100000] Time: 2949.03s | D_loss: 1.1093 | G_adv: 2.5330 | Sty: 0.0086 | Cyc: 0.3421\n",
            "Iter [18000/100000] Time: 2965.40s | D_loss: 0.5405 | G_adv: 1.3796 | Sty: 0.0099 | Cyc: 0.3074\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/018000_grid.jpg\n",
            "Iter [18100/100000] Time: 2981.89s | D_loss: 0.1387 | G_adv: 1.2859 | Sty: 0.0076 | Cyc: 0.3232\n",
            "Iter [18200/100000] Time: 2998.25s | D_loss: 0.2997 | G_adv: 3.6089 | Sty: 0.0091 | Cyc: 0.3712\n",
            "Iter [18300/100000] Time: 3014.68s | D_loss: 0.2437 | G_adv: 1.4217 | Sty: 0.0088 | Cyc: 0.3070\n",
            "Iter [18400/100000] Time: 3031.04s | D_loss: 0.6783 | G_adv: 1.6184 | Sty: 0.0076 | Cyc: 0.3513\n",
            "Iter [18500/100000] Time: 3047.40s | D_loss: 0.2145 | G_adv: 2.3844 | Sty: 0.0085 | Cyc: 0.3161\n",
            "Iter [18600/100000] Time: 3063.81s | D_loss: 0.5892 | G_adv: 2.0348 | Sty: 0.0095 | Cyc: 0.3331\n",
            "Iter [18700/100000] Time: 3080.21s | D_loss: 0.1808 | G_adv: 1.1981 | Sty: 0.0091 | Cyc: 0.3307\n",
            "Iter [18800/100000] Time: 3096.66s | D_loss: 0.6928 | G_adv: 2.1815 | Sty: 0.0086 | Cyc: 0.3109\n",
            "Iter [18900/100000] Time: 3113.10s | D_loss: 0.6139 | G_adv: 2.3581 | Sty: 0.0089 | Cyc: 0.3400\n",
            "Iter [19000/100000] Time: 3129.52s | D_loss: 0.8496 | G_adv: 1.3462 | Sty: 0.0082 | Cyc: 0.2997\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/019000_grid.jpg\n",
            "Iter [19100/100000] Time: 3146.09s | D_loss: 1.4810 | G_adv: 3.6231 | Sty: 0.0095 | Cyc: 0.3351\n",
            "Iter [19200/100000] Time: 3162.49s | D_loss: 0.2852 | G_adv: 3.0629 | Sty: 0.0088 | Cyc: 0.3625\n",
            "Iter [19300/100000] Time: 3178.89s | D_loss: 0.3897 | G_adv: 2.6342 | Sty: 0.0093 | Cyc: 0.3134\n",
            "Iter [19400/100000] Time: 3195.28s | D_loss: 0.5825 | G_adv: 2.3884 | Sty: 0.0093 | Cyc: 0.3709\n",
            "Iter [19500/100000] Time: 3211.71s | D_loss: 0.4886 | G_adv: 2.8210 | Sty: 0.0091 | Cyc: 0.4443\n",
            "Iter [19600/100000] Time: 3228.09s | D_loss: 0.3680 | G_adv: 1.3095 | Sty: 0.0083 | Cyc: 0.3253\n",
            "Iter [19700/100000] Time: 3244.46s | D_loss: 0.5896 | G_adv: 2.9825 | Sty: 0.0084 | Cyc: 0.3294\n",
            "Iter [19800/100000] Time: 3260.83s | D_loss: 0.8146 | G_adv: 0.9118 | Sty: 0.0079 | Cyc: 0.3015\n",
            "Iter [19900/100000] Time: 3277.30s | D_loss: 0.4176 | G_adv: 2.3404 | Sty: 0.0078 | Cyc: 0.2636\n",
            "Iter [20000/100000] Time: 3293.68s | D_loss: 0.1997 | G_adv: 1.3228 | Sty: 0.0092 | Cyc: 0.3494\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/samples/020000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v2/checkpoints/020000.ckpt\n",
            "Iter [20100/100000] Time: 3311.18s | D_loss: 0.4808 | G_adv: 2.1587 | Sty: 0.0074 | Cyc: 0.3315\n",
            "Iter [20200/100000] Time: 3327.53s | D_loss: 0.2372 | G_adv: 2.0914 | Sty: 0.0078 | Cyc: 0.2909\n",
            "Iter [20300/100000] Time: 3343.96s | D_loss: 0.6642 | G_adv: 2.0432 | Sty: 0.0092 | Cyc: 0.2977\n",
            "Iter [20400/100000] Time: 3360.36s | D_loss: 1.0062 | G_adv: 1.1720 | Sty: 0.0087 | Cyc: 0.2837\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-685970417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;31m# Solver 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-685970417.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0moptims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'F'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# 기존 설정과 모델 불러오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder\n",
        "\n",
        "def load_model(args, device, checkpoint_step=None):\n",
        "    \"\"\"저장된 체크포인트를 불러옵니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "\n",
        "    # 체크포인트 지정이 없으면 가장 마지막(최신) 파일 로드\n",
        "    if checkpoint_step is None:\n",
        "        ckpts = sorted([f for f in os.listdir(model_path) if f.endswith('.ckpt')])\n",
        "        if not ckpts:\n",
        "            raise FileNotFoundError(\"체크포인트가 없습니다!\")\n",
        "        latest_ckpt = ckpts[-1]\n",
        "    else:\n",
        "        latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 모델 초기화 및 가중치 로드\n",
        "    generator = Generator(args.img_size, args.style_dim).to(device)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device)\n",
        "    # Style Encoder는 Reference Guided Synthesis 할 때 필요 (여기서는 Latent Guided만 시연)\n",
        "\n",
        "    generator.load_state_dict(ckpt['nets']['G'])\n",
        "    mapping_network.load_state_dict(ckpt['nets']['F'])\n",
        "\n",
        "    generator.eval()\n",
        "    mapping_network.eval()\n",
        "\n",
        "    return generator, mapping_network\n",
        "\n",
        "def inference(args, device):\n",
        "    # 1. 모델 로드\n",
        "    generator, mapping_net = load_model(args, device)\n",
        "\n",
        "    # 2. 테스트 데이터 로드 (학습에 안 쓴 데이터)\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True) # 10장만 샘플링\n",
        "\n",
        "    # 3. 소스 이미지 가져오기\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    # 4. 시각화 준비\n",
        "    domain_labels = get_domain_labels()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(11, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    # 고정된 Random Noise z 하나를 모든 도메인에 적용해 봅니다.\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains): # 0~9 (각 도메인별)\n",
        "        # 해당 도메인(row_idx)의 스타일 코드 생성\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = mapping_net(z_trg, y_trg) # (1, style_dim)\n",
        "\n",
        "        # 스타일 코드를 배치 크기만큼 복사 (1 -> 10)\n",
        "        s_trg = s_trg.repeat(10, 1)\n",
        "\n",
        "        # 이미지 생성\n",
        "        with torch.no_grad():\n",
        "            x_fake = generator(x_real, s_trg)\n",
        "\n",
        "        # 결과 출력\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(11, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 왼쪽 첫 열에만 도메인 이름 표시\n",
        "            if col_idx == 0:\n",
        "                plt.text(-10, 32, domain_labels[row_idx], fontsize=10, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "if __name__ == '__main__':\n",
        "    config = get_config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inference(config, device)"
      ],
      "metadata": {
        "id": "7DKw-bIshf8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}