{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMYMN49CzsmAjal9IjMCVdj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/system_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 필수 라이브러리 설치\n",
        "!pip install requests torch transformers faiss-cpu\n",
        "!pip install tqdm # 진행률 표시바"
      ],
      "metadata": {
        "id": "g3xvN3pHDKgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rdSXQp6-Ztc",
        "outputId": "075f8ca6-1160-45f3-8abe-5ab7ea015b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API로부터 레시피 데이터 수집을 시작합니다...\n",
            "1 ~ 1000 번째 데이터 수집 완료 (누적 1000건 / 전체 1146건)\n",
            "1001 ~ 2000 번째 데이터 수집 완료 (누적 1146건 / 전체 1146건)\n",
            "총 1146개의 레시피 수집 완료!\n",
            "KoBERT 모델 로딩 중...\n",
            "데이터 벡터화 진행 중 (시간이 좀 걸립니다)...\n",
            "FAISS 인덱스 생성 중...\n",
            "✅ 모든 작업 완료! 'project_data.index'와 'project_data.pkl' 파일을 다운로드하세요.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "import time\n",
        "from transformers import BertModel, BertTokenizer, AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- 1. 설정 (API 키 입력) ---\n",
        "# 'sample' 키는 하루 요청 횟수나 데이터 건수에 제한이 있을 수 있습니다.\n",
        "# 공공데이터포털에서 발급받은 본인의 인증키를 넣는 것을 권장합니다.\n",
        "API_KEY = \"ce4c9a9035f643bb9dba\"  # 여기에 발급받은 API KEY 입력\n",
        "SERVICE_ID = \"COOKRCP01\" # 조리식품 레시피 서비스 ID\n",
        "TYPE = \"json\" # xml 대신 json 사용 (파이썬 친화적)\n",
        "\n",
        "# --- 2. 데이터 수집 함수 (API) ---\n",
        "def fetch_all_recipes():\n",
        "    all_documents = []\n",
        "\n",
        "    # 한 번에 요청할 개수 (최대 1000개까지 가능)\n",
        "    step = 1000\n",
        "    start_idx = 1\n",
        "    end_idx = step\n",
        "\n",
        "    print(\"API로부터 레시피 데이터 수집을 시작합니다...\")\n",
        "\n",
        "    while True:\n",
        "        # URL 생성 (start/end 인덱스 방식)\n",
        "        url = f\"http://openapi.foodsafetykorea.go.kr/api/{API_KEY}/{SERVICE_ID}/{TYPE}/{start_idx}/{end_idx}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            data = response.json()\n",
        "\n",
        "            # API 응답 코드 확인\n",
        "            try:\n",
        "                code = data[SERVICE_ID]['RESULT']['CODE']\n",
        "                total_count = int(data[SERVICE_ID]['total_count']) # 전체 데이터 개수\n",
        "            except KeyError:\n",
        "                # 데이터가 없거나 에러인 경우 (sample 키 사용 시 자주 발생)\n",
        "                print(f\"응답 구조가 예상과 다릅니다: {data}\")\n",
        "                break\n",
        "\n",
        "            if code != 'INFO-000':\n",
        "                print(f\"API 호출 종료 (Code: {code})\")\n",
        "                break\n",
        "\n",
        "            rows = data[SERVICE_ID]['row']\n",
        "            if not rows:\n",
        "                break\n",
        "\n",
        "            for row in rows:\n",
        "                # 필요한 필드 추출\n",
        "                menu_name = row.get('RCP_NM', '이름 없음')    # 메뉴명\n",
        "                ingredients = row.get('RCP_PARTS_DTLS', '') # 재료 정보\n",
        "\n",
        "                # 조리 순서 합치기 (MANUAL01 ~ MANUAL20)\n",
        "                manual_text = \"\"\n",
        "                for i in range(1, 21):\n",
        "                    step_key = f'MANUAL{i:02d}'\n",
        "                    step_desc = row.get(step_key, '')\n",
        "                    if step_desc:\n",
        "                        # 줄바꿈 문자 제거하고 붙이기\n",
        "                        manual_text += step_desc.replace('\\n', ' ').strip() + \" \"\n",
        "\n",
        "                # [중요] RAG 검색을 위해 하나의 텍스트 덩어리로 만듦\n",
        "                # 재료 기반 추천을 원하신다면 재료 정보를 앞쪽에 배치하는 것이 좋습니다.\n",
        "                full_text = f\"요리명: {menu_name}\\n재료: {ingredients}\\n조리법: {manual_text}\"\n",
        "                all_documents.append(full_text)\n",
        "\n",
        "            print(f\"{start_idx} ~ {end_idx} 번째 데이터 수집 완료 (누적 {len(all_documents)}건 / 전체 {total_count}건)\")\n",
        "\n",
        "            # 다음 페이지 준비\n",
        "            start_idx += step\n",
        "            end_idx += step\n",
        "\n",
        "            # 전체 개수를 넘으면 종료\n",
        "            if start_idx > total_count:\n",
        "                break\n",
        "\n",
        "            # 서버 부하 방지용 딜레이\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            break\n",
        "\n",
        "    print(f\"총 {len(all_documents)}개의 레시피 수집 완료!\")\n",
        "    return all_documents\n",
        "\n",
        "# --- 3. 실행: 데이터 수집 ---\n",
        "documents = fetch_all_recipes()\n",
        "\n",
        "# (테스트용) 데이터가 너무 적으면 샘플 데이터 추가 (sample 키 제한 때문)\n",
        "if len(documents) < 5:\n",
        "    print(\"⚠️ 데이터가 너무 적어 테스트용 데이터를 추가합니다.\")\n",
        "    documents.extend([\n",
        "        \"요리명: 김치찌개\\n재료: 김치, 돼지고기, 두부, 파\\n조리법: 김치를 볶다가 물을 붓고 끓입니다.\",\n",
        "        \"요리명: 된장찌개\\n재료: 된장, 애호박, 두부, 감자\\n조리법: 멸치 육수에 된장을 풀고 야채를 넣습니다.\"\n",
        "    ])\n",
        "\n",
        "# --- 4. KoBERT 임베딩 및 FAISS 인덱스 생성 ---\n",
        "print(\"KoBERT 모델 로딩 중...\")\n",
        "# skt/kobert-base-v1 대신 호환성이 좋은 kobert-tokenizer 사용 권장 방식\n",
        "# 여기서는 가장 에러가 적은 transformer 기본 방식으로 진행\n",
        "MODEL_NAME = \"skt/kobert-base-v1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# GPU 사용 설정 (Colab에서는 GPU 써야 빠릅니다)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def get_embedding(text_list):\n",
        "    embeddings = []\n",
        "    # 배치 단위 처리 (메모리 관리)\n",
        "    batch_size = 32\n",
        "\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch_texts = text_list[i:i+batch_size]\n",
        "        inputs = tokenizer(\n",
        "            batch_texts,\n",
        "            return_tensors='pt',\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=256 # 레시피는 기니까 길이를 좀 넉넉하게\n",
        "        ).to(device)\n",
        "\n",
        "        # token_type_ids가 문제를 일으킬 수 있으므로 제거합니다.\n",
        "        # 단일 텍스트 임베딩에서는 일반적으로 필요하지 않습니다.\n",
        "        if 'token_type_ids' in inputs: # token_type_ids 키가 있는 경우에만 삭제\n",
        "            del inputs['token_type_ids']\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        # [CLS] 토큰 벡터 추출\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "        embeddings.append(cls_emb)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "print(\"데이터 벡터화 진행 중 (시간이 좀 걸립니다)...\")\n",
        "embeddings = get_embedding(documents)\n",
        "\n",
        "print(\"FAISS 인덱스 생성 중...\")\n",
        "dimension = 768 # KoBERT 출력 차원\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "index.add(embeddings)\n",
        "\n",
        "# --- 5. 파일 저장 ---\n",
        "# 이 두 파일을 다운로드해서 로컬 Docker 폴더에 넣으세요\n",
        "faiss.write_index(index, \"project_data.index\")\n",
        "with open(\"project_data.pkl\", \"wb\") as f:\n",
        "    pickle.dump(documents, f)\n",
        "\n",
        "print(\"✅ 모든 작업 완료! 'project_data.index'와 'project_data.pkl' 파일을 다운로드하세요.\")\n"
      ]
    }
  ]
}