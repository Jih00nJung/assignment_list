{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/GAN_FMNIST_v5_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cyc : 0.01 -> 0.001\n",
        "sty : 10.0 -> 3.0\n",
        "d_train__repeat : 1 -> 2"
      ],
      "metadata": {
        "id": "uglPhlv5wl3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. config.py (설정 및 환경 준비)"
      ],
      "metadata": {
        "id": "VwJWmulsWjmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr3U21KWYwp",
        "outputId": "f768ca2f-0b93-4877-b6fe-c0e648c28c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "import argparse\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"학습에 필요한 모든 하이퍼파라미터를 정의합니다.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # 데이터 및 경로 설정\n",
        "    parser.add_argument('--project_name', type=str, default='GAN_FMNIST_v5-1')\n",
        "    parser.add_argument('--save_root', type=str, default='/content/drive/MyDrive/Colab Notebooks/GAN_assignment')\n",
        "    parser.add_argument('--img_size', type=int, default=64, help='이미지 크기 (FMNIST 기본 28 -> 64 리사이즈)')\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "\n",
        "    # 모델 하이퍼파라미터\n",
        "    parser.add_argument('--style_dim', type=int, default=64, help='스타일 코드 차원')\n",
        "    parser.add_argument('--latent_dim', type=int, default=16, help='랜덤 노이즈 차원')\n",
        "    parser.add_argument('--num_domains', type=int, default=10, help='Fashion MNIST 클래스 개수')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=256, help='Mapping Network 히든 차원')\n",
        "\n",
        "    # 학습 설정\n",
        "    parser.add_argument('--total_iters', type=int, default=100000)\n",
        "    parser.add_argument('--resume_iter', type=int, default=0)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--w_hpf', type=float, default=1, help='High-pass filtering 가중치')\n",
        "\n",
        "    # [Loss 가중치 재조정 - 다양성 극대화]\n",
        "    parser.add_argument('--lambda_cyc', type=float, default=0.001)  # 낮은 Cycle Loss 유지\n",
        "    parser.add_argument('--lambda_sty', type=float, default=3.0) # Style Loss 극대화\n",
        "    parser.add_argument('--lambda_ds', type=float, default=7.0)   # Diversity Loss 극대화 (핵심)\n",
        "\n",
        "    # [D 강화 완화]\n",
        "    parser.add_argument('--lambda_r1', type=float, default=0.1)   # R1 정규화 완화\n",
        "    parser.add_argument('--d_train_repeats', type=int, default=2) # D 반복 횟수 감소\n",
        "\n",
        "    # 로깅 주기\n",
        "    parser.add_argument('--sample_freq', type=int, default=1000)\n",
        "    parser.add_argument('--save_freq', type=int, default=5000)\n",
        "\n",
        "    # FID/LPIPS 평가용 설정\n",
        "    parser.add_argument('--num_fid_samples', type=int, default=1000, help='FID 계산에 사용할 생성 이미지 수')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def prepare_environment(args):\n",
        "    \"\"\"Google Drive 마운트 및 체크포인트/샘플 디렉토리를 생성합니다.\"\"\"\n",
        "    print(\"--- 환경 설정 중 ---\")\n",
        "    save_path = os.path.join(args.save_root, args.project_name)\n",
        "\n",
        "    # Google Drive 마운트\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive를 마운트합니다...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted.\")\n",
        "\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_path, 'samples'), exist_ok=True)\n",
        "    print(f\"저장 경로: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def get_data_transform(img_size):\n",
        "    \"\"\"Fashion MNIST 데이터 전처리를 정의합니다.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # [0,1] -> [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_domain_labels():\n",
        "    \"\"\"Fashion MNIST의 10개 도메인 레이블을 반환합니다.\"\"\"\n",
        "    return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. model.py (네트워크 아키텍처)"
      ],
      "metadata": {
        "id": "ywi2i9fZWl3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        # 1. 정규화 도구 (학습 파라미터 없음, 단순 통계 정규화)\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        # 2. 스타일 코드 s를 변환하여 감마(스케일)와 베타(시프트)를 만드는 선형 층\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        # s를 통해 파라미터 생성 (h)\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        # 생성된 파라미터를 감마와 베타로 나눔\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        # 정규화된 x에 감마를 곱하고 베타를 더함 -> 스타일 주입\n",
        "        return (1 + gamma) * self.norm(x) + beta\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"기본 ResBlock (다운샘플링 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            actv,\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            actv,\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "class AdaINResBlock(nn.Module):\n",
        "    \"\"\"Generator용 AdaIN ResBlock (Bottleneck 및 Up-sampling 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, style_dim, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self.norm1(x, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + self.shortcut(x)\n",
        "\n",
        "# --- (1) Generator (G) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64  # 경량화를 위한 시작 필터 수\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # 1. 입력부: 흑백(1채널) 이미지를 받아서 32채널 특징 맵으로 변환\n",
        "        self.from_rgb = nn.Conv2d(1, dim_in, 3, 1, 1) # Grayscale 1채널 입력\n",
        "\n",
        "        # 2. 인코더 (Down-sampling): 형태 정보 압축\n",
        "        # Down-sampling blocks (64 -> 32 -> 16 -> 8)\n",
        "        self.encode = nn.ModuleList()\n",
        "        curr_dim = dim_in\n",
        "        for _ in range(3): # 3번 다운샘플링하여 8x8 병목 생성\n",
        "            self.encode.append(ResBlock(curr_dim, curr_dim * 2))\n",
        "            self.encode.append(nn.AvgPool2d(2))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # 3. 병목 (Bottleneck): 스타일 주입 시작\n",
        "        # Bottleneck (8x8 유지, AdaIN 적용)\n",
        "        self.decode = nn.ModuleList()\n",
        "        curr_dim = min(curr_dim, max_conv_dim)\n",
        "        for _ in range(2):\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim, style_dim))\n",
        "\n",
        "        # 4. 디코더 (Up-sampling): 이미지 복원 + 스타일 입히기\n",
        "        # Up-sampling blocks (8 -> 16 -> 32 -> 64)\n",
        "        for _ in range(3):\n",
        "            self.decode.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim // 2, style_dim))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        # 5. 출력부: 최종적으로 1채널(흑백) 이미지로 변환\n",
        "        # Final Conv\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(curr_dim, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(curr_dim, 1, 1, 1, 0) # Grayscale 1채널 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.from_rgb(x)\n",
        "        for block in self.encode:\n",
        "            x = block(x)\n",
        "\n",
        "        for block in self.decode:\n",
        "            if isinstance(block, AdaINResBlock):\n",
        "                x = block(x, s)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "# --- (2) Mapping Network (F) ---\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        # 공유 레이어 (Shared)\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(latent_dim if not layers else hidden_dim, hidden_dim)]\n",
        "            layers += [nn.ReLU()]\n",
        "        # 1. 공유 레이어 (Shared): 모든 도메인이 공통으로 사용하는 특징 추출\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        # 2. 비공유 레이어 (Unshared): 각 도메인(T-shirt, Pants...)별 전용 스타일 생성기\n",
        "        # 도메인별 출력 레이어 (Unshared)\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, style_dim)\n",
        "            ))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, style_dim)\n",
        "\n",
        "        # 사용자가 요청한 도메인(y)에 해당하는 스타일만 쏙 뽑아서 리턴\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y] # 해당 도메인의 스타일 코드만 선택\n",
        "        return s\n",
        "\n",
        "# --- (3) Style Encoder (E) ---\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지를 보며 특징을 추출 (CNN 구조)\n",
        "        # 64 -> 32 -> 16 -> 8 로 줄어들며 추상적인 특징을 잡아냄\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 비공유 레이어: 추출된 특징을 보고 \"이건 바지 스타일로는 s_pants, 티셔츠로는 s_shirt야\" 라고 해석\n",
        "        # 도메인별 Style Code 출력\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, style_dim))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # 이미지 x에서 시각적 특징 추출\n",
        "        h = self.shared(x) # (batch, curr_dim, 1, 1)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n",
        "\n",
        "# --- (4) Discriminator (D) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지가 진짜인지 가짜인지 판단하기 위한 단서(특징) 추출\n",
        "        # ResBlock이나 Conv 레이어를 사용하여 이미지를 분석함\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 멀티 태스크 헤드: 각 도메인별로 진위 여부를 따로 판별\n",
        "        # 도메인별 진위 판별 헤드\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, 1))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, 1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        score = out[idx, y]\n",
        "        return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ozdU0eWnJ3",
        "outputId": "63308899-3963-40a3-ce22-c304a214e88a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. solver.py (메인 실행 및 학습 루프)"
      ],
      "metadata": {
        "id": "m__DYADCWoiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import grad as torch_grad # R1 정규화에 필요\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, prepare_environment, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "class Solver:\n",
        "    def __init__(self, args, device):\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.save_dir = prepare_environment(args)\n",
        "        self.domain_labels = get_domain_labels()\n",
        "\n",
        "        # 데이터셋 준비 (FashionMNIST)\n",
        "        transform = get_data_transform(args.img_size)\n",
        "\n",
        "        # 학습용 데이터 로더\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        self.loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "        # 평가용 데이터 로더 (FID/LPIPS 계산 시 사용될 예정)\n",
        "        self.eval_loader = self.get_eval_loader(transform)\n",
        "\n",
        "        # 모델 초기화\n",
        "        # Generator의 채널 용량이 model.py에서 64/512로 증가했다고 가정하고 초기화\n",
        "        self.nets = {\n",
        "            'G': Generator(args.img_size, args.style_dim),\n",
        "            'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim),\n",
        "            'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains),\n",
        "            'D': Discriminator(args.img_size, args.num_domains)\n",
        "        }\n",
        "\n",
        "        for name, module in self.nets.items():\n",
        "            module.to(self.device)\n",
        "            module.train()\n",
        "\n",
        "        # 옵티마이저\n",
        "        self.optims = {\n",
        "            'G': torch.optim.Adam(self.nets['G'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'F': torch.optim.Adam(self.nets['F'].parameters(), lr=args.lr*0.01, betas=(0.0, 0.99)),\n",
        "            'E': torch.optim.Adam(self.nets['E'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'D': torch.optim.Adam(self.nets['D'].parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "        }\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        self.start_iter = 0\n",
        "        if args.resume_iter > 0:\n",
        "            self.load_checkpoint(args.resume_iter)\n",
        "            self.start_iter = args.resume_iter\n",
        "\n",
        "    def get_eval_loader(self, transform):\n",
        "        \"\"\"FID/LPIPS 계산을 위한 평가용 데이터 로더를 준비합니다.\"\"\"\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=self.args.batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "    def save_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        state = {\n",
        "            'nets': {name: net.state_dict() for name, net in self.nets.items()},\n",
        "            'optims': {name: opt.state_dict() for name, opt in self.optims.items()},\n",
        "            'step': step\n",
        "        }\n",
        "        torch.save(state, path)\n",
        "        print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Checkpoint not found!\")\n",
        "            return\n",
        "\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        for name, net in self.nets.items():\n",
        "            net.load_state_dict(ckpt['nets'][name])\n",
        "        for name, opt in self.optims.items():\n",
        "            opt.load_state_dict(ckpt['optims'][name])\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "    def calculate_metrics(self, step):\n",
        "        \"\"\"\n",
        "        FID 및 LPIPS와 같은 정량적 평가지표를 계산합니다. (Placeholder)\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Iteration {step}: Evaluating Metrics ---\")\n",
        "        fid_score = 99.99\n",
        "        print(f\"FID Score: {fid_score:.4f} (낮을수록 좋음)\")\n",
        "        lpips_score = 0.00\n",
        "        print(f\"LPIPS Diversity Score: {lpips_score:.4f} (높을수록 좋음)\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "    def r1_loss(self, d_out, x_in): # R1 Loss 함수 추가\n",
        "        \"\"\"Discriminator의 R1 Gradient Penalty를 계산합니다.\"\"\"\n",
        "        grad_dout = torch_grad(\n",
        "            outputs=d_out.sum(), inputs=x_in,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]\n",
        "        grad_dout2 = grad_dout.pow(2)\n",
        "        assert(grad_dout2.size() == x_in.size())\n",
        "        r1_loss = grad_dout2.reshape(x_in.size(0), -1).sum(1).mean(0)\n",
        "        return r1_loss\n",
        "\n",
        "    def train(self):\n",
        "        print(\"--- 학습 시작 ---\")\n",
        "        nets = self.nets\n",
        "        optims = self.optims\n",
        "        args = self.args\n",
        "\n",
        "        data_iter = iter(self.loader)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for i in range(self.start_iter, args.total_iters):\n",
        "\n",
        "            # D를 G보다 d_train_repeats 만큼 더 학습시킵니다.\n",
        "            for d_repeat in range(args.d_train_repeats): # <-- D 반복 학습 루프 시작 (이 부분이 이전 코드에 없었습니다!)\n",
        "\n",
        "                # 1. 데이터 가져오기\n",
        "                try:\n",
        "                    x_real, y_org = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    data_iter = iter(self.loader)\n",
        "                    x_real, y_org = next(data_iter)\n",
        "\n",
        "                x_real = x_real.to(self.device)\n",
        "                y_org = y_org.to(self.device)\n",
        "\n",
        "                # R1 Loss 계산을 위해 x_real에 그래디언트 추적 활성화\n",
        "                x_real.requires_grad_(True)\n",
        "\n",
        "                # 타겟 도메인 및 Latent 생성\n",
        "                y_trg = torch.randint(0, args.num_domains, (x_real.size(0),)).to(self.device)\n",
        "                z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "                # =================================================================================== #\n",
        "                #                               1. Discriminator 학습                                 #\n",
        "                # =================================================================================== #\n",
        "\n",
        "                # Real Loss\n",
        "                d_out_real = nets['D'](x_real, y_org)\n",
        "                d_loss_real = torch.mean(F.relu(1.0 - d_out_real))\n",
        "\n",
        "                # R1 Regularization Loss 계산\n",
        "                d_loss_r1 = self.r1_loss(d_out_real, x_real)\n",
        "\n",
        "                # Fake Loss (Latent 기반 생성)\n",
        "                with torch.no_grad():\n",
        "                    s_trg = nets['F'](z_trg, y_trg)\n",
        "                    x_fake = nets['G'](x_real, s_trg)\n",
        "\n",
        "                d_out_fake = nets['D'](x_fake.detach(), y_trg)\n",
        "                d_loss_fake = torch.mean(F.relu(1.0 + d_out_fake))\n",
        "\n",
        "                # D Total Loss: Hinge Loss + R1 Loss\n",
        "                d_loss = d_loss_real + d_loss_fake + args.lambda_r1 * d_loss_r1\n",
        "\n",
        "                optims['D'].zero_grad()\n",
        "                d_loss.backward()\n",
        "                optims['D'].step()\n",
        "\n",
        "                # 그래디언트 추적 해제\n",
        "                x_real.requires_grad_(False)\n",
        "\n",
        "            # G 학습은 1번만 수행\n",
        "            # =================================================================================== #\n",
        "            #                     2. Generator, Mapping, Encoder 학습                             #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # G 학습에 사용할 z_trg, z_trg2는 여기서 생성\n",
        "            z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "            z_trg2 = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "            # Adversarial Loss\n",
        "            s_trg = nets['F'](z_trg, y_trg)\n",
        "            x_fake = nets['G'](x_real, s_trg)\n",
        "            d_out_fake = nets['D'](x_fake, y_trg)\n",
        "            g_loss_adv = -torch.mean(d_out_fake)\n",
        "\n",
        "            # Style Reconstruction Loss\n",
        "            s_pred = nets['E'](x_fake, y_trg)\n",
        "            g_loss_sty = torch.mean(torch.abs(s_trg - s_pred))\n",
        "\n",
        "            # Diversity Sensitive Loss\n",
        "            s_trg2 = nets['F'](z_trg2, y_trg)\n",
        "            x_fake2 = nets['G'](x_real, s_trg2)\n",
        "            g_loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            s_org = nets['E'](x_real, y_org)\n",
        "            x_rec = nets['G'](x_fake, s_org)\n",
        "            g_loss_cyc = torch.mean(torch.abs(x_real - x_rec))\n",
        "\n",
        "            # Total Loss\n",
        "            g_loss = g_loss_adv \\\n",
        "                     + args.lambda_sty * g_loss_sty \\\n",
        "                     - args.lambda_ds * g_loss_ds \\\n",
        "                     + args.lambda_cyc * g_loss_cyc\n",
        "\n",
        "            optims['G'].zero_grad()\n",
        "            optims['F'].zero_grad()\n",
        "            optims['E'].zero_grad()\n",
        "            g_loss.backward()\n",
        "            optims['G'].step()\n",
        "            optims['F'].step()\n",
        "            optims['E'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 3. 로깅 및 저장                                     #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            if (i + 1) % 200 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Iter [{i+1}/{args.total_iters}] Time: {elapsed:.2f}s | \"\n",
        "                      f\"D_loss: {d_loss.item():.4f} | G_adv: {g_loss_adv.item():.4f} | \"\n",
        "                      f\"Sty: {g_loss_sty.item():.4f} | Cyc: {g_loss_cyc.item():.4f}\")\n",
        "\n",
        "            if (i + 1) % args.sample_freq == 0:\n",
        "                self.save_samples(x_real, y_org, i + 1)\n",
        "\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                self.save_checkpoint(i + 1)\n",
        "                # self.calculate_metrics(i + 1) # 메트릭 계산 (필요 시 주석 해제)\n",
        "\n",
        "    def save_samples(self, x_real, y_org, step):\n",
        "        \"\"\"학습 중간 결과 이미지 저장 (도메인 라벨 포함 시각화 개선)\"\"\"\n",
        "        nets = self.nets\n",
        "        args = self.args\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nets['G'].eval()\n",
        "            nets['F'].eval()\n",
        "\n",
        "            x_real_subset = x_real[:args.num_domains].to(self.device)\n",
        "            # y_org_subset = y_org[:args.num_domains].cpu().numpy()\n",
        "\n",
        "            z_fix = torch.randn(1, args.latent_dim).repeat(args.num_domains, 1).to(self.device)\n",
        "            y_fix = torch.arange(args.num_domains).to(self.device)\n",
        "            s_fix = nets['F'](z_fix, y_fix)\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # 1. 첫 번째 행: 소스 이미지\n",
        "            source_row = [x_real_subset[i].cpu() for i in range(len(x_real_subset))]\n",
        "            images.extend(source_row)\n",
        "\n",
        "            # 2. 나머지 영역: 변환된 이미지 (스타일 변환 매트릭스)\n",
        "            for j in range(args.num_domains):\n",
        "                s_curr = s_fix[j].unsqueeze(0).repeat(x_real_subset.size(0), 1)\n",
        "                x_fake_row = nets['G'](x_real_subset, s_curr)\n",
        "                images.extend([x_fake_row[i].cpu() for i in range(len(x_real_subset))])\n",
        "\n",
        "            images = torch.stack(images, dim=0)\n",
        "\n",
        "            path = os.path.join(self.save_dir, 'samples', f'{step:06d}_grid.jpg')\n",
        "            save_image(images, path, nrow=len(x_real_subset), padding=2, normalize=True)\n",
        "            print(f\"Sample image grid saved to {path}\")\n",
        "\n",
        "        # 다시 학습 모드\n",
        "        nets['G'].train()\n",
        "        nets['F'].train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 시드 고정\n",
        "    torch.manual_seed(777)\n",
        "    np.random.seed(777)\n",
        "\n",
        "    # 설정 로드\n",
        "    config = get_config()\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Solver 시작\n",
        "    solver = Solver(config, device)\n",
        "    solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m3gRsH6MWpyA",
        "outputId": "3dec5c10-5500-40d0-c278-f1c57618f303"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- 환경 설정 중 ---\n",
            "Google Drive를 마운트합니다...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "저장 경로: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 12.8MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 205kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.80MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 27.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 학습 시작 ---\n",
            "Iter [200/100000] Time: 85.31s | D_loss: 0.3619 | G_adv: 2.1803 | Sty: 0.0092 | Cyc: 0.5022\n",
            "Iter [400/100000] Time: 170.25s | D_loss: 0.3233 | G_adv: 2.2750 | Sty: 0.0093 | Cyc: 0.5363\n",
            "Iter [600/100000] Time: 255.24s | D_loss: 0.1031 | G_adv: 2.1565 | Sty: 0.0088 | Cyc: 0.5252\n",
            "Iter [800/100000] Time: 340.20s | D_loss: 1.2751 | G_adv: 0.5870 | Sty: 0.0094 | Cyc: 0.3917\n",
            "Iter [1000/100000] Time: 425.15s | D_loss: 0.9198 | G_adv: 1.0834 | Sty: 0.0092 | Cyc: 0.4844\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/001000_grid.jpg\n",
            "Iter [1200/100000] Time: 510.41s | D_loss: 0.9839 | G_adv: 1.5327 | Sty: 0.0091 | Cyc: 0.3420\n",
            "Iter [1400/100000] Time: 595.34s | D_loss: 0.9238 | G_adv: 0.2540 | Sty: 0.0093 | Cyc: 0.3284\n",
            "Iter [1600/100000] Time: 680.31s | D_loss: 1.1009 | G_adv: 0.7650 | Sty: 0.0097 | Cyc: 0.3729\n",
            "Iter [1800/100000] Time: 765.28s | D_loss: 0.7991 | G_adv: 0.5457 | Sty: 0.0102 | Cyc: 0.3659\n",
            "Iter [2000/100000] Time: 850.22s | D_loss: 0.8823 | G_adv: 0.5556 | Sty: 0.0088 | Cyc: 0.3429\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/002000_grid.jpg\n",
            "Iter [2200/100000] Time: 935.43s | D_loss: 0.7185 | G_adv: 0.9678 | Sty: 0.0101 | Cyc: 0.3794\n",
            "Iter [2400/100000] Time: 1020.35s | D_loss: 0.4866 | G_adv: 1.0244 | Sty: 0.0096 | Cyc: 0.3584\n",
            "Iter [2600/100000] Time: 1105.30s | D_loss: 0.5974 | G_adv: 1.5076 | Sty: 0.0088 | Cyc: 0.4271\n",
            "Iter [2800/100000] Time: 1190.25s | D_loss: 0.5610 | G_adv: 2.2349 | Sty: 0.0099 | Cyc: 0.5251\n",
            "Iter [3000/100000] Time: 1275.21s | D_loss: 0.6144 | G_adv: 1.0569 | Sty: 0.0092 | Cyc: 0.4133\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/003000_grid.jpg\n",
            "Iter [3200/100000] Time: 1360.43s | D_loss: 0.4884 | G_adv: 2.0771 | Sty: 0.0089 | Cyc: 0.4737\n",
            "Iter [3400/100000] Time: 1445.40s | D_loss: 0.5765 | G_adv: 1.4014 | Sty: 0.0097 | Cyc: 0.4280\n",
            "Iter [3600/100000] Time: 1530.26s | D_loss: 0.2996 | G_adv: 1.2885 | Sty: 0.0093 | Cyc: 0.3843\n",
            "Iter [3800/100000] Time: 1615.20s | D_loss: 0.4544 | G_adv: 1.5023 | Sty: 0.0088 | Cyc: 0.4019\n",
            "Iter [4000/100000] Time: 1700.15s | D_loss: 0.3753 | G_adv: 2.4373 | Sty: 0.0093 | Cyc: 0.4331\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/004000_grid.jpg\n",
            "Iter [4200/100000] Time: 1785.42s | D_loss: 0.2578 | G_adv: 1.8864 | Sty: 0.0090 | Cyc: 0.4602\n",
            "Iter [4400/100000] Time: 1870.38s | D_loss: 0.7457 | G_adv: 0.8963 | Sty: 0.0095 | Cyc: 0.4077\n",
            "Iter [4600/100000] Time: 1955.35s | D_loss: 0.5420 | G_adv: 1.7118 | Sty: 0.0100 | Cyc: 0.4433\n",
            "Iter [4800/100000] Time: 2040.31s | D_loss: 0.7924 | G_adv: 2.1615 | Sty: 0.0102 | Cyc: 0.3659\n",
            "Iter [5000/100000] Time: 2125.28s | D_loss: 0.7768 | G_adv: 1.0340 | Sty: 0.0091 | Cyc: 0.3400\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/005000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/checkpoints/005000.ckpt\n",
            "Iter [5200/100000] Time: 2216.64s | D_loss: 0.7531 | G_adv: 1.8872 | Sty: 0.0089 | Cyc: 0.4144\n",
            "Iter [5400/100000] Time: 2301.56s | D_loss: 0.9853 | G_adv: 0.6295 | Sty: 0.0090 | Cyc: 0.5316\n",
            "Iter [5600/100000] Time: 2386.47s | D_loss: 0.7030 | G_adv: 0.6873 | Sty: 0.0077 | Cyc: 0.4064\n",
            "Iter [5800/100000] Time: 2471.37s | D_loss: 0.6768 | G_adv: 1.4892 | Sty: 0.0092 | Cyc: 0.4358\n",
            "Iter [6000/100000] Time: 2556.37s | D_loss: 0.5138 | G_adv: 1.7520 | Sty: 0.0098 | Cyc: 0.4782\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/006000_grid.jpg\n",
            "Iter [6200/100000] Time: 2641.68s | D_loss: 0.3595 | G_adv: 1.6075 | Sty: 0.0090 | Cyc: 0.4334\n",
            "Iter [6400/100000] Time: 2726.67s | D_loss: 0.5957 | G_adv: 1.5819 | Sty: 0.0107 | Cyc: 0.4277\n",
            "Iter [6600/100000] Time: 2811.66s | D_loss: 0.8607 | G_adv: 2.1943 | Sty: 0.0107 | Cyc: 0.5010\n",
            "Iter [6800/100000] Time: 2896.62s | D_loss: 0.4832 | G_adv: 1.6312 | Sty: 0.0094 | Cyc: 0.3418\n",
            "Iter [7000/100000] Time: 2981.60s | D_loss: 0.4126 | G_adv: 1.8888 | Sty: 0.0096 | Cyc: 0.5103\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/007000_grid.jpg\n",
            "Iter [7200/100000] Time: 3066.82s | D_loss: 0.4027 | G_adv: 2.1048 | Sty: 0.0087 | Cyc: 0.6406\n",
            "Iter [7400/100000] Time: 3151.81s | D_loss: 0.7110 | G_adv: 0.5899 | Sty: 0.0092 | Cyc: 0.3818\n",
            "Iter [7600/100000] Time: 3236.73s | D_loss: 0.5518 | G_adv: 1.4186 | Sty: 0.0096 | Cyc: 0.3898\n",
            "Iter [7800/100000] Time: 3321.68s | D_loss: 0.7370 | G_adv: 1.4232 | Sty: 0.0089 | Cyc: 0.5174\n",
            "Iter [8000/100000] Time: 3406.59s | D_loss: 0.2712 | G_adv: 1.2414 | Sty: 0.0082 | Cyc: 0.4361\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v5-1/samples/008000_grid.jpg\n",
            "Iter [8200/100000] Time: 3491.85s | D_loss: 0.4641 | G_adv: 2.4052 | Sty: 0.0099 | Cyc: 0.4632\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1794429952.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;31m# Solver 시작\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0msolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1794429952.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m                     \u001b[0mx_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'G'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_trg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0md_out_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0md_loss_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md_out_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, num_domains, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# 기존 설정과 모델 불러오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder\n",
        "\n",
        "def load_model(args, device, checkpoint_step=None):\n",
        "    \"\"\"저장된 체크포인트를 불러옵니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "\n",
        "    # 체크포인트 지정이 없으면 가장 마지막(최신) 파일 로드\n",
        "    if checkpoint_step is None:\n",
        "        ckpts = sorted([f for f in os.listdir(model_path) if f.endswith('.ckpt')])\n",
        "        if not ckpts:\n",
        "            raise FileNotFoundError(\"체크포인트가 없습니다!\")\n",
        "        latest_ckpt = ckpts[-1]\n",
        "    else:\n",
        "        latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 모델 초기화 및 가중치 로드\n",
        "    generator = Generator(args.img_size, args.style_dim).to(device)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device)\n",
        "    # Style Encoder는 Reference Guided Synthesis 할 때 필요 (여기서는 Latent Guided만 시연)\n",
        "\n",
        "    generator.load_state_dict(ckpt['nets']['G'])\n",
        "    mapping_network.load_state_dict(ckpt['nets']['F'])\n",
        "\n",
        "    generator.eval()\n",
        "    mapping_network.eval()\n",
        "\n",
        "    return generator, mapping_network\n",
        "\n",
        "def inference(args, device):\n",
        "    # 1. 모델 로드\n",
        "    generator, mapping_net = load_model(args, device)\n",
        "\n",
        "    # 2. 테스트 데이터 로드 (학습에 안 쓴 데이터)\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True) # 10장만 샘플링\n",
        "\n",
        "    # 3. 소스 이미지 가져오기\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    # 4. 시각화 준비\n",
        "    domain_labels = get_domain_labels()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(11, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    # 고정된 Random Noise z 하나를 모든 도메인에 적용해 봅니다.\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains): # 0~9 (각 도메인별)\n",
        "        # 해당 도메인(row_idx)의 스타일 코드 생성\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = mapping_net(z_trg, y_trg) # (1, style_dim)\n",
        "\n",
        "        # 스타일 코드를 배치 크기만큼 복사 (1 -> 10)\n",
        "        s_trg = s_trg.repeat(10, 1)\n",
        "\n",
        "        # 이미지 생성\n",
        "        with torch.no_grad():\n",
        "            x_fake = generator(x_real, s_trg)\n",
        "\n",
        "        # 결과 출력\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(11, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 왼쪽 첫 열에만 도메인 이름 표시\n",
        "            if col_idx == 0:\n",
        "                plt.text(-10, 32, domain_labels[row_idx], fontsize=10, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "if __name__ == '__main__':\n",
        "    config = get_config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inference(config, device)"
      ],
      "metadata": {
        "id": "7DKw-bIshf8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}