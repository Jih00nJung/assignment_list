{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jih00nJung/assignment_list/blob/main/GAN_FMNIST_v3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "stop"
      ],
      "metadata": {
        "id": "hh-NIjg7sPZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. config.py (설정 및 환경 준비)"
      ],
      "metadata": {
        "id": "VwJWmulsWjmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKr3U21KWYwp",
        "outputId": "262f248c-1821-4520-a67e-daed7c4774f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile config.py\n",
        "import os\n",
        "import argparse\n",
        "from torchvision import transforms\n",
        "from google.colab import drive\n",
        "\n",
        "def get_config():\n",
        "    \"\"\"학습에 필요한 모든 하이퍼파라미터를 정의합니다.\"\"\"\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # 데이터 및 경로 설정\n",
        "    parser.add_argument('--project_name', type=str, default='GAN_FMNIST_v3-2')\n",
        "    parser.add_argument('--save_root', type=str, default='/content/drive/MyDrive/Colab Notebooks/GAN_assignment')\n",
        "    parser.add_argument('--img_size', type=int, default=64, help='이미지 크기 (FMNIST 기본 28 -> 64 리사이즈)')\n",
        "    parser.add_argument('--batch_size', type=int, default=16)\n",
        "\n",
        "    # 모델 하이퍼파라미터\n",
        "    parser.add_argument('--style_dim', type=int, default=64, help='스타일 코드 차원')\n",
        "    parser.add_argument('--latent_dim', type=int, default=16, help='랜덤 노이즈 차원')\n",
        "    parser.add_argument('--num_domains', type=int, default=10, help='Fashion MNIST 클래스 개수')\n",
        "    parser.add_argument('--hidden_dim', type=int, default=256, help='Mapping Network 히든 차원')\n",
        "\n",
        "    # 학습 설정\n",
        "    parser.add_argument('--total_iters', type=int, default=100000)\n",
        "    parser.add_argument('--resume_iter', type=int, default=20000)\n",
        "    parser.add_argument('--lr', type=float, default=1e-4)\n",
        "    parser.add_argument('--w_hpf', type=float, default=1, help='High-pass filtering 가중치')\n",
        "    parser.add_argument('--d_train_repeats', type=int, default=5, help='Discriminator 학습 반복 횟수') # 추가된 부분\n",
        "\n",
        "    # Loss 가중치\n",
        "    parser.add_argument('--lambda_sty', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_ds', type=float, default=5.0)\n",
        "    parser.add_argument('--lambda_cyc', type=float, default=0)\n",
        "    parser.add_argument('--lambda_r1', type=float, default=1.0, help='R1 regularization loss 가중치') # R1 loss 가중치 추가\n",
        "\n",
        "    # 로깅 주기\n",
        "    parser.add_argument('--sample_freq', type=int, default=1000)\n",
        "    parser.add_argument('--save_freq', type=int, default=5000)\n",
        "\n",
        "    # FID/LPIPS 평가용 설정\n",
        "    parser.add_argument('--num_fid_samples', type=int, default=1000, help='FID 계산에 사용할 생성 이미지 수')\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "    return args\n",
        "\n",
        "def prepare_environment(args):\n",
        "    \"\"\"Google Drive 마운트 및 체크포인트/샘플 디렉토리를 생성합니다.\"\"\"\n",
        "    print(\"--- 환경 설정 중 ---\")\n",
        "    save_path = os.path.join(args.save_root, args.project_name)\n",
        "\n",
        "    # Google Drive 마운트\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        print(\"Google Drive를 마운트합니다...\")\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted.\")\n",
        "\n",
        "    os.makedirs(os.path.join(save_path, 'checkpoints'), exist_ok=True)\n",
        "    os.makedirs(os.path.join(save_path, 'samples'), exist_ok=True)\n",
        "    print(f\"저장 경로: {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "def get_data_transform(img_size):\n",
        "    \"\"\"Fashion MNIST 데이터 전처리를 정의합니다.\"\"\"\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=(0.5,), std=(0.5,)) # [0,1] -> [-1,1]\n",
        "    ])\n",
        "\n",
        "def get_domain_labels():\n",
        "    \"\"\"Fashion MNIST의 10개 도메인 레이블을 반환합니다.\"\"\"\n",
        "    return ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. model.py (네트워크 아키텍처)"
      ],
      "metadata": {
        "id": "ywi2i9fZWl3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class AdaIN(nn.Module):\n",
        "    \"\"\"Adaptive Instance Normalization\"\"\"\n",
        "    def __init__(self, style_dim, num_features):\n",
        "        super().__init__()\n",
        "        # 1. 정규화 도구 (학습 파라미터 없음, 단순 통계 정규화)\n",
        "        self.norm = nn.InstanceNorm2d(num_features, affine=False)\n",
        "        # 2. 스타일 코드 s를 변환하여 감마(스케일)와 베타(시프트)를 만드는 선형 층\n",
        "        self.fc = nn.Linear(style_dim, num_features * 2)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        # s를 통해 파라미터 생성 (h)\n",
        "        h = self.fc(s)\n",
        "        h = h.view(h.size(0), h.size(1), 1, 1)\n",
        "        # 생성된 파라미터를 감마와 베타로 나눔\n",
        "        gamma, beta = torch.chunk(h, chunks=2, dim=1)\n",
        "        # 정규화된 x에 감마를 곱하고 베타를 더함 -> 스타일 주입\n",
        "        return (1 + gamma) * self.norm(x) + beta\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    \"\"\"기본 ResBlock (다운샘플링 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            actv,\n",
        "            nn.Conv2d(dim_in, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            actv,\n",
        "            nn.Conv2d(dim_out, dim_out, 3, 1, 1),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True)\n",
        "        )\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "class AdaINResBlock(nn.Module):\n",
        "    \"\"\"Generator용 AdaIN ResBlock (Bottleneck 및 Up-sampling 블록에서 사용)\"\"\"\n",
        "    def __init__(self, dim_in, dim_out, style_dim, actv=nn.LeakyReLU(0.2)):\n",
        "        super().__init__()\n",
        "        self.actv = actv\n",
        "        self.conv1 = nn.Conv2d(dim_in, dim_out, 3, 1, 1)\n",
        "        self.conv2 = nn.Conv2d(dim_out, dim_out, 3, 1, 1)\n",
        "        self.norm1 = AdaIN(style_dim, dim_in)\n",
        "        self.norm2 = AdaIN(style_dim, dim_out)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if dim_in != dim_out:\n",
        "            self.shortcut = nn.Conv2d(dim_in, dim_out, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        out = self.norm1(x, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.norm2(out, s)\n",
        "        out = self.actv(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + self.shortcut(x)\n",
        "\n",
        "# --- (1) Generator (G) ---\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, max_conv_dim=512):\n",
        "        super().__init__()\n",
        "        dim_in = 64  # 경량화를 위한 시작 필터 수\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # 1. 입력부: 흑백(1채널) 이미지를 받아서 32채널 특징 맵으로 변환\n",
        "        self.from_rgb = nn.Conv2d(1, dim_in, 3, 1, 1) # Grayscale 1채널 입력\n",
        "\n",
        "        # 2. 인코더 (Down-sampling): 형태 정보 압축\n",
        "        # Down-sampling blocks (64 -> 32 -> 16 -> 8)\n",
        "        self.encode = nn.ModuleList()\n",
        "        curr_dim = dim_in\n",
        "        for _ in range(3): # 3번 다운샘플링하여 8x8 병목 생성\n",
        "            self.encode.append(ResBlock(curr_dim, curr_dim * 2))\n",
        "            self.encode.append(nn.AvgPool2d(2))\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        # 3. 병목 (Bottleneck): 스타일 주입 시작\n",
        "        # Bottleneck (8x8 유지, AdaIN 적용)\n",
        "        self.decode = nn.ModuleList()\n",
        "        curr_dim = min(curr_dim, max_conv_dim)\n",
        "        for _ in range(2):\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim, style_dim))\n",
        "\n",
        "        # 4. 디코더 (Up-sampling): 이미지 복원 + 스타일 입히기\n",
        "        # Up-sampling blocks (8 -> 16 -> 32 -> 64)\n",
        "        for _ in range(3):\n",
        "            self.decode.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "            self.decode.append(AdaINResBlock(curr_dim, curr_dim // 2, style_dim))\n",
        "            curr_dim = curr_dim // 2\n",
        "\n",
        "        # 5. 출력부: 최종적으로 1채널(흑백) 이미지로 변환\n",
        "        # Final Conv\n",
        "        self.to_rgb = nn.Sequential(\n",
        "            nn.InstanceNorm2d(curr_dim, affine=True),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(curr_dim, 1, 1, 1, 0) # Grayscale 1채널 출력\n",
        "        )\n",
        "\n",
        "    def forward(self, x, s):\n",
        "        x = self.from_rgb(x)\n",
        "        for block in self.encode:\n",
        "            x = block(x)\n",
        "\n",
        "        for block in self.decode:\n",
        "            if isinstance(block, AdaINResBlock):\n",
        "                x = block(x, s)\n",
        "            else:\n",
        "                x = block(x)\n",
        "\n",
        "        return self.to_rgb(x)\n",
        "\n",
        "# --- (2) Mapping Network (F) ---\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, latent_dim=16, style_dim=64, num_domains=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        # 공유 레이어 (Shared)\n",
        "        for _ in range(3):\n",
        "            layers += [nn.Linear(latent_dim if not layers else hidden_dim, hidden_dim)]\n",
        "            layers += [nn.ReLU()]\n",
        "        # 1. 공유 레이어 (Shared): 모든 도메인이 공통으로 사용하는 특징 추출\n",
        "        self.shared = nn.Sequential(*layers)\n",
        "\n",
        "        # 2. 비공유 레이어 (Unshared): 각 도메인(T-shirt, Pants...)별 전용 스타일 생성기\n",
        "        # 도메인별 출력 레이어 (Unshared)\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, style_dim)\n",
        "            ))\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        h = self.shared(z)\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, style_dim)\n",
        "\n",
        "        # 사용자가 요청한 도메인(y)에 해당하는 스타일만 쏙 뽑아서 리턴\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y] # 해당 도메인의 스타일 코드만 선택\n",
        "        return s\n",
        "\n",
        "# --- (3) Style Encoder (E) ---\n",
        "class StyleEncoder(nn.Module):\n",
        "    def __init__(self, img_size=64, style_dim=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지를 보며 특징을 추출 (CNN 구조)\n",
        "        # 64 -> 32 -> 16 -> 8 로 줄어들며 추상적인 특징을 잡아냄\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 비공유 레이어: 추출된 특징을 보고 \"이건 바지 스타일로는 s_pants, 티셔츠로는 s_shirt야\" 라고 해석\n",
        "        # 도메인별 Style Code 출력\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, style_dim))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # 이미지 x에서 시각적 특징 추출\n",
        "        h = self.shared(x) # (batch, curr_dim, 1, 1)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        s = out[idx, y]\n",
        "        return s\n",
        "\n",
        "# --- (4) Discriminator (D) ---\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_size=64, num_domains=10):\n",
        "        super().__init__()\n",
        "        dim_in = 32\n",
        "        blocks = []\n",
        "        blocks += [nn.Conv2d(1, dim_in, 3, 1, 1)] # 1ch input\n",
        "\n",
        "        curr_dim = dim_in\n",
        "        # Downsample to small size (64 -> 8)\n",
        "        for _ in range(3): # 64 -> 32 -> 16 -> 8\n",
        "            blocks += [nn.LeakyReLU(0.2)]\n",
        "            blocks += [nn.Conv2d(curr_dim, curr_dim * 2, 3, 2, 1)]\n",
        "            curr_dim = curr_dim * 2\n",
        "\n",
        "        blocks += [nn.LeakyReLU(0.2)]\n",
        "        blocks += [nn.Conv2d(curr_dim, curr_dim, 8, 1, 0)] # 8x8 -> 1x1\n",
        "        # 1. 공유 레이어: 이미지가 진짜인지 가짜인지 판단하기 위한 단서(특징) 추출\n",
        "        # ResBlock이나 Conv 레이어를 사용하여 이미지를 분석함\n",
        "        self.shared = nn.Sequential(*blocks)\n",
        "\n",
        "        # 2. 멀티 태스크 헤드: 각 도메인별로 진위 여부를 따로 판별\n",
        "        # 도메인별 진위 판별 헤드\n",
        "        self.unshared = nn.ModuleList()\n",
        "        for _ in range(num_domains):\n",
        "            self.unshared.append(nn.Linear(curr_dim, 1))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        h = self.shared(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "\n",
        "        out = []\n",
        "        for layer in self.unshared:\n",
        "            out += [layer(h)]\n",
        "        out = torch.stack(out, dim=1) # (batch, num_domains, 1)\n",
        "\n",
        "        idx = torch.arange(y.size(0)).to(y.device)\n",
        "        score = out[idx, y]\n",
        "        return score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1ozdU0eWnJ3",
        "outputId": "311a8930-90a6-42b0-c23c-cacc010ffc85"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. solver.py (메인 실행 및 학습 루프)"
      ],
      "metadata": {
        "id": "m__DYADCWoiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.utils import save_image, make_grid\n",
        "from torch.autograd import grad as torch_grad # R1 정규화에 필요\n",
        "\n",
        "# 분리된 파일에서 모듈 가져오기\n",
        "from config import get_config, prepare_environment, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder, Discriminator\n",
        "\n",
        "\n",
        "class Solver:\n",
        "    def __init__(self, args, device):\n",
        "        self.args = args\n",
        "        self.device = device\n",
        "        self.save_dir = prepare_environment(args)\n",
        "        self.domain_labels = get_domain_labels()\n",
        "\n",
        "        # 데이터셋 준비 (FashionMNIST)\n",
        "        transform = get_data_transform(args.img_size)\n",
        "\n",
        "        # 학습용 데이터 로더\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "        self.loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "        # 평가용 데이터 로더 (FID/LPIPS 계산 시 사용될 예정)\n",
        "        self.eval_loader = self.get_eval_loader(transform)\n",
        "\n",
        "        # 모델 초기화\n",
        "        # Generator의 채널 용량이 model.py에서 64/512로 증가했다고 가정하고 초기화\n",
        "        self.nets = {\n",
        "            'G': Generator(args.img_size, args.style_dim),\n",
        "            'F': MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim),\n",
        "            'E': StyleEncoder(args.img_size, args.style_dim, args.num_domains),\n",
        "            'D': Discriminator(args.img_size, args.num_domains)\n",
        "        }\n",
        "\n",
        "        for name, module in self.nets.items():\n",
        "            module.to(self.device)\n",
        "            module.train()\n",
        "\n",
        "        # 옵티마이저\n",
        "        self.optims = {\n",
        "            'G': torch.optim.Adam(self.nets['G'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'F': torch.optim.Adam(self.nets['F'].parameters(), lr=args.lr*0.01, betas=(0.0, 0.99)),\n",
        "            'E': torch.optim.Adam(self.nets['E'].parameters(), lr=args.lr, betas=(0.0, 0.99)),\n",
        "            'D': torch.optim.Adam(self.nets['D'].parameters(), lr=args.lr, betas=(0.0, 0.99))\n",
        "        }\n",
        "\n",
        "        # 체크포인트 로드\n",
        "        self.start_iter = 0\n",
        "        if args.resume_iter > 0:\n",
        "            self.load_checkpoint(args.resume_iter)\n",
        "            self.start_iter = args.resume_iter\n",
        "\n",
        "    def get_eval_loader(self, transform):\n",
        "        \"\"\"FID/LPIPS 계산을 위한 평가용 데이터 로더를 준비합니다.\"\"\"\n",
        "        dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "        return DataLoader(dataset, batch_size=self.args.batch_size, shuffle=False, num_workers=2, drop_last=False)\n",
        "\n",
        "    def save_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        state = {\n",
        "            'nets': {name: net.state_dict() for name, net in self.nets.items()},\n",
        "            'optims': {name: opt.state_dict() for name, opt in self.optims.items()},\n",
        "            'step': step\n",
        "        }\n",
        "        torch.save(state, path)\n",
        "        print(f\"Saved checkpoint to {path}\")\n",
        "\n",
        "    def load_checkpoint(self, step):\n",
        "        path = os.path.join(self.save_dir, 'checkpoints', f'{step:06d}.ckpt')\n",
        "        if not os.path.exists(path):\n",
        "            print(\"Checkpoint not found!\")\n",
        "            return\n",
        "\n",
        "        ckpt = torch.load(path, map_location=self.device)\n",
        "        for name, net in self.nets.items():\n",
        "            net.load_state_dict(ckpt['nets'][name])\n",
        "        for name, opt in self.optims.items():\n",
        "            opt.load_state_dict(ckpt['optims'][name])\n",
        "        print(f\"Loaded checkpoint from {path}\")\n",
        "\n",
        "    def calculate_metrics(self, step):\n",
        "        \"\"\"\n",
        "        FID 및 LPIPS와 같은 정량적 평가지표를 계산합니다. (Placeholder)\n",
        "        \"\"\"\n",
        "        print(f\"\\n--- Iteration {step}: Evaluating Metrics ---\")\n",
        "        fid_score = 99.99\n",
        "        print(f\"FID Score: {fid_score:.4f} (낮을수록 좋음)\")\n",
        "        lpips_score = 0.00\n",
        "        print(f\"LPIPS Diversity Score: {lpips_score:.4f} (높을수록 좋음)\")\n",
        "        print(\"------------------------------------------\\n\")\n",
        "\n",
        "    def r1_loss(self, d_out, x_in): # R1 Loss 함수 추가\n",
        "        \"\"\"Discriminator의 R1 Gradient Penalty를 계산합니다.\"\"\"\n",
        "        grad_dout = torch_grad(\n",
        "            outputs=d_out.sum(), inputs=x_in,\n",
        "            create_graph=True, retain_graph=True, only_inputs=True\n",
        "        )[0]\n",
        "        grad_dout2 = grad_dout.pow(2)\n",
        "        assert(grad_dout2.size() == x_in.size())\n",
        "        r1_loss = grad_dout2.reshape(x_in.size(0), -1).sum(1).mean(0)\n",
        "        return r1_loss\n",
        "\n",
        "    def train(self):\n",
        "        print(\"--- 학습 시작 ---\")\n",
        "        nets = self.nets\n",
        "        optims = self.optims\n",
        "        args = self.args\n",
        "\n",
        "        data_iter = iter(self.loader)\n",
        "\n",
        "        start_time = time.time()\n",
        "        for i in range(self.start_iter, args.total_iters):\n",
        "\n",
        "            # D를 G보다 d_train_repeats 만큼 더 학습시킵니다.\n",
        "            for d_repeat in range(args.d_train_repeats): # <-- D 반복 학습 루프 시작 (이 부분이 이전 코드에 없었습니다!)\n",
        "\n",
        "                # 1. 데이터 가져오기\n",
        "                try:\n",
        "                    x_real, y_org = next(data_iter)\n",
        "                except StopIteration:\n",
        "                    data_iter = iter(self.loader)\n",
        "                    x_real, y_org = next(data_iter)\n",
        "\n",
        "                x_real = x_real.to(self.device)\n",
        "                y_org = y_org.to(self.device)\n",
        "\n",
        "                # R1 Loss 계산을 위해 x_real에 그래디언트 추적 활성화\n",
        "                x_real.requires_grad_(True)\n",
        "\n",
        "                # 타겟 도메인 및 Latent 생성\n",
        "                y_trg = torch.randint(0, args.num_domains, (x_real.size(0),)).to(self.device)\n",
        "                z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "                # =================================================================================== #\n",
        "                #                               1. Discriminator 학습                                 #\n",
        "                # =================================================================================== #\n",
        "\n",
        "                # Real Loss\n",
        "                d_out_real = nets['D'](x_real, y_org)\n",
        "                d_loss_real = torch.mean(F.relu(1.0 - d_out_real))\n",
        "\n",
        "                # R1 Regularization Loss 계산\n",
        "                d_loss_r1 = self.r1_loss(d_out_real, x_real)\n",
        "\n",
        "                # Fake Loss (Latent 기반 생성)\n",
        "                with torch.no_grad():\n",
        "                    s_trg = nets['F'](z_trg, y_trg)\n",
        "                    x_fake = nets['G'](x_real, s_trg)\n",
        "\n",
        "                d_out_fake = nets['D'](x_fake.detach(), y_trg)\n",
        "                d_loss_fake = torch.mean(F.relu(1.0 + d_out_fake))\n",
        "\n",
        "                # D Total Loss: Hinge Loss + R1 Loss\n",
        "                d_loss = d_loss_real + d_loss_fake + args.lambda_r1 * d_loss_r1\n",
        "\n",
        "                optims['D'].zero_grad()\n",
        "                d_loss.backward()\n",
        "                optims['D'].step()\n",
        "\n",
        "                # 그래디언트 추적 해제\n",
        "                x_real.requires_grad_(False)\n",
        "\n",
        "            # G 학습은 1번만 수행\n",
        "            # =================================================================================== #\n",
        "            #                     2. Generator, Mapping, Encoder 학습                             #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            # G 학습에 사용할 z_trg, z_trg2는 여기서 생성\n",
        "            z_trg = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "            z_trg2 = torch.randn(x_real.size(0), args.latent_dim).to(self.device)\n",
        "\n",
        "            # Adversarial Loss\n",
        "            s_trg = nets['F'](z_trg, y_trg)\n",
        "            x_fake = nets['G'](x_real, s_trg)\n",
        "            d_out_fake = nets['D'](x_fake, y_trg)\n",
        "            g_loss_adv = -torch.mean(d_out_fake)\n",
        "\n",
        "            # Style Reconstruction Loss\n",
        "            s_pred = nets['E'](x_fake, y_trg)\n",
        "            g_loss_sty = torch.mean(torch.abs(s_trg - s_pred))\n",
        "\n",
        "            # Diversity Sensitive Loss\n",
        "            s_trg2 = nets['F'](z_trg2, y_trg)\n",
        "            x_fake2 = nets['G'](x_real, s_trg2)\n",
        "            g_loss_ds = torch.mean(torch.abs(x_fake - x_fake2))\n",
        "\n",
        "            # Cycle Consistency Loss\n",
        "            s_org = nets['E'](x_real, y_org)\n",
        "            x_rec = nets['G'](x_fake, s_org)\n",
        "            g_loss_cyc = torch.mean(torch.abs(x_real - x_rec))\n",
        "\n",
        "            # Total Loss\n",
        "            g_loss = g_loss_adv \\\n",
        "                     + args.lambda_sty * g_loss_sty \\\n",
        "                     - args.lambda_ds * g_loss_ds \\\n",
        "                     + args.lambda_cyc * g_loss_cyc\n",
        "\n",
        "            optims['G'].zero_grad()\n",
        "            optims['F'].zero_grad()\n",
        "            optims['E'].zero_grad()\n",
        "            g_loss.backward()\n",
        "            optims['G'].step()\n",
        "            optims['F'].step()\n",
        "            optims['E'].step()\n",
        "\n",
        "            # =================================================================================== #\n",
        "            #                                 3. 로깅 및 저장                                     #\n",
        "            # =================================================================================== #\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                print(f\"Iter [{i+1}/{args.total_iters}] Time: {elapsed:.2f}s | \"\n",
        "                      f\"D_loss: {d_loss.item():.4f} | G_adv: {g_loss_adv.item():.4f} | \"\n",
        "                      f\"Sty: {g_loss_sty.item():.4f} | Cyc: {g_loss_cyc.item():.4f}\")\n",
        "\n",
        "            if (i + 1) % args.sample_freq == 0:\n",
        "                self.save_samples(x_real, y_org, i + 1)\n",
        "\n",
        "            if (i + 1) % args.save_freq == 0:\n",
        "                self.save_checkpoint(i + 1)\n",
        "                # self.calculate_metrics(i + 1) # 메트릭 계산 (필요 시 주석 해제)\n",
        "\n",
        "    def save_samples(self, x_real, y_org, step):\n",
        "        \"\"\"학습 중간 결과 이미지 저장 (도메인 라벨 포함 시각화 개선)\"\"\"\n",
        "        nets = self.nets\n",
        "        args = self.args\n",
        "\n",
        "        with torch.no_grad():\n",
        "            nets['G'].eval()\n",
        "            nets['F'].eval()\n",
        "\n",
        "            x_real_subset = x_real[:args.num_domains].to(self.device)\n",
        "            # y_org_subset = y_org[:args.num_domains].cpu().numpy()\n",
        "\n",
        "            z_fix = torch.randn(1, args.latent_dim).repeat(args.num_domains, 1).to(self.device)\n",
        "            y_fix = torch.arange(args.num_domains).to(self.device)\n",
        "            s_fix = nets['F'](z_fix, y_fix)\n",
        "\n",
        "            images = []\n",
        "\n",
        "            # 1. 첫 번째 행: 소스 이미지\n",
        "            source_row = [x_real_subset[i].cpu() for i in range(len(x_real_subset))]\n",
        "            images.extend(source_row)\n",
        "\n",
        "            # 2. 나머지 영역: 변환된 이미지 (스타일 변환 매트릭스)\n",
        "            for j in range(args.num_domains):\n",
        "                s_curr = s_fix[j].unsqueeze(0).repeat(x_real_subset.size(0), 1)\n",
        "                x_fake_row = nets['G'](x_real_subset, s_curr)\n",
        "                images.extend([x_fake_row[i].cpu() for i in range(len(x_real_subset))])\n",
        "\n",
        "            images = torch.stack(images, dim=0)\n",
        "\n",
        "            path = os.path.join(self.save_dir, 'samples', f'{step:06d}_grid.jpg')\n",
        "            save_image(images, path, nrow=len(x_real_subset), padding=2, normalize=True)\n",
        "            print(f\"Sample image grid saved to {path}\")\n",
        "\n",
        "        # 다시 학습 모드\n",
        "        nets['G'].train()\n",
        "        nets['F'].train()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # 시드 고정\n",
        "    torch.manual_seed(777)\n",
        "    np.random.seed(777)\n",
        "\n",
        "    # 설정 로드\n",
        "    config = get_config()\n",
        "\n",
        "    # 장치 설정\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Solver 시작\n",
        "    solver = Solver(config, device)\n",
        "    solver.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3gRsH6MWpyA",
        "outputId": "3da6817a-c347-4188-ac9d-c1c7ecdbf587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "--- 환경 설정 중 ---\n",
            "Google Drive를 마운트합니다...\n",
            "Mounted at /content/drive\n",
            "Google Drive mounted.\n",
            "저장 경로: /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:02<00:00, 10.6MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 191kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:01<00:00, 3.51MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 13.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/checkpoints/020000.ckpt\n",
            "--- 학습 시작 ---\n",
            "Iter [20100/100000] Time: 65.04s | D_loss: 1.0919 | G_adv: 0.7816 | Sty: 0.0087 | Cyc: 0.5103\n",
            "Iter [20200/100000] Time: 128.46s | D_loss: 0.8116 | G_adv: 0.7108 | Sty: 0.0092 | Cyc: 0.4514\n",
            "Iter [20300/100000] Time: 191.83s | D_loss: 0.7829 | G_adv: 0.7503 | Sty: 0.0089 | Cyc: 0.4410\n",
            "Iter [20400/100000] Time: 255.16s | D_loss: 1.4249 | G_adv: 1.0443 | Sty: 0.0084 | Cyc: 0.4021\n",
            "Iter [20500/100000] Time: 318.57s | D_loss: 1.2057 | G_adv: 1.0511 | Sty: 0.0093 | Cyc: 0.4368\n",
            "Iter [20600/100000] Time: 382.01s | D_loss: 0.9485 | G_adv: 1.0561 | Sty: 0.0095 | Cyc: 0.4071\n",
            "Iter [20700/100000] Time: 445.27s | D_loss: 0.7829 | G_adv: 1.1381 | Sty: 0.0095 | Cyc: 0.5695\n",
            "Iter [20800/100000] Time: 508.74s | D_loss: 1.1785 | G_adv: 0.5817 | Sty: 0.0089 | Cyc: 0.4003\n",
            "Iter [20900/100000] Time: 572.13s | D_loss: 1.0063 | G_adv: 0.2179 | Sty: 0.0092 | Cyc: 0.5262\n",
            "Iter [21000/100000] Time: 635.42s | D_loss: 0.9902 | G_adv: 0.1104 | Sty: 0.0082 | Cyc: 0.4443\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/021000_grid.jpg\n",
            "Iter [21100/100000] Time: 700.57s | D_loss: 1.1259 | G_adv: 1.0270 | Sty: 0.0084 | Cyc: 0.3559\n",
            "Iter [21200/100000] Time: 763.96s | D_loss: 1.0242 | G_adv: 1.1747 | Sty: 0.0095 | Cyc: 0.4209\n",
            "Iter [21300/100000] Time: 827.28s | D_loss: 1.1023 | G_adv: 1.3642 | Sty: 0.0081 | Cyc: 0.3841\n",
            "Iter [21400/100000] Time: 890.68s | D_loss: 0.8593 | G_adv: 1.2064 | Sty: 0.0090 | Cyc: 0.3911\n",
            "Iter [21500/100000] Time: 953.96s | D_loss: 0.7914 | G_adv: 0.7162 | Sty: 0.0096 | Cyc: 0.4857\n",
            "Iter [21600/100000] Time: 1017.53s | D_loss: 0.9913 | G_adv: 0.9730 | Sty: 0.0091 | Cyc: 0.3764\n",
            "Iter [21700/100000] Time: 1080.78s | D_loss: 1.1346 | G_adv: 0.7712 | Sty: 0.0094 | Cyc: 0.4090\n",
            "Iter [21800/100000] Time: 1144.08s | D_loss: 0.5745 | G_adv: 0.8796 | Sty: 0.0089 | Cyc: 0.4781\n",
            "Iter [21900/100000] Time: 1207.39s | D_loss: 0.9971 | G_adv: 1.2708 | Sty: 0.0090 | Cyc: 0.4353\n",
            "Iter [22000/100000] Time: 1270.67s | D_loss: 1.0045 | G_adv: 0.4424 | Sty: 0.0099 | Cyc: 0.3485\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/022000_grid.jpg\n",
            "Iter [22100/100000] Time: 1334.25s | D_loss: 1.3020 | G_adv: 1.9342 | Sty: 0.0087 | Cyc: 0.4871\n",
            "Iter [22200/100000] Time: 1397.67s | D_loss: 0.8724 | G_adv: 1.2299 | Sty: 0.0090 | Cyc: 0.3862\n",
            "Iter [22300/100000] Time: 1461.20s | D_loss: 1.0256 | G_adv: 0.8618 | Sty: 0.0089 | Cyc: 0.5179\n",
            "Iter [22400/100000] Time: 1524.54s | D_loss: 0.9548 | G_adv: 0.5957 | Sty: 0.0094 | Cyc: 0.4881\n",
            "Iter [22500/100000] Time: 1587.96s | D_loss: 0.9736 | G_adv: 0.7984 | Sty: 0.0080 | Cyc: 0.5046\n",
            "Iter [22600/100000] Time: 1651.35s | D_loss: 0.6000 | G_adv: 1.2444 | Sty: 0.0096 | Cyc: 0.3734\n",
            "Iter [22700/100000] Time: 1714.94s | D_loss: 1.1507 | G_adv: 0.5592 | Sty: 0.0092 | Cyc: 0.3812\n",
            "Iter [22800/100000] Time: 1778.33s | D_loss: 1.2185 | G_adv: 1.8712 | Sty: 0.0086 | Cyc: 0.4706\n",
            "Iter [22900/100000] Time: 1841.51s | D_loss: 1.0084 | G_adv: 1.6440 | Sty: 0.0092 | Cyc: 0.6115\n",
            "Iter [23000/100000] Time: 1904.69s | D_loss: 1.0095 | G_adv: 0.8299 | Sty: 0.0095 | Cyc: 0.3027\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/023000_grid.jpg\n",
            "Iter [23100/100000] Time: 1968.28s | D_loss: 0.9744 | G_adv: 0.5912 | Sty: 0.0089 | Cyc: 0.5385\n",
            "Iter [23200/100000] Time: 2031.32s | D_loss: 1.0018 | G_adv: 1.2910 | Sty: 0.0088 | Cyc: 0.5559\n",
            "Iter [23300/100000] Time: 2094.49s | D_loss: 1.5720 | G_adv: 0.8328 | Sty: 0.0088 | Cyc: 0.4454\n",
            "Iter [23400/100000] Time: 2157.40s | D_loss: 1.0611 | G_adv: 1.3431 | Sty: 0.0083 | Cyc: 0.4478\n",
            "Iter [23500/100000] Time: 2220.34s | D_loss: 1.1745 | G_adv: 1.3161 | Sty: 0.0103 | Cyc: 0.5000\n",
            "Iter [23600/100000] Time: 2283.27s | D_loss: 0.6679 | G_adv: 1.0158 | Sty: 0.0088 | Cyc: 0.4234\n",
            "Iter [23700/100000] Time: 2346.16s | D_loss: 0.9509 | G_adv: 1.3120 | Sty: 0.0083 | Cyc: 0.4095\n",
            "Iter [23800/100000] Time: 2409.29s | D_loss: 0.8325 | G_adv: 1.3614 | Sty: 0.0089 | Cyc: 0.5722\n",
            "Iter [23900/100000] Time: 2472.28s | D_loss: 1.1840 | G_adv: 0.7780 | Sty: 0.0094 | Cyc: 0.4823\n",
            "Iter [24000/100000] Time: 2535.52s | D_loss: 1.0135 | G_adv: 0.7766 | Sty: 0.0098 | Cyc: 0.4126\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/024000_grid.jpg\n",
            "Iter [24100/100000] Time: 2599.00s | D_loss: 1.0388 | G_adv: 0.6855 | Sty: 0.0085 | Cyc: 0.3608\n",
            "Iter [24200/100000] Time: 2662.07s | D_loss: 0.6587 | G_adv: 1.0275 | Sty: 0.0088 | Cyc: 0.6000\n",
            "Iter [24300/100000] Time: 2725.14s | D_loss: 0.9297 | G_adv: 1.1152 | Sty: 0.0089 | Cyc: 0.6080\n",
            "Iter [24400/100000] Time: 2788.52s | D_loss: 1.1015 | G_adv: 0.9196 | Sty: 0.0087 | Cyc: 0.4504\n",
            "Iter [24500/100000] Time: 2851.59s | D_loss: 1.2601 | G_adv: 0.2863 | Sty: 0.0084 | Cyc: 0.4468\n",
            "Iter [24600/100000] Time: 2914.79s | D_loss: 1.0620 | G_adv: 0.3759 | Sty: 0.0091 | Cyc: 0.4028\n",
            "Iter [24700/100000] Time: 2977.81s | D_loss: 0.8727 | G_adv: 1.2536 | Sty: 0.0083 | Cyc: 0.4674\n",
            "Iter [24800/100000] Time: 3040.86s | D_loss: 1.1723 | G_adv: 0.7552 | Sty: 0.0088 | Cyc: 0.3911\n",
            "Iter [24900/100000] Time: 3104.04s | D_loss: 1.1944 | G_adv: 0.5710 | Sty: 0.0085 | Cyc: 0.4980\n",
            "Iter [25000/100000] Time: 3167.10s | D_loss: 1.2564 | G_adv: 0.8553 | Sty: 0.0094 | Cyc: 0.3569\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/025000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/checkpoints/025000.ckpt\n",
            "Iter [25100/100000] Time: 3231.40s | D_loss: 0.9553 | G_adv: 0.9140 | Sty: 0.0089 | Cyc: 0.4489\n",
            "Iter [25200/100000] Time: 3294.38s | D_loss: 0.8119 | G_adv: 0.7335 | Sty: 0.0101 | Cyc: 0.4212\n",
            "Iter [25300/100000] Time: 3357.56s | D_loss: 1.0001 | G_adv: 1.0003 | Sty: 0.0086 | Cyc: 0.4366\n",
            "Iter [25400/100000] Time: 3420.73s | D_loss: 1.3243 | G_adv: 1.0692 | Sty: 0.0099 | Cyc: 0.4151\n",
            "Iter [25500/100000] Time: 3484.07s | D_loss: 0.8132 | G_adv: 1.1733 | Sty: 0.0103 | Cyc: 0.4507\n",
            "Iter [25600/100000] Time: 3547.16s | D_loss: 1.0490 | G_adv: 0.8288 | Sty: 0.0085 | Cyc: 0.4199\n",
            "Iter [25700/100000] Time: 3610.27s | D_loss: 0.9103 | G_adv: 0.7950 | Sty: 0.0093 | Cyc: 0.4631\n",
            "Iter [25800/100000] Time: 3673.41s | D_loss: 1.0616 | G_adv: 0.7357 | Sty: 0.0092 | Cyc: 0.3876\n",
            "Iter [25900/100000] Time: 3736.50s | D_loss: 1.1620 | G_adv: 0.9195 | Sty: 0.0098 | Cyc: 0.4588\n",
            "Iter [26000/100000] Time: 3799.68s | D_loss: 1.1524 | G_adv: 0.9692 | Sty: 0.0094 | Cyc: 0.4359\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/026000_grid.jpg\n",
            "Iter [26100/100000] Time: 3863.28s | D_loss: 1.2749 | G_adv: 0.9496 | Sty: 0.0089 | Cyc: 0.3257\n",
            "Iter [26200/100000] Time: 3926.25s | D_loss: 1.5872 | G_adv: 1.4289 | Sty: 0.0098 | Cyc: 0.3178\n",
            "Iter [26300/100000] Time: 3989.34s | D_loss: 0.9288 | G_adv: 1.5008 | Sty: 0.0081 | Cyc: 0.4774\n",
            "Iter [26400/100000] Time: 4052.37s | D_loss: 0.9669 | G_adv: 0.5703 | Sty: 0.0090 | Cyc: 0.3552\n",
            "Iter [26500/100000] Time: 4115.46s | D_loss: 0.8700 | G_adv: 0.3679 | Sty: 0.0090 | Cyc: 0.3447\n",
            "Iter [26600/100000] Time: 4178.63s | D_loss: 0.9597 | G_adv: 0.5480 | Sty: 0.0086 | Cyc: 0.3849\n",
            "Iter [26700/100000] Time: 4241.79s | D_loss: 1.1698 | G_adv: 0.7103 | Sty: 0.0092 | Cyc: 0.3404\n",
            "Iter [26800/100000] Time: 4305.08s | D_loss: 0.6306 | G_adv: 0.9613 | Sty: 0.0095 | Cyc: 0.3972\n",
            "Iter [26900/100000] Time: 4368.16s | D_loss: 1.0358 | G_adv: 1.6432 | Sty: 0.0085 | Cyc: 0.5151\n",
            "Iter [27000/100000] Time: 4431.31s | D_loss: 0.8420 | G_adv: 0.5853 | Sty: 0.0084 | Cyc: 0.4994\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/027000_grid.jpg\n",
            "Iter [27100/100000] Time: 4494.64s | D_loss: 0.9358 | G_adv: 0.9928 | Sty: 0.0087 | Cyc: 0.4605\n",
            "Iter [27200/100000] Time: 4557.79s | D_loss: 0.9273 | G_adv: 1.1532 | Sty: 0.0089 | Cyc: 0.3922\n",
            "Iter [27300/100000] Time: 4620.67s | D_loss: 0.7876 | G_adv: 0.8073 | Sty: 0.0084 | Cyc: 0.4220\n",
            "Iter [27400/100000] Time: 4683.63s | D_loss: 1.1919 | G_adv: 0.5640 | Sty: 0.0092 | Cyc: 0.3618\n",
            "Iter [27500/100000] Time: 4746.61s | D_loss: 1.0247 | G_adv: 0.4230 | Sty: 0.0090 | Cyc: 0.4591\n",
            "Iter [27600/100000] Time: 4809.74s | D_loss: 0.9491 | G_adv: 0.8374 | Sty: 0.0091 | Cyc: 0.4320\n",
            "Iter [27700/100000] Time: 4872.81s | D_loss: 1.2268 | G_adv: 1.0168 | Sty: 0.0101 | Cyc: 0.3784\n",
            "Iter [27800/100000] Time: 4936.00s | D_loss: 0.9836 | G_adv: 0.6598 | Sty: 0.0086 | Cyc: 0.4818\n",
            "Iter [27900/100000] Time: 4999.12s | D_loss: 0.7985 | G_adv: 1.0051 | Sty: 0.0090 | Cyc: 0.4383\n",
            "Iter [28000/100000] Time: 5062.22s | D_loss: 1.1163 | G_adv: 0.6837 | Sty: 0.0093 | Cyc: 0.3665\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/028000_grid.jpg\n",
            "Iter [28100/100000] Time: 5125.48s | D_loss: 0.7816 | G_adv: 1.3961 | Sty: 0.0093 | Cyc: 0.5042\n",
            "Iter [28200/100000] Time: 5188.51s | D_loss: 1.1362 | G_adv: 0.9221 | Sty: 0.0097 | Cyc: 0.4340\n",
            "Iter [28300/100000] Time: 5251.80s | D_loss: 1.3952 | G_adv: 0.2168 | Sty: 0.0089 | Cyc: 0.4044\n",
            "Iter [28400/100000] Time: 5315.09s | D_loss: 1.3506 | G_adv: 0.4677 | Sty: 0.0091 | Cyc: 0.4044\n",
            "Iter [28500/100000] Time: 5378.33s | D_loss: 1.2085 | G_adv: 1.0354 | Sty: 0.0096 | Cyc: 0.4415\n",
            "Iter [28600/100000] Time: 5441.49s | D_loss: 1.0607 | G_adv: 0.9292 | Sty: 0.0083 | Cyc: 0.5641\n",
            "Iter [28700/100000] Time: 5504.63s | D_loss: 0.9383 | G_adv: 1.2662 | Sty: 0.0088 | Cyc: 0.4866\n",
            "Iter [28800/100000] Time: 5567.63s | D_loss: 0.9127 | G_adv: 0.8229 | Sty: 0.0100 | Cyc: 0.7082\n",
            "Iter [28900/100000] Time: 5630.71s | D_loss: 1.2291 | G_adv: 0.7667 | Sty: 0.0093 | Cyc: 0.5397\n",
            "Iter [29000/100000] Time: 5693.67s | D_loss: 1.2601 | G_adv: 1.2223 | Sty: 0.0087 | Cyc: 0.4893\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/029000_grid.jpg\n",
            "Iter [29100/100000] Time: 5757.00s | D_loss: 1.0662 | G_adv: 0.8670 | Sty: 0.0090 | Cyc: 0.5487\n",
            "Iter [29200/100000] Time: 5820.03s | D_loss: 1.0698 | G_adv: 0.8297 | Sty: 0.0085 | Cyc: 0.3228\n",
            "Iter [29300/100000] Time: 5883.12s | D_loss: 1.1215 | G_adv: 0.7738 | Sty: 0.0090 | Cyc: 0.5023\n",
            "Iter [29400/100000] Time: 5946.18s | D_loss: 0.9891 | G_adv: 0.8950 | Sty: 0.0082 | Cyc: 0.5777\n",
            "Iter [29500/100000] Time: 6009.30s | D_loss: 0.8851 | G_adv: 1.4402 | Sty: 0.0094 | Cyc: 0.6312\n",
            "Iter [29600/100000] Time: 6072.23s | D_loss: 1.0569 | G_adv: 1.1392 | Sty: 0.0095 | Cyc: 0.6079\n",
            "Iter [29700/100000] Time: 6135.19s | D_loss: 1.2129 | G_adv: 0.4771 | Sty: 0.0084 | Cyc: 0.4327\n",
            "Iter [29800/100000] Time: 6198.33s | D_loss: 1.1986 | G_adv: 1.4111 | Sty: 0.0081 | Cyc: 0.3704\n",
            "Iter [29900/100000] Time: 6261.36s | D_loss: 0.8689 | G_adv: 0.7195 | Sty: 0.0092 | Cyc: 0.6664\n",
            "Iter [30000/100000] Time: 6324.52s | D_loss: 0.9690 | G_adv: 0.9363 | Sty: 0.0101 | Cyc: 0.3581\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/030000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/checkpoints/030000.ckpt\n",
            "Iter [30100/100000] Time: 6388.80s | D_loss: 1.0703 | G_adv: 0.9966 | Sty: 0.0091 | Cyc: 0.5249\n",
            "Iter [30200/100000] Time: 6451.80s | D_loss: 1.4086 | G_adv: 0.6684 | Sty: 0.0092 | Cyc: 0.3917\n",
            "Iter [30300/100000] Time: 6515.05s | D_loss: 0.9287 | G_adv: 0.6513 | Sty: 0.0079 | Cyc: 0.3901\n",
            "Iter [30400/100000] Time: 6578.22s | D_loss: 1.2115 | G_adv: 0.6088 | Sty: 0.0088 | Cyc: 0.4090\n",
            "Iter [30500/100000] Time: 6641.35s | D_loss: 1.2395 | G_adv: 0.7909 | Sty: 0.0096 | Cyc: 0.5081\n",
            "Iter [30600/100000] Time: 6704.74s | D_loss: 1.3421 | G_adv: 0.8177 | Sty: 0.0092 | Cyc: 0.3806\n",
            "Iter [30700/100000] Time: 6768.01s | D_loss: 1.2730 | G_adv: 0.6152 | Sty: 0.0088 | Cyc: 0.4971\n",
            "Iter [30800/100000] Time: 6831.08s | D_loss: 0.9399 | G_adv: 1.1313 | Sty: 0.0095 | Cyc: 0.4606\n",
            "Iter [30900/100000] Time: 6894.10s | D_loss: 1.1108 | G_adv: 0.4130 | Sty: 0.0085 | Cyc: 0.4517\n",
            "Iter [31000/100000] Time: 6957.15s | D_loss: 1.2128 | G_adv: 1.4151 | Sty: 0.0083 | Cyc: 0.6357\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/031000_grid.jpg\n",
            "Iter [31100/100000] Time: 7020.49s | D_loss: 0.9601 | G_adv: 0.6362 | Sty: 0.0096 | Cyc: 0.5125\n",
            "Iter [31200/100000] Time: 7083.55s | D_loss: 1.6301 | G_adv: 1.7607 | Sty: 0.0087 | Cyc: 0.4783\n",
            "Iter [31300/100000] Time: 7146.65s | D_loss: 1.2062 | G_adv: 1.3897 | Sty: 0.0103 | Cyc: 0.4367\n",
            "Iter [31400/100000] Time: 7209.66s | D_loss: 1.0565 | G_adv: 0.8868 | Sty: 0.0089 | Cyc: 0.3783\n",
            "Iter [31500/100000] Time: 7272.65s | D_loss: 0.6822 | G_adv: 0.5991 | Sty: 0.0090 | Cyc: 0.3455\n",
            "Iter [31600/100000] Time: 7335.59s | D_loss: 1.2691 | G_adv: 0.7596 | Sty: 0.0097 | Cyc: 0.5106\n",
            "Iter [31700/100000] Time: 7398.60s | D_loss: 0.9285 | G_adv: 1.1185 | Sty: 0.0088 | Cyc: 0.5995\n",
            "Iter [31800/100000] Time: 7461.64s | D_loss: 0.8736 | G_adv: 0.7152 | Sty: 0.0090 | Cyc: 0.4146\n",
            "Iter [31900/100000] Time: 7524.63s | D_loss: 1.1633 | G_adv: 0.5802 | Sty: 0.0096 | Cyc: 0.4244\n",
            "Iter [32000/100000] Time: 7587.55s | D_loss: 1.1201 | G_adv: 1.1026 | Sty: 0.0093 | Cyc: 0.5136\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/032000_grid.jpg\n",
            "Iter [32100/100000] Time: 7650.99s | D_loss: 1.2475 | G_adv: 0.7015 | Sty: 0.0088 | Cyc: 0.3719\n",
            "Iter [32200/100000] Time: 7713.94s | D_loss: 1.3052 | G_adv: 1.0004 | Sty: 0.0083 | Cyc: 0.3885\n",
            "Iter [32300/100000] Time: 7776.95s | D_loss: 1.0031 | G_adv: 0.7724 | Sty: 0.0086 | Cyc: 0.3959\n",
            "Iter [32400/100000] Time: 7840.10s | D_loss: 1.2340 | G_adv: 0.5465 | Sty: 0.0086 | Cyc: 0.3260\n",
            "Iter [32500/100000] Time: 7903.10s | D_loss: 1.1880 | G_adv: 0.7670 | Sty: 0.0086 | Cyc: 0.3972\n",
            "Iter [32600/100000] Time: 7966.05s | D_loss: 1.0274 | G_adv: 1.3226 | Sty: 0.0093 | Cyc: 0.5584\n",
            "Iter [32700/100000] Time: 8029.12s | D_loss: 0.9758 | G_adv: 0.7919 | Sty: 0.0090 | Cyc: 0.5215\n",
            "Iter [32800/100000] Time: 8092.19s | D_loss: 0.8961 | G_adv: 0.8952 | Sty: 0.0083 | Cyc: 0.4925\n",
            "Iter [32900/100000] Time: 8155.31s | D_loss: 1.2718 | G_adv: 1.3205 | Sty: 0.0094 | Cyc: 0.4653\n",
            "Iter [33000/100000] Time: 8218.50s | D_loss: 0.8566 | G_adv: 1.1016 | Sty: 0.0087 | Cyc: 0.3847\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/033000_grid.jpg\n",
            "Iter [33100/100000] Time: 8281.73s | D_loss: 1.0570 | G_adv: 1.1041 | Sty: 0.0096 | Cyc: 0.3479\n",
            "Iter [33200/100000] Time: 8344.80s | D_loss: 1.4447 | G_adv: 0.5137 | Sty: 0.0082 | Cyc: 0.4200\n",
            "Iter [33300/100000] Time: 8407.78s | D_loss: 1.0211 | G_adv: 0.4780 | Sty: 0.0092 | Cyc: 0.5058\n",
            "Iter [33400/100000] Time: 8470.63s | D_loss: 1.2062 | G_adv: 1.0963 | Sty: 0.0095 | Cyc: 0.3451\n",
            "Iter [33500/100000] Time: 8533.63s | D_loss: 1.0350 | G_adv: 0.9976 | Sty: 0.0103 | Cyc: 0.4423\n",
            "Iter [33600/100000] Time: 8596.89s | D_loss: 1.3532 | G_adv: 0.8929 | Sty: 0.0091 | Cyc: 0.4337\n",
            "Iter [33700/100000] Time: 8660.17s | D_loss: 0.8701 | G_adv: 0.5015 | Sty: 0.0093 | Cyc: 0.4420\n",
            "Iter [33800/100000] Time: 8723.26s | D_loss: 1.0390 | G_adv: 1.2605 | Sty: 0.0097 | Cyc: 0.4221\n",
            "Iter [33900/100000] Time: 8786.34s | D_loss: 1.3826 | G_adv: 0.6546 | Sty: 0.0091 | Cyc: 0.3183\n",
            "Iter [34000/100000] Time: 8849.38s | D_loss: 1.2618 | G_adv: 0.8930 | Sty: 0.0097 | Cyc: 0.4482\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/034000_grid.jpg\n",
            "Iter [34100/100000] Time: 8912.69s | D_loss: 1.2419 | G_adv: 0.4138 | Sty: 0.0095 | Cyc: 0.5462\n",
            "Iter [34200/100000] Time: 8975.78s | D_loss: 1.0520 | G_adv: 1.4331 | Sty: 0.0097 | Cyc: 0.4387\n",
            "Iter [34300/100000] Time: 9038.89s | D_loss: 0.9148 | G_adv: 0.9409 | Sty: 0.0087 | Cyc: 0.4953\n",
            "Iter [34400/100000] Time: 9102.02s | D_loss: 0.8021 | G_adv: 0.7058 | Sty: 0.0092 | Cyc: 0.5465\n",
            "Iter [34500/100000] Time: 9164.99s | D_loss: 0.7209 | G_adv: 0.8576 | Sty: 0.0096 | Cyc: 0.3899\n",
            "Iter [34600/100000] Time: 9227.97s | D_loss: 1.0477 | G_adv: 0.7523 | Sty: 0.0096 | Cyc: 0.3345\n",
            "Iter [34700/100000] Time: 9290.86s | D_loss: 1.0071 | G_adv: 0.6640 | Sty: 0.0102 | Cyc: 0.4871\n",
            "Iter [34800/100000] Time: 9353.82s | D_loss: 1.0698 | G_adv: 1.2330 | Sty: 0.0096 | Cyc: 0.4593\n",
            "Iter [34900/100000] Time: 9417.00s | D_loss: 0.9319 | G_adv: 0.9533 | Sty: 0.0088 | Cyc: 0.4822\n",
            "Iter [35000/100000] Time: 9480.05s | D_loss: 1.2583 | G_adv: 1.0174 | Sty: 0.0088 | Cyc: 0.4924\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/035000_grid.jpg\n",
            "Saved checkpoint to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/checkpoints/035000.ckpt\n",
            "Iter [35100/100000] Time: 9544.43s | D_loss: 0.8964 | G_adv: 1.2025 | Sty: 0.0086 | Cyc: 0.3803\n",
            "Iter [35200/100000] Time: 9607.59s | D_loss: 1.3303 | G_adv: 0.2661 | Sty: 0.0091 | Cyc: 0.4284\n",
            "Iter [35300/100000] Time: 9670.67s | D_loss: 1.3735 | G_adv: 0.9611 | Sty: 0.0094 | Cyc: 0.5246\n",
            "Iter [35400/100000] Time: 9733.78s | D_loss: 0.9527 | G_adv: 0.4944 | Sty: 0.0093 | Cyc: 0.4557\n",
            "Iter [35500/100000] Time: 9797.02s | D_loss: 1.0822 | G_adv: 1.0544 | Sty: 0.0090 | Cyc: 0.4565\n",
            "Iter [35600/100000] Time: 9860.11s | D_loss: 1.0565 | G_adv: 0.7213 | Sty: 0.0093 | Cyc: 0.5558\n",
            "Iter [35700/100000] Time: 9923.41s | D_loss: 1.0654 | G_adv: 0.4344 | Sty: 0.0085 | Cyc: 0.4417\n",
            "Iter [35800/100000] Time: 9986.98s | D_loss: 1.0472 | G_adv: 0.9351 | Sty: 0.0089 | Cyc: 0.3067\n",
            "Iter [35900/100000] Time: 10050.25s | D_loss: 1.3208 | G_adv: 1.1332 | Sty: 0.0101 | Cyc: 0.5219\n",
            "Iter [36000/100000] Time: 10113.39s | D_loss: 1.3034 | G_adv: 1.0410 | Sty: 0.0092 | Cyc: 0.4503\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/036000_grid.jpg\n",
            "Iter [36100/100000] Time: 10177.09s | D_loss: 1.2262 | G_adv: 1.1127 | Sty: 0.0082 | Cyc: 0.4455\n",
            "Iter [36200/100000] Time: 10240.42s | D_loss: 1.4390 | G_adv: 0.9836 | Sty: 0.0088 | Cyc: 0.4412\n",
            "Iter [36300/100000] Time: 10303.65s | D_loss: 1.0497 | G_adv: 1.4544 | Sty: 0.0093 | Cyc: 0.4986\n",
            "Iter [36400/100000] Time: 10366.91s | D_loss: 1.1759 | G_adv: 1.2073 | Sty: 0.0088 | Cyc: 0.4334\n",
            "Iter [36500/100000] Time: 10430.11s | D_loss: 0.9070 | G_adv: 0.9001 | Sty: 0.0094 | Cyc: 0.3931\n",
            "Iter [36600/100000] Time: 10493.41s | D_loss: 0.7570 | G_adv: 1.1324 | Sty: 0.0084 | Cyc: 0.3269\n",
            "Iter [36700/100000] Time: 10556.69s | D_loss: 1.0327 | G_adv: 1.1335 | Sty: 0.0086 | Cyc: 0.3547\n",
            "Iter [36800/100000] Time: 10620.03s | D_loss: 1.1845 | G_adv: 1.4082 | Sty: 0.0095 | Cyc: 0.4064\n",
            "Iter [36900/100000] Time: 10683.28s | D_loss: 1.0874 | G_adv: 1.2348 | Sty: 0.0089 | Cyc: 0.3669\n",
            "Iter [37000/100000] Time: 10746.54s | D_loss: 0.7586 | G_adv: 0.7779 | Sty: 0.0091 | Cyc: 0.3936\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/037000_grid.jpg\n",
            "Iter [37100/100000] Time: 10810.06s | D_loss: 1.4873 | G_adv: 0.8356 | Sty: 0.0083 | Cyc: 0.3857\n",
            "Iter [37200/100000] Time: 10873.30s | D_loss: 1.0323 | G_adv: 0.7530 | Sty: 0.0090 | Cyc: 0.3752\n",
            "Iter [37300/100000] Time: 10936.65s | D_loss: 1.2827 | G_adv: 0.4721 | Sty: 0.0087 | Cyc: 0.4105\n",
            "Iter [37400/100000] Time: 10999.83s | D_loss: 1.4333 | G_adv: 0.5724 | Sty: 0.0083 | Cyc: 0.3488\n",
            "Iter [37500/100000] Time: 11063.01s | D_loss: 1.1692 | G_adv: 0.8279 | Sty: 0.0094 | Cyc: 0.4616\n",
            "Iter [37600/100000] Time: 11126.20s | D_loss: 1.1169 | G_adv: 1.2159 | Sty: 0.0090 | Cyc: 0.4041\n",
            "Iter [37700/100000] Time: 11189.37s | D_loss: 1.0471 | G_adv: 0.7930 | Sty: 0.0088 | Cyc: 0.3528\n",
            "Iter [37800/100000] Time: 11252.60s | D_loss: 0.8024 | G_adv: 0.8623 | Sty: 0.0096 | Cyc: 0.4733\n",
            "Iter [37900/100000] Time: 11315.79s | D_loss: 1.0886 | G_adv: 1.1352 | Sty: 0.0098 | Cyc: 0.4469\n",
            "Iter [38000/100000] Time: 11379.07s | D_loss: 1.0877 | G_adv: 0.7525 | Sty: 0.0083 | Cyc: 0.4305\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/038000_grid.jpg\n",
            "Iter [38100/100000] Time: 11442.73s | D_loss: 0.8590 | G_adv: 0.9479 | Sty: 0.0087 | Cyc: 0.4492\n",
            "Iter [38200/100000] Time: 11505.98s | D_loss: 0.9154 | G_adv: 0.8550 | Sty: 0.0081 | Cyc: 0.4220\n",
            "Iter [38300/100000] Time: 11569.23s | D_loss: 1.4472 | G_adv: 0.6650 | Sty: 0.0087 | Cyc: 0.3459\n",
            "Iter [38400/100000] Time: 11632.47s | D_loss: 0.9635 | G_adv: 0.6424 | Sty: 0.0094 | Cyc: 0.5557\n",
            "Iter [38500/100000] Time: 11695.51s | D_loss: 1.4434 | G_adv: 1.2812 | Sty: 0.0093 | Cyc: 0.5130\n",
            "Iter [38600/100000] Time: 11758.56s | D_loss: 1.1124 | G_adv: 0.7348 | Sty: 0.0084 | Cyc: 0.3759\n",
            "Iter [38700/100000] Time: 11821.64s | D_loss: 1.1138 | G_adv: 0.0077 | Sty: 0.0087 | Cyc: 0.3360\n",
            "Iter [38800/100000] Time: 11884.80s | D_loss: 1.0692 | G_adv: 0.5687 | Sty: 0.0092 | Cyc: 0.4994\n",
            "Iter [38900/100000] Time: 11947.84s | D_loss: 1.3338 | G_adv: 0.6444 | Sty: 0.0085 | Cyc: 0.4561\n",
            "Iter [39000/100000] Time: 12011.02s | D_loss: 1.0566 | G_adv: 1.1356 | Sty: 0.0096 | Cyc: 0.3130\n",
            "Sample image grid saved to /content/drive/MyDrive/Colab Notebooks/GAN_assignment/GAN_FMNIST_v3-2/samples/039000_grid.jpg\n",
            "Iter [39100/100000] Time: 12074.32s | D_loss: 1.1465 | G_adv: 0.5958 | Sty: 0.0087 | Cyc: 0.3442\n",
            "Iter [39200/100000] Time: 12137.38s | D_loss: 1.1827 | G_adv: 0.8129 | Sty: 0.0094 | Cyc: 0.3192\n",
            "Iter [39300/100000] Time: 12200.40s | D_loss: 0.8579 | G_adv: 1.3544 | Sty: 0.0094 | Cyc: 0.4243\n",
            "Iter [39400/100000] Time: 12263.42s | D_loss: 0.7900 | G_adv: 1.0637 | Sty: 0.0093 | Cyc: 0.4877\n",
            "Iter [39500/100000] Time: 12326.45s | D_loss: 0.8154 | G_adv: 0.5036 | Sty: 0.0085 | Cyc: 0.4116\n",
            "Iter [39600/100000] Time: 12389.64s | D_loss: 1.1828 | G_adv: 0.4595 | Sty: 0.0095 | Cyc: 0.3450\n",
            "Iter [39700/100000] Time: 12452.90s | D_loss: 1.0992 | G_adv: 0.6580 | Sty: 0.0094 | Cyc: 0.3836\n",
            "Iter [39800/100000] Time: 12516.24s | D_loss: 1.3172 | G_adv: 0.3191 | Sty: 0.0104 | Cyc: 0.3154\n",
            "Iter [39900/100000] Time: 12579.97s | D_loss: 1.0751 | G_adv: 0.6855 | Sty: 0.0087 | Cyc: 0.5234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "# 기존 설정과 모델 불러오기\n",
        "from config import get_config, get_data_transform, get_domain_labels\n",
        "from model import Generator, MappingNetwork, StyleEncoder\n",
        "\n",
        "def load_model(args, device, checkpoint_step=None):\n",
        "    \"\"\"저장된 체크포인트를 불러옵니다.\"\"\"\n",
        "    model_path = os.path.join(args.save_root, args.project_name, 'checkpoints')\n",
        "\n",
        "    # 체크포인트 지정이 없으면 가장 마지막(최신) 파일 로드\n",
        "    if checkpoint_step is None:\n",
        "        ckpts = sorted([f for f in os.listdir(model_path) if f.endswith('.ckpt')])\n",
        "        if not ckpts:\n",
        "            raise FileNotFoundError(\"체크포인트가 없습니다!\")\n",
        "        latest_ckpt = ckpts[-1]\n",
        "    else:\n",
        "        latest_ckpt = f'{checkpoint_step:06d}.ckpt'\n",
        "\n",
        "    ckpt_path = os.path.join(model_path, latest_ckpt)\n",
        "    print(f\"Loading checkpoint: {ckpt_path}\")\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "\n",
        "    # 모델 초기화 및 가중치 로드\n",
        "    generator = Generator(args.img_size, args.style_dim).to(device)\n",
        "    mapping_network = MappingNetwork(args.latent_dim, args.style_dim, args.num_domains, args.hidden_dim).to(device)\n",
        "    # Style Encoder는 Reference Guided Synthesis 할 때 필요 (여기서는 Latent Guided만 시연)\n",
        "\n",
        "    generator.load_state_dict(ckpt['nets']['G'])\n",
        "    mapping_network.load_state_dict(ckpt['nets']['F'])\n",
        "\n",
        "    generator.eval()\n",
        "    mapping_network.eval()\n",
        "\n",
        "    return generator, mapping_network\n",
        "\n",
        "def inference(args, device):\n",
        "    # 1. 모델 로드\n",
        "    generator, mapping_net = load_model(args, device)\n",
        "\n",
        "    # 2. 테스트 데이터 로드 (학습에 안 쓴 데이터)\n",
        "    transform = get_data_transform(args.img_size)\n",
        "    test_dataset = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True) # 10장만 샘플링\n",
        "\n",
        "    # 3. 소스 이미지 가져오기\n",
        "    x_real, y_org = next(iter(test_loader))\n",
        "    x_real = x_real.to(device)\n",
        "\n",
        "    # 4. 시각화 준비\n",
        "    domain_labels = get_domain_labels()\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # [Row 1] 원본 이미지 (Source)\n",
        "    for i in range(10):\n",
        "        plt.subplot(11, 10, i + 1)\n",
        "        img = x_real[i].cpu().squeeze().numpy()\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.axis('off')\n",
        "        if i == 0: plt.title(\"Source\", fontsize=12, loc='left')\n",
        "\n",
        "    # [Rows 2-11] 각 도메인으로 스타일 변환 (Latent Guided)\n",
        "    # 고정된 Random Noise z 하나를 모든 도메인에 적용해 봅니다.\n",
        "    z_trg = torch.randn(1, args.latent_dim).to(device)\n",
        "\n",
        "    for row_idx in range(args.num_domains): # 0~9 (각 도메인별)\n",
        "        # 해당 도메인(row_idx)의 스타일 코드 생성\n",
        "        y_trg = torch.tensor([row_idx]).to(device)\n",
        "        s_trg = mapping_net(z_trg, y_trg) # (1, style_dim)\n",
        "\n",
        "        # 스타일 코드를 배치 크기만큼 복사 (1 -> 10)\n",
        "        s_trg = s_trg.repeat(10, 1)\n",
        "\n",
        "        # 이미지 생성\n",
        "        with torch.no_grad():\n",
        "            x_fake = generator(x_real, s_trg)\n",
        "\n",
        "        # 결과 출력\n",
        "        for col_idx in range(10):\n",
        "            plt.subplot(11, 10, (row_idx + 1) * 10 + col_idx + 1)\n",
        "            img = x_fake[col_idx].cpu().squeeze().numpy()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # 왼쪽 첫 열에만 도메인 이름 표시\n",
        "            if col_idx == 0:\n",
        "                plt.text(-10, 32, domain_labels[row_idx], fontsize=10, va='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 실행\n",
        "if __name__ == '__main__':\n",
        "    config = get_config()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    inference(config, device)"
      ],
      "metadata": {
        "id": "7DKw-bIshf8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}